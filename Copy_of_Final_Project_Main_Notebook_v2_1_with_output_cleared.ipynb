{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Tyler Malka\n",
        "\n",
        "CSCI E-82 Advanced Machine Learning\n",
        "\n",
        "Final Project: Self-Supervised Attention for Super-Resolution\n",
        "\n",
        "Main notebook"
      ],
      "metadata": {
        "id": "ESVmPyPfZeEf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlJSK2c6G_zB"
      },
      "source": [
        "Now that we're familiar with implementing various SR techniques, we're going to set up the official RCAN implementation as our baseline and attempt to outperform it. (See preliminary notebook for initial testing of different SR methods)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ChatGPT assisted with code generation"
      ],
      "metadata": {
        "id": "5HFPmxizaVg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nj-moADpEWd0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import OrderedDict\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import figure\n",
        "from google.colab import drive\n",
        "\n",
        "def setup_rcan():\n",
        "    \"\"\"Setup RCAN repository and models\"\"\"\n",
        "    # Mount Google Drive if not already mounted\n",
        "    if not os.path.exists('/content/drive'):\n",
        "        drive.mount('/content/drive')\n",
        "\n",
        "    # Clone RCAN if not exists\n",
        "    if not os.path.exists('RCAN'):\n",
        "        !git clone https://github.com/yulunzhang/RCAN.git\n",
        "\n",
        "    # Add RCAN to path\n",
        "    if 'RCAN' not in sys.path:\n",
        "        sys.path.append('./RCAN/RCAN_TrainCode/code')\n",
        "\n",
        "    # Import necessary modules from RCAN\n",
        "    from model.rcan import RCAN\n",
        "    return RCAN\n",
        "\n",
        "class RCANFeatureExtractor(nn.Module):\n",
        "    \"\"\"Wrapper for RCAN to extract intermediate features\"\"\"\n",
        "    def __init__(self, rcan_model):\n",
        "        super().__init__()\n",
        "        self.head = rcan_model.head\n",
        "        self.body = rcan_model.body\n",
        "        self.tail = rcan_model.tail\n",
        "\n",
        "    def extract_features(self, x):\n",
        "        \"\"\"Extract features before the final upsampling\"\"\"\n",
        "        x = self.head(x)\n",
        "        body_output = self.body(x)\n",
        "        # Store the input for residual connection\n",
        "        return x, body_output\n",
        "\n",
        "    def complete_sr(self, features):\n",
        "        \"\"\"Complete super-resolution with extracted features\"\"\"\n",
        "        input_feat, body_output = features\n",
        "        # Add the residual connection\n",
        "        x = body_output + input_feat\n",
        "        # Apply final upsampling\n",
        "        x = self.tail(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass through the entire model\"\"\"\n",
        "        features = self.extract_features(x)\n",
        "        return self.complete_sr(features)\n",
        "\n",
        "def load_rcan_model(model_path='/content/drive/MyDrive/E82/finalproject/RCAN_BIX4.pt'):\n",
        "    \"\"\"Load pretrained RCAN model\"\"\"\n",
        "    # Import RCAN\n",
        "    RCAN = setup_rcan()\n",
        "\n",
        "    # Complete args dictionary with all required parameters\n",
        "    args = {\n",
        "        'n_resgroups': 10,\n",
        "        'n_resblocks': 20,\n",
        "        'n_feats': 64,\n",
        "        'scale': [4],\n",
        "        'rgb_range': 255,\n",
        "        'n_colors': 3,\n",
        "        'res_scale': 1,\n",
        "        'reduction': 16,\n",
        "        'conv': nn.Conv2d,\n",
        "        'kernel_size': 3,\n",
        "        'act': nn.ReLU(True),\n",
        "        'precision': 'single',\n",
        "        'cpu': False,\n",
        "    }\n",
        "\n",
        "    # Convert args dictionary to object-like structure\n",
        "    class Args:\n",
        "        def __init__(self, **entries):\n",
        "            self.__dict__.update(entries)\n",
        "\n",
        "    args = Args(**args)\n",
        "\n",
        "    # Create model\n",
        "    print(\"Creating RCAN model...\")\n",
        "    model = RCAN(args)\n",
        "\n",
        "    # Load pretrained weights\n",
        "    print(f\"Loading weights from {model_path}\")\n",
        "    state_dict = torch.load(model_path, weights_only=True)  # Added weights_only=True\n",
        "    model.load_state_dict(state_dict)\n",
        "    print(\"Weights loaded successfully\")\n",
        "\n",
        "    # Wrap model for feature extraction\n",
        "    model = RCANFeatureExtractor(model)\n",
        "    model.eval()\n",
        "\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Test setup\n",
        "    try:\n",
        "        print(\"Setting up RCAN model...\")\n",
        "        model = load_rcan_model()\n",
        "        print(\"RCAN model loaded successfully\")\n",
        "\n",
        "        # Test with dummy input\n",
        "        x = torch.randn(1, 3, 32, 32)\n",
        "        with torch.no_grad():\n",
        "            features = model.extract_features(x)\n",
        "            print(f\"Input feature shape: {features[0].shape}\")\n",
        "            print(f\"Body output shape: {features[1].shape}\")\n",
        "\n",
        "            output = model.complete_sr(features)\n",
        "            print(f\"Final output shape: {output.shape}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during setup: {str(e)}\")\n",
        "        import traceback\n",
        "        print(traceback.format_exc())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWiT3_OHGaCU"
      },
      "source": [
        "Input features are being transformed to 64 channels: [1, 64, 32, 32]\n",
        "\n",
        "Body maintains the same dimensions: [1, 64, 32, 32]\n",
        "\n",
        "Final output is correctly upscaled 4x with 3 channels: [1, 3, 128, 128]\n",
        "\n",
        "Correctly scaling to 4x, so we can continue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXC-Ce1EH5EO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "class SRDataset(Dataset):\n",
        "    def __init__(self, root_dir, scale=4, patch_size=96, train=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (str): Directory with images (e.g., Set5 or Set14 folder)\n",
        "            scale (int): Super resolution scale factor\n",
        "            patch_size (int): Size of HR patches to extract\n",
        "            train (bool): If True, prepare for training (random crops), else for evaluation\n",
        "        \"\"\"\n",
        "        self.scale = scale\n",
        "        self.patch_size = patch_size\n",
        "        self.train = train\n",
        "\n",
        "        # Mount Google Drive if not already mounted\n",
        "        if not os.path.exists('/content/drive'):\n",
        "            drive.mount('/content/drive')\n",
        "\n",
        "        # Find all images in the directory\n",
        "        self.image_files = sorted(glob.glob(os.path.join(root_dir, '*.png')))\n",
        "        if len(self.image_files) == 0:\n",
        "            raise RuntimeError(f\"No PNG images found in {root_dir}\")\n",
        "\n",
        "        print(f\"Found {len(self.image_files)} images in {root_dir}\")\n",
        "\n",
        "        # Basic augmentations for training\n",
        "        self.augment = transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomVerticalFlip()\n",
        "        ]) if train else None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load HR image\n",
        "        img_path = self.image_files[idx]\n",
        "        hr_image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        # Handle training vs evaluation\n",
        "        if self.train:\n",
        "            # Random crop for training\n",
        "            i, j, h, w = transforms.RandomCrop.get_params(\n",
        "                hr_image, output_size=(self.patch_size, self.patch_size))\n",
        "            hr_image = TF.crop(hr_image, i, j, h, w)\n",
        "\n",
        "            # Apply augmentations\n",
        "            if self.augment:\n",
        "                hr_image = self.augment(hr_image)\n",
        "        else:\n",
        "            # For validation, ensure dimensions are divisible by scale\n",
        "            w, h = hr_image.size\n",
        "            w = w - w % self.scale\n",
        "            h = h - h % self.scale\n",
        "            hr_image = hr_image.crop((0, 0, w, h))\n",
        "\n",
        "        # Convert to tensor\n",
        "        hr_tensor = TF.to_tensor(hr_image)\n",
        "\n",
        "        # Create LR image using bicubic downsampling\n",
        "        lr_tensor = TF.resize(hr_tensor,\n",
        "                            size=[s // self.scale for s in hr_tensor.shape[-2:]],\n",
        "                            interpolation=TF.InterpolationMode.BICUBIC)\n",
        "\n",
        "        return lr_tensor, hr_tensor\n",
        "\n",
        "def setup_datasets(batch_size=16):\n",
        "    \"\"\"Setup training and validation dataloaders\"\"\"\n",
        "    # Paths to your datasets\n",
        "    set5_path = '/content/drive/MyDrive/E82/finalproject/Set5'\n",
        "    set14_path = '/content/drive/MyDrive/E82/finalproject/Set14'\n",
        "\n",
        "    # Create datasets\n",
        "    set5_dataset = SRDataset(root_dir=set5_path, scale=4, patch_size=96, train=True)\n",
        "    set14_dataset = SRDataset(root_dir=set14_path, scale=4, patch_size=96, train=False)\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(\n",
        "        set5_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        set14_dataset,\n",
        "        batch_size=1,  # Evaluate one image at a time\n",
        "        shuffle=False,\n",
        "        num_workers=1,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Test the dataset setup\n",
        "    try:\n",
        "        print(\"Setting up datasets...\")\n",
        "        train_loader, val_loader = setup_datasets(batch_size=4)\n",
        "\n",
        "        print(f\"\\nNumber of training batches: {len(train_loader)}\")\n",
        "        print(f\"Number of validation images: {len(val_loader)}\")\n",
        "\n",
        "        # Test a batch\n",
        "        lr_batch, hr_batch = next(iter(train_loader))\n",
        "        print(f\"\\nSample batch shapes:\")\n",
        "        print(f\"LR batch: {lr_batch.shape}\")\n",
        "        print(f\"HR batch: {hr_batch.shape}\")\n",
        "\n",
        "        # Verify scale factor\n",
        "        lr_h, lr_w = lr_batch.shape[-2:]\n",
        "        hr_h, hr_w = hr_batch.shape[-2:]\n",
        "        print(f\"\\nScale factor verification:\")\n",
        "        print(f\"Height scale: {hr_h/lr_h:.0f}x\")\n",
        "        print(f\"Width scale: {hr_w/lr_w:.0f}x\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during setup: {str(e)}\")\n",
        "        import traceback\n",
        "        print(traceback.format_exc())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1dHZ-2cJX4m"
      },
      "source": [
        "Everything is working properly. Let's build our attention mechanism next."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAUI5JaDK3-H"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "class SelfSupervisedAttention(nn.Module):\n",
        "    \"\"\"Self-supervised auxiliary network for dynamic pixel importance prediction\"\"\"\n",
        "    def __init__(self, in_channels=64):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        # Spatial feature extraction\n",
        "        self.conv1 = nn.Conv2d(in_channels, in_channels, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
        "        self.conv2 = nn.Conv2d(in_channels, in_channels, 3, padding=1)  # Keep same channels\n",
        "        self.bn2 = nn.BatchNorm2d(in_channels)\n",
        "\n",
        "        # Attention prediction\n",
        "        self.conv3 = nn.Conv2d(in_channels, in_channels//2, 3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(in_channels//2)\n",
        "        self.conv4 = nn.Conv2d(in_channels//2, 1, 1)\n",
        "\n",
        "        # Channel attention\n",
        "        self.spatial_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.channel_attention = nn.Sequential(\n",
        "            nn.Linear(in_channels, in_channels//4),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(in_channels//4, in_channels),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initial feature extraction\n",
        "        feat = F.relu(self.bn1(self.conv1(x)))\n",
        "        feat = F.relu(self.bn2(self.conv2(feat)))  # [B, 64, H, W]\n",
        "\n",
        "        # Channel attention\n",
        "        channel_att = self.spatial_pool(x).squeeze(-1).squeeze(-1)  # [B, 64]\n",
        "        channel_att = self.channel_attention(channel_att)  # [B, 64]\n",
        "        channel_att = channel_att.view(-1, self.in_channels, 1, 1)  # [B, 64, 1, 1]\n",
        "\n",
        "        # Apply channel attention\n",
        "        feat = feat * channel_att  # [B, 64, H, W]\n",
        "\n",
        "        # Final attention prediction\n",
        "        feat = F.relu(self.bn3(self.conv3(feat)))  # [B, 32, H, W]\n",
        "        attention = torch.sigmoid(self.conv4(feat))  # [B, 1, H, W]\n",
        "\n",
        "        return attention\n",
        "\n",
        "class AttentionAugmentedRCAN(nn.Module):\n",
        "    def __init__(self, base_rcan, freeze_base=True):\n",
        "        super().__init__()\n",
        "        self.rcan = base_rcan\n",
        "        self.attention_net = SelfSupervisedAttention(64)  # RCAN uses 64 channels\n",
        "\n",
        "        if freeze_base:\n",
        "            for param in self.rcan.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, x, mode='inference'):\n",
        "        if mode == 'pre_training':\n",
        "            # Extract features but don't apply attention yet\n",
        "            feats = self.rcan.extract_features(x)\n",
        "            attention = self.attention_net(feats[1])  # Use body output for attention\n",
        "            return attention\n",
        "\n",
        "        # Normal forward pass with attention\n",
        "        input_feat, body_feat = self.rcan.extract_features(x)\n",
        "        attention = self.attention_net(body_feat)\n",
        "        weighted_feat = body_feat * attention\n",
        "\n",
        "        # Complete super-resolution\n",
        "        sr_output = self.rcan.complete_sr((input_feat, weighted_feat))\n",
        "\n",
        "        return sr_output, attention\n",
        "\n",
        "def entropy_loss(attention_maps):\n",
        "    \"\"\"Entropy-based regularization for attention maps\"\"\"\n",
        "    eps = 1e-8\n",
        "    entropy = -(attention_maps * torch.log(attention_maps + eps))\n",
        "    return entropy.mean()\n",
        "\n",
        "def spatial_consistency_loss(attention_maps):\n",
        "    \"\"\"Spatial consistency loss for attention maps\"\"\"\n",
        "    horizontal = F.l1_loss(attention_maps[..., :, 1:], attention_maps[..., :, :-1])\n",
        "    vertical = F.l1_loss(attention_maps[..., 1:, :], attention_maps[..., :-1, :])\n",
        "    return horizontal + vertical\n",
        "\n",
        "def get_reconstruction_difficulty(sr_output, hr_target):\n",
        "    \"\"\"Calculate pixel-wise reconstruction difficulty\"\"\"\n",
        "    with torch.no_grad():\n",
        "        diff = torch.abs(sr_output - hr_target)\n",
        "        # Normalize to [0, 1] range\n",
        "        diff = (diff - diff.min()) / (diff.max() - diff.min() + 1e-8)\n",
        "        return diff.mean(dim=1, keepdim=True)  # Average across channels\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model, train_loader, val_loader, device):\n",
        "        self.model = model\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.device = device\n",
        "        self.writer = SummaryWriter('runs/attention_rcan')\n",
        "\n",
        "    def pre_training_step(self, batch, optimizer):\n",
        "        \"\"\"Pre-training step for attention network\"\"\"\n",
        "        lr_imgs, hr_imgs = [x.to(self.device) for x in batch]\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Get base RCAN output for difficulty estimation\n",
        "        with torch.no_grad():\n",
        "            sr_output = self.model.rcan(lr_imgs)\n",
        "            difficulty = get_reconstruction_difficulty(sr_output, hr_imgs)\n",
        "\n",
        "        # Train attention network\n",
        "        attention = self.model(lr_imgs, mode='pre_training')\n",
        "\n",
        "        # Losses\n",
        "        prediction_loss = F.mse_loss(attention, difficulty)\n",
        "        entropy_reg = entropy_loss(attention)\n",
        "        consistency = spatial_consistency_loss(attention)\n",
        "\n",
        "        total_loss = prediction_loss + 0.1 * entropy_reg + 0.1 * consistency\n",
        "\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        return {\n",
        "            'prediction_loss': prediction_loss.item(),\n",
        "            'entropy_loss': entropy_reg.item(),\n",
        "            'consistency_loss': consistency.item(),\n",
        "            'total_loss': total_loss.item()\n",
        "        }\n",
        "\n",
        "    def fine_tuning_step(self, batch, optimizer):\n",
        "        \"\"\"Fine-tuning step for joint training\"\"\"\n",
        "        lr_imgs, hr_imgs = [x.to(self.device) for x in batch]\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass with attention\n",
        "        sr_output, attention = self.model(lr_imgs)\n",
        "\n",
        "        # Calculate losses\n",
        "        reconstruction_loss = F.l1_loss(sr_output, hr_imgs)\n",
        "        entropy_reg = entropy_loss(attention)\n",
        "        consistency = spatial_consistency_loss(attention)\n",
        "\n",
        "        # Get current reconstruction difficulty\n",
        "        difficulty = get_reconstruction_difficulty(sr_output, hr_imgs)\n",
        "        attention_guidance = F.mse_loss(attention, difficulty.detach())\n",
        "\n",
        "        total_loss = reconstruction_loss + 0.1 * entropy_reg + 0.1 * consistency + 0.1 * attention_guidance\n",
        "\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        return {\n",
        "            'reconstruction_loss': reconstruction_loss.item(),\n",
        "            'entropy_loss': entropy_reg.item(),\n",
        "            'consistency_loss': consistency.item(),\n",
        "            'attention_guidance': attention_guidance.item(),\n",
        "            'total_loss': total_loss.item()\n",
        "        }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Quick test of attention mechanism\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    x = torch.randn(4, 64, 32, 32).to(device)\n",
        "    attention_net = SelfSupervisedAttention().to(device)\n",
        "    attention_maps = attention_net(x)\n",
        "    print(f\"Input shape: {x.shape}\")\n",
        "    print(f\"Attention map shape: {attention_maps.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz_QKlxpLRnP"
      },
      "source": [
        "Input: [4, 64, 32, 32] - batch of 4, 64 channels from RCAN features\n",
        "\n",
        "Output: [4, 1, 32, 32] - same spatial dimensions but single channel attention map\n",
        "\n",
        "Everything is working properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrfd3E1cMCtq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Use models and classes we defined above\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model, train_loader, val_loader, device):\n",
        "        self.model = model\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.device = device\n",
        "        self.writer = SummaryWriter('runs/attention_rcan')\n",
        "\n",
        "    def pre_training_step(self, batch, optimizer):\n",
        "        \"\"\"Pre-training step for attention network\"\"\"\n",
        "        lr_imgs, hr_imgs = [x.to(self.device) for x in batch]\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Get base RCAN output for difficulty estimation\n",
        "        with torch.no_grad():\n",
        "            sr_output = self.model.rcan(lr_imgs)\n",
        "            difficulty = get_reconstruction_difficulty(sr_output, hr_imgs)  # [B, 1, H, H]\n",
        "\n",
        "        # Train attention network\n",
        "        attention = self.model(lr_imgs, mode='pre_training')  # [B, 1, h, h]\n",
        "\n",
        "        # Upscale attention to match HR resolution\n",
        "        attention_upscaled = F.interpolate(\n",
        "            attention,\n",
        "            size=difficulty.shape[-2:],\n",
        "            mode='bilinear',\n",
        "            align_corners=False\n",
        "        )\n",
        "\n",
        "        # Losses\n",
        "        prediction_loss = F.mse_loss(attention_upscaled, difficulty)\n",
        "        entropy_reg = entropy_loss(attention)  # Keep entropy loss on LR attention\n",
        "        consistency = spatial_consistency_loss(attention)  # Keep consistency on LR attention\n",
        "\n",
        "        total_loss = prediction_loss + 0.1 * entropy_reg + 0.1 * consistency\n",
        "\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        return {\n",
        "            'prediction_loss': prediction_loss.item(),\n",
        "            'entropy_loss': entropy_reg.item(),\n",
        "            'consistency_loss': consistency.item(),\n",
        "            'total_loss': total_loss.item()\n",
        "        }\n",
        "\n",
        "    def fine_tuning_step(self, batch, optimizer):\n",
        "        \"\"\"Fine-tuning step for joint training\"\"\"\n",
        "        lr_imgs, hr_imgs = [x.to(self.device) for x in batch]\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass with attention\n",
        "        sr_output, attention = self.model(lr_imgs)\n",
        "\n",
        "        # Calculate losses\n",
        "        reconstruction_loss = F.l1_loss(sr_output, hr_imgs)\n",
        "        entropy_reg = entropy_loss(attention)\n",
        "        consistency = spatial_consistency_loss(attention)\n",
        "\n",
        "        # Get current reconstruction difficulty\n",
        "        difficulty = get_reconstruction_difficulty(sr_output, hr_imgs)\n",
        "\n",
        "        # Upscale attention to match HR resolution\n",
        "        attention_upscaled = F.interpolate(\n",
        "            attention,\n",
        "            size=difficulty.shape[-2:],\n",
        "            mode='bilinear',\n",
        "            align_corners=False\n",
        "        )\n",
        "        attention_guidance = F.mse_loss(attention_upscaled, difficulty.detach())\n",
        "\n",
        "        total_loss = reconstruction_loss + 0.1 * entropy_reg + 0.1 * consistency + 0.1 * attention_guidance\n",
        "\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        return {\n",
        "            'reconstruction_loss': reconstruction_loss.item(),\n",
        "            'entropy_loss': entropy_reg.item(),\n",
        "            'consistency_loss': consistency.item(),\n",
        "            'attention_guidance': attention_guidance.item(),\n",
        "            'total_loss': total_loss.item()\n",
        "        }\n",
        "\n",
        "# Load RCAN model\n",
        "print(\"Loading base RCAN model...\")\n",
        "base_model = load_rcan_model()\n",
        "base_model = base_model.to(device)\n",
        "\n",
        "# Create augmented model\n",
        "print(\"Creating attention-augmented model...\")\n",
        "model = AttentionAugmentedRCAN(base_model, freeze_base=True)\n",
        "model = model.to(device)\n",
        "\n",
        "# Setup data\n",
        "print(\"Setting up datasets...\")\n",
        "train_loader, val_loader = setup_datasets(batch_size=4)\n",
        "\n",
        "# Create trainer\n",
        "trainer = Trainer(model, train_loader, val_loader, device)\n",
        "\n",
        "# Training configuration\n",
        "config = {\n",
        "    'pre_train_epochs': 10,\n",
        "    'fine_tune_epochs': 20,\n",
        "    'pre_train_lr': 1e-4,\n",
        "    'fine_tune_lr': 5e-5\n",
        "}\n",
        "\n",
        "# Stage 1: Pre-training attention network\n",
        "print(\"\\nStage 1: Pre-training attention network\")\n",
        "attention_optimizer = torch.optim.Adam(\n",
        "    model.attention_net.parameters(),\n",
        "    lr=config['pre_train_lr']\n",
        ")\n",
        "\n",
        "for epoch in range(config['pre_train_epochs']):\n",
        "    model.train()\n",
        "\n",
        "    with tqdm(train_loader, desc=f'Pre-training Epoch {epoch+1}/{config[\"pre_train_epochs\"]}') as pbar:\n",
        "        for lr_imgs, hr_imgs in pbar:\n",
        "            losses = trainer.pre_training_step((lr_imgs, hr_imgs), attention_optimizer)\n",
        "            pbar.set_postfix({k: f'{v:.4f}' for k, v in losses.items()})\n",
        "\n",
        "    # Validation\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        print(f\"\\nValidating epoch {epoch+1}...\")\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "        with torch.no_grad():\n",
        "            for lr_imgs, hr_imgs in val_loader:\n",
        "                lr_imgs, hr_imgs = lr_imgs.to(device), hr_imgs.to(device)\n",
        "                attention = model(lr_imgs, mode='pre_training')\n",
        "                # Log first batch attention maps\n",
        "                if len(val_losses) == 0:\n",
        "                    print(f\"Sample attention map range: {attention.min().item():.4f} to {attention.max().item():.4f}\")\n",
        "\n",
        "# Stage 2: Fine-tuning\n",
        "print(\"\\nStage 2: Joint fine-tuning\")\n",
        "# Unfreeze RCAN\n",
        "for param in model.rcan.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=config['fine_tune_lr']\n",
        ")\n",
        "\n",
        "best_psnr = 0\n",
        "for epoch in range(config['fine_tune_epochs']):\n",
        "    model.train()\n",
        "\n",
        "    with tqdm(train_loader, desc=f'Fine-tuning Epoch {epoch+1}/{config[\"fine_tune_epochs\"]}') as pbar:\n",
        "        for lr_imgs, hr_imgs in pbar:\n",
        "            losses = trainer.fine_tuning_step((lr_imgs, hr_imgs), optimizer)\n",
        "            pbar.set_postfix({k: f'{v:.4f}' for k, v in losses.items()})\n",
        "\n",
        "    # Validation\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        print(f\"\\nValidating epoch {epoch+1}...\")\n",
        "        model.eval()\n",
        "        psnr_values = []\n",
        "        with torch.no_grad():\n",
        "            for lr_imgs, hr_imgs in val_loader:\n",
        "                lr_imgs, hr_imgs = lr_imgs.to(device), hr_imgs.to(device)\n",
        "                sr_output, attention = model(lr_imgs)\n",
        "\n",
        "                # Calculate PSNR\n",
        "                mse = torch.mean((sr_output - hr_imgs) ** 2)\n",
        "                psnr = 20 * torch.log10(1.0 / torch.sqrt(mse))\n",
        "                psnr_values.append(psnr.item())\n",
        "\n",
        "        avg_psnr = sum(psnr_values) / len(psnr_values)\n",
        "        print(f\"Average PSNR: {avg_psnr:.2f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if avg_psnr > best_psnr:\n",
        "            best_psnr = avg_psnr\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'psnr': best_psnr,\n",
        "            }, 'best_model.pt')\n",
        "            print(f\"Saved new best model with PSNR {best_psnr:.2f}\")\n",
        "\n",
        "print(\"Training completed!\")\n",
        "print(f\"Best PSNR achieved: {best_psnr:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYAXQl7pNqoS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "from PIL import Image\n",
        "import glob\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "import gdown\n",
        "\n",
        "class SRDataset(Dataset):\n",
        "    def __init__(self, root_dir, scale=4, patch_size=96, train=True):\n",
        "        self.scale = scale\n",
        "        self.patch_size = patch_size\n",
        "        self.train = train\n",
        "\n",
        "        # Mount Google Drive if not already mounted\n",
        "        if not os.path.exists('/content/drive'):\n",
        "            drive.mount('/content/drive')\n",
        "\n",
        "        # Find all images in the directory\n",
        "        self.image_files = sorted(glob.glob(os.path.join(root_dir, '*.png')))\n",
        "        if len(self.image_files) == 0:\n",
        "            raise RuntimeError(f\"No PNG images found in {root_dir}\")\n",
        "\n",
        "        print(f\"Found {len(self.image_files)} images in {root_dir}\")\n",
        "\n",
        "        # Basic augmentations for training\n",
        "        self.augment = transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomVerticalFlip()\n",
        "        ]) if train else None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load HR image\n",
        "        img_path = self.image_files[idx]\n",
        "        hr_image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        # Handle training vs evaluation\n",
        "        if self.train:\n",
        "            # Random crop for training\n",
        "            i, j, h, w = transforms.RandomCrop.get_params(\n",
        "                hr_image, output_size=(self.patch_size, self.patch_size))\n",
        "            hr_image = TF.crop(hr_image, i, j, h, w)\n",
        "\n",
        "            # Apply augmentations\n",
        "            if self.augment:\n",
        "                hr_image = self.augment(hr_image)\n",
        "        else:\n",
        "            # For validation, ensure dimensions are divisible by scale\n",
        "            w, h = hr_image.size\n",
        "            w = w - w % self.scale\n",
        "            h = h - h % self.scale\n",
        "            hr_image = hr_image.crop((0, 0, w, h))\n",
        "\n",
        "        # Convert to tensor\n",
        "        hr_tensor = TF.to_tensor(hr_image)\n",
        "\n",
        "        # Create LR image using bicubic downsampling\n",
        "        lr_tensor = TF.resize(hr_tensor,\n",
        "                            size=[s // self.scale for s in hr_tensor.shape[-2:]],\n",
        "                            interpolation=TF.InterpolationMode.BICUBIC)\n",
        "\n",
        "        return lr_tensor, hr_tensor\n",
        "\n",
        "def download_div2k():\n",
        "    \"\"\"Download DIV2K dataset if not present\"\"\"\n",
        "    # DIV2K training data\n",
        "    train_url = \"http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip\"\n",
        "    if not os.path.exists('DIV2K_train_HR'):\n",
        "        print(\"Downloading DIV2K training data...\")\n",
        "        gdown.download(train_url, 'DIV2K_train_HR.zip', quiet=False)\n",
        "        !unzip -q DIV2K_train_HR.zip\n",
        "        !rm DIV2K_train_HR.zip\n",
        "\n",
        "def setup_datasets(batch_size=16):\n",
        "    \"\"\"Setup training and validation dataloaders\"\"\"\n",
        "    # Download DIV2K if needed\n",
        "    download_div2k()\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = SRDataset(\n",
        "        root_dir='DIV2K_train_HR',\n",
        "        scale=4,\n",
        "        patch_size=96,\n",
        "        train=True\n",
        "    )\n",
        "\n",
        "    val_dataset = SRDataset(\n",
        "        root_dir='/content/drive/MyDrive/E82/finalproject/Set14',\n",
        "        scale=4,\n",
        "        patch_size=96,\n",
        "        train=False\n",
        "    )\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=1,\n",
        "        shuffle=False,\n",
        "        num_workers=1,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "def visualize_results(model, val_loader, device, save_path=None):\n",
        "    \"\"\"Visualize and compare results between bicubic, RCAN, and our model\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Get first validation image\n",
        "        lr_img, hr_img = next(iter(val_loader))\n",
        "        lr_img, hr_img = lr_img.to(device), hr_img.to(device)\n",
        "\n",
        "        # Bicubic upscaling\n",
        "        bicubic = F.interpolate(\n",
        "            lr_img,\n",
        "            scale_factor=4,\n",
        "            mode='bicubic',\n",
        "            align_corners=False\n",
        "        )\n",
        "\n",
        "        # Get RCAN output\n",
        "        rcan_output = model.rcan(lr_img)\n",
        "\n",
        "        # Get our model output\n",
        "        sr_output, attention = model(lr_img)\n",
        "\n",
        "        # Convert to images for plotting\n",
        "        def tensor_to_image(x):\n",
        "            x = x.cpu().squeeze(0).permute(1, 2, 0).clamp(0, 1).numpy()\n",
        "            return x\n",
        "\n",
        "        # Create figure\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "\n",
        "        # Plot images\n",
        "        axes[0, 0].imshow(tensor_to_image(lr_img))\n",
        "        axes[0, 0].set_title('LR Input')\n",
        "        axes[0, 1].imshow(tensor_to_image(bicubic))\n",
        "        axes[0, 1].set_title('Bicubic')\n",
        "        axes[0, 2].imshow(tensor_to_image(hr_img))\n",
        "        axes[0, 2].set_title('HR Ground Truth')\n",
        "        axes[1, 0].imshow(tensor_to_image(rcan_output))\n",
        "        axes[1, 0].set_title('RCAN Output')\n",
        "        axes[1, 1].imshow(tensor_to_image(sr_output))\n",
        "        axes[1, 1].set_title('Our Model Output')\n",
        "        axes[1, 2].imshow(tensor_to_image(attention.repeat(1, 3, 1, 1)))\n",
        "        axes[1, 2].set_title('Attention Map')\n",
        "\n",
        "        # Remove axes\n",
        "        for ax in axes.flat:\n",
        "            ax.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        if save_path:\n",
        "            plt.savefig(save_path)\n",
        "        plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Test the dataset setup\n",
        "    try:\n",
        "        print(\"Setting up datasets...\")\n",
        "        train_loader, val_loader = setup_datasets(batch_size=16)\n",
        "\n",
        "        print(f\"\\nNumber of training batches: {len(train_loader)}\")\n",
        "        print(f\"Number of validation images: {len(val_loader)}\")\n",
        "\n",
        "        # Test a batch\n",
        "        lr_batch, hr_batch = next(iter(train_loader))\n",
        "        print(f\"\\nSample batch shapes:\")\n",
        "        print(f\"LR batch: {lr_batch.shape}\")\n",
        "        print(f\"HR batch: {hr_batch.shape}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during setup: {str(e)}\")\n",
        "        import traceback\n",
        "        print(traceback.format_exc())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itdZVjsjPHqa"
      },
      "outputs": [],
      "source": [
        "# Load best model\n",
        "checkpoint = torch.load('best_model.pt')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# Visualize results\n",
        "visualize_results(model, val_loader, device, save_path='comparison_before_div2k.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrj28bwMPpet"
      },
      "outputs": [],
      "source": [
        "# Use models and classes we defined above\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load RCAN model\n",
        "print(\"Loading base RCAN model...\")\n",
        "base_model = load_rcan_model()\n",
        "base_model = base_model.to(device)\n",
        "\n",
        "# Create augmented model\n",
        "print(\"Creating attention-augmented model...\")\n",
        "model = AttentionAugmentedRCAN(base_model, freeze_base=True)\n",
        "model = model.to(device)\n",
        "\n",
        "# Setup data\n",
        "print(\"Setting up datasets...\")\n",
        "train_loader, val_loader = setup_datasets(batch_size=16)\n",
        "\n",
        "# Create trainer\n",
        "trainer = Trainer(model, train_loader, val_loader, device)\n",
        "\n",
        "# Training configuration\n",
        "config = {\n",
        "    'pre_train_epochs': 50,\n",
        "    'fine_tune_epochs': 100,\n",
        "    'pre_train_lr': 1e-4,\n",
        "    'fine_tune_lr': 1e-5,\n",
        "    'val_frequency': 5  # Validate every 5 epochs to save time\n",
        "}\n",
        "\n",
        "# Stage 1: Pre-training attention network\n",
        "print(\"\\nStage 1: Pre-training attention network\")\n",
        "attention_optimizer = torch.optim.Adam(\n",
        "    model.attention_net.parameters(),\n",
        "    lr=config['pre_train_lr']\n",
        ")\n",
        "\n",
        "for epoch in range(config['pre_train_epochs']):\n",
        "    model.train()\n",
        "    epoch_losses = []\n",
        "\n",
        "    with tqdm(train_loader, desc=f'Pre-training Epoch {epoch+1}/{config[\"pre_train_epochs\"]}') as pbar:\n",
        "        for lr_imgs, hr_imgs in pbar:\n",
        "            losses = trainer.pre_training_step((lr_imgs, hr_imgs), attention_optimizer)\n",
        "            epoch_losses.append(losses)\n",
        "            pbar.set_postfix({k: f'{v:.4f}' for k, v in losses.items()})\n",
        "\n",
        "    # Calculate average losses for epoch\n",
        "    avg_losses = {k: sum(d[k] for d in epoch_losses) / len(epoch_losses)\n",
        "                 for k in epoch_losses[0].keys()}\n",
        "\n",
        "    # Validation\n",
        "    if (epoch + 1) % config['val_frequency'] == 0:\n",
        "        print(f\"\\nEpoch {epoch+1} average losses:\")\n",
        "        for k, v in avg_losses.items():\n",
        "            print(f\"{k}: {v:.4f}\")\n",
        "\n",
        "        print(f\"\\nValidating epoch {epoch+1}...\")\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "        with torch.no_grad():\n",
        "            for lr_imgs, hr_imgs in val_loader:\n",
        "                lr_imgs, hr_imgs = lr_imgs.to(device), hr_imgs.to(device)\n",
        "                attention = model(lr_imgs, mode='pre_training')\n",
        "                if len(val_losses) == 0:\n",
        "                    print(f\"Sample attention map range: {attention.min().item():.4f} to {attention.max().item():.4f}\")\n",
        "\n",
        "# Stage 2: Fine-tuning\n",
        "print(\"\\nStage 2: Joint fine-tuning\")\n",
        "# Unfreeze RCAN\n",
        "for param in model.rcan.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=config['fine_tune_lr']\n",
        ")\n",
        "\n",
        "best_psnr = 0\n",
        "for epoch in range(config['fine_tune_epochs']):\n",
        "    model.train()\n",
        "    epoch_losses = []\n",
        "\n",
        "    with tqdm(train_loader, desc=f'Fine-tuning Epoch {epoch+1}/{config[\"fine_tune_epochs\"]}') as pbar:\n",
        "        for lr_imgs, hr_imgs in pbar:\n",
        "            losses = trainer.fine_tuning_step((lr_imgs, hr_imgs), optimizer)\n",
        "            epoch_losses.append(losses)\n",
        "            pbar.set_postfix({k: f'{v:.4f}' for k, v in losses.items()})\n",
        "\n",
        "    # Calculate average losses for epoch\n",
        "    avg_losses = {k: sum(d[k] for d in epoch_losses) / len(epoch_losses)\n",
        "                 for k in epoch_losses[0].keys()}\n",
        "\n",
        "    # Validation\n",
        "    if (epoch + 1) % config['val_frequency'] == 0:\n",
        "        print(f\"\\nEpoch {epoch+1} average losses:\")\n",
        "        for k, v in avg_losses.items():\n",
        "            print(f\"{k}: {v:.4f}\")\n",
        "\n",
        "        print(f\"\\nValidating epoch {epoch+1}...\")\n",
        "        model.eval()\n",
        "        psnr_values = []\n",
        "        with torch.no_grad():\n",
        "            for lr_imgs, hr_imgs in val_loader:\n",
        "                lr_imgs, hr_imgs = lr_imgs.to(device), hr_imgs.to(device)\n",
        "                sr_output, attention = model(lr_imgs)\n",
        "\n",
        "                # Calculate PSNR\n",
        "                mse = torch.mean((sr_output - hr_imgs) ** 2)\n",
        "                psnr = 20 * torch.log10(1.0 / torch.sqrt(mse))\n",
        "                psnr_values.append(psnr.item())\n",
        "\n",
        "        avg_psnr = sum(psnr_values) / len(psnr_values)\n",
        "        print(f\"Average PSNR: {avg_psnr:.2f}\")\n",
        "\n",
        "        # Save checkpoint\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'psnr': avg_psnr,\n",
        "        }, f'checkpoint_epoch_{epoch+1}.pt')\n",
        "\n",
        "        # Save best model\n",
        "        if avg_psnr > best_psnr:\n",
        "            best_psnr = avg_psnr\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'psnr': best_psnr,\n",
        "            }, 'best_model.pt')\n",
        "            print(f\"Saved new best model with PSNR {best_psnr:.2f}\")\n",
        "\n",
        "        # Visualize current results\n",
        "        visualize_results(model, val_loader, device, f'results_epoch_{epoch+1}.png')\n",
        "\n",
        "print(\"Training completed!\")\n",
        "print(f\"Best PSNR achieved: {best_psnr:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbpZZYGnmuR1"
      },
      "source": [
        "PSNR is still quite low, and we're seeing degradation in the RCAN output and a blank attention map. Have to diagnose the problem here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wisE5ErOnInB"
      },
      "outputs": [],
      "source": [
        "def check_attention_values(model, val_loader, device):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        lr_img, _ = next(iter(val_loader))\n",
        "        lr_img = lr_img.to(device)\n",
        "        _, attention = model(lr_img)\n",
        "        print(f\"Attention stats:\")\n",
        "        print(f\"Min: {attention.min().item():.6f}\")\n",
        "        print(f\"Max: {attention.max().item():.6f}\")\n",
        "        print(f\"Mean: {attention.mean().item():.6f}\")\n",
        "        print(f\"Std: {attention.std().item():.6f}\")\n",
        "\n",
        "# Load best model and check\n",
        "checkpoint = torch.load('best_model.pt')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "check_attention_values(model, val_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cjq1ftUnNOt"
      },
      "outputs": [],
      "source": [
        "def check_rcan_output(model, val_loader, device):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        lr_img, _ = next(iter(val_loader))\n",
        "        lr_img = lr_img.to(device)\n",
        "        output = model.rcan(lr_img)\n",
        "        print(f\"RCAN output stats:\")\n",
        "        print(f\"Min: {output.min().item():.6f}\")\n",
        "        print(f\"Max: {output.max().item():.6f}\")\n",
        "        print(f\"Mean: {output.mean().item():.6f}\")\n",
        "        print(f\"Std: {output.std().item():.6f}\")\n",
        "\n",
        "check_rcan_output(model, val_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raQ_4kD7nh3V"
      },
      "outputs": [],
      "source": [
        "def check_raw_rcan(device):\n",
        "    print(\"Loading fresh RCAN model...\")\n",
        "    base_rcan = load_rcan_model()\n",
        "    base_rcan = base_rcan.to(device)\n",
        "    base_rcan.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        lr_img, _ = next(iter(val_loader))\n",
        "        lr_img = lr_img.to(device)\n",
        "\n",
        "        # Get output directly from RCAN\n",
        "        output = base_rcan(lr_img)\n",
        "\n",
        "        print(\"\\nRaw RCAN output stats:\")\n",
        "        print(f\"Min: {output.min().item():.6f}\")\n",
        "        print(f\"Max: {output.max().item():.6f}\")\n",
        "        print(f\"Mean: {output.mean().item():.6f}\")\n",
        "        print(f\"Std: {output.std().item():.6f}\")\n",
        "\n",
        "        # Visualize without our wrapper\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.title('Raw RCAN Output')\n",
        "        img = output.cpu().squeeze(0).permute(1, 2, 0).clamp(0, 1).numpy()\n",
        "        plt.imshow(img)\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.title('Green Channel')\n",
        "        plt.imshow(img[:, :, 1], cmap='gray')\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "check_raw_rcan(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEkd4UX7n-6F"
      },
      "source": [
        "Things look okay. Pushed some changes to ensure stability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJhNb0B4oFqB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "class SelfSupervisedAttention(nn.Module):\n",
        "    \"\"\"Self-supervised auxiliary network for dynamic pixel importance prediction\"\"\"\n",
        "    def __init__(self, in_channels=64):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        # Spatial feature extraction\n",
        "        self.conv1 = nn.Conv2d(in_channels, in_channels, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
        "        self.conv2 = nn.Conv2d(in_channels, in_channels, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(in_channels)\n",
        "\n",
        "        # Attention prediction\n",
        "        self.conv3 = nn.Conv2d(in_channels, in_channels//2, 3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(in_channels//2)\n",
        "        self.conv4 = nn.Conv2d(in_channels//2, 1, 1)\n",
        "\n",
        "        # Channel attention\n",
        "        self.spatial_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.channel_attention = nn.Sequential(\n",
        "            nn.Linear(in_channels, in_channels//4),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(in_channels//4, in_channels),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initial feature extraction\n",
        "        feat = F.relu(self.bn1(self.conv1(x)))\n",
        "        feat = F.relu(self.bn2(self.conv2(feat)))\n",
        "\n",
        "        # Channel attention\n",
        "        channel_att = self.spatial_pool(x).squeeze(-1).squeeze(-1)\n",
        "        channel_att = self.channel_attention(channel_att)\n",
        "        channel_att = channel_att.view(-1, self.in_channels, 1, 1)\n",
        "\n",
        "        # Apply channel attention\n",
        "        feat = feat * channel_att\n",
        "\n",
        "        # Final attention prediction\n",
        "        feat = F.relu(self.bn3(self.conv3(feat)))\n",
        "        # Modified activation to ensure stronger attention values\n",
        "        attention = torch.sigmoid(self.conv4(feat)) * 2  # Scale to [0,2] range\n",
        "\n",
        "        return attention\n",
        "\n",
        "class RCANFeatureExtractor(nn.Module):\n",
        "    def __init__(self, rcan_model):\n",
        "        super().__init__()\n",
        "        self.rcan = rcan_model\n",
        "\n",
        "    def extract_features(self, x):\n",
        "        \"\"\"Extract features before the final upsampling\"\"\"\n",
        "        # Get initial features\n",
        "        x = self.rcan.head(x)\n",
        "        # Get body features\n",
        "        body_feat = self.rcan.body(x)\n",
        "        return x, body_feat\n",
        "\n",
        "    def complete_sr(self, features):\n",
        "        \"\"\"Complete super-resolution with extracted features\"\"\"\n",
        "        input_feat, body_feat = features\n",
        "        # Residual connection\n",
        "        x = body_feat + input_feat\n",
        "        # Apply tail (upsampling)\n",
        "        x = self.rcan.tail(x)\n",
        "        # Ensure output is in valid range\n",
        "        x = x.clamp(0, 1)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.extract_features(x)\n",
        "        return self.complete_sr(features)\n",
        "\n",
        "class AttentionAugmentedRCAN(nn.Module):\n",
        "    def __init__(self, base_rcan, freeze_base=True):\n",
        "        super().__init__()\n",
        "        self.rcan = base_rcan\n",
        "        self.attention_net = SelfSupervisedAttention(64)\n",
        "\n",
        "        if freeze_base:\n",
        "            for param in self.rcan.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, x, mode='inference'):\n",
        "        if mode == 'pre_training':\n",
        "            feats = self.rcan.extract_features(x)\n",
        "            attention = self.attention_net(feats[1])\n",
        "            return attention\n",
        "\n",
        "        # Normal forward pass with attention\n",
        "        input_feat, body_feat = self.rcan.extract_features(x)\n",
        "        attention = self.attention_net(body_feat)\n",
        "        weighted_feat = body_feat * attention\n",
        "\n",
        "        # Complete super-resolution\n",
        "        sr_output = self.rcan.complete_sr((input_feat, weighted_feat))\n",
        "        return sr_output, attention\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model, train_loader, val_loader, device):\n",
        "        self.model = model\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.device = device\n",
        "        self.writer = SummaryWriter('runs/attention_rcan')\n",
        "\n",
        "    def pre_training_step(self, batch, optimizer):\n",
        "        lr_imgs, hr_imgs = [x.to(self.device) for x in batch]\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Get base RCAN output for difficulty estimation\n",
        "        with torch.no_grad():\n",
        "            sr_output = self.model.rcan(lr_imgs)\n",
        "            difficulty = get_reconstruction_difficulty(sr_output, hr_imgs)\n",
        "\n",
        "        # Train attention network\n",
        "        attention = self.model(lr_imgs, mode='pre_training')\n",
        "\n",
        "        # Upscale attention to match HR resolution\n",
        "        attention_upscaled = F.interpolate(\n",
        "            attention,\n",
        "            size=difficulty.shape[-2:],\n",
        "            mode='bilinear',\n",
        "            align_corners=False\n",
        "        )\n",
        "\n",
        "        # Losses\n",
        "        prediction_loss = F.mse_loss(attention_upscaled, difficulty)\n",
        "        entropy_reg = entropy_loss(attention)\n",
        "        consistency = spatial_consistency_loss(attention)\n",
        "\n",
        "        total_loss = prediction_loss + 0.1 * entropy_reg + 0.1 * consistency\n",
        "\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        return {\n",
        "            'prediction_loss': prediction_loss.item(),\n",
        "            'entropy_loss': entropy_reg.item(),\n",
        "            'consistency_loss': consistency.item(),\n",
        "            'total_loss': total_loss.item()\n",
        "        }\n",
        "\n",
        "    def fine_tuning_step(self, batch, optimizer):\n",
        "        lr_imgs, hr_imgs = [x.to(self.device) for x in batch]\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass with attention\n",
        "        sr_output, attention = self.model(lr_imgs)\n",
        "\n",
        "        # Calculate losses\n",
        "        reconstruction_loss = F.l1_loss(sr_output, hr_imgs)\n",
        "        entropy_reg = entropy_loss(attention)\n",
        "        consistency = spatial_consistency_loss(attention)\n",
        "\n",
        "        # Get current reconstruction difficulty\n",
        "        difficulty = get_reconstruction_difficulty(sr_output, hr_imgs)\n",
        "\n",
        "        # Upscale attention to match HR resolution\n",
        "        attention_upscaled = F.interpolate(\n",
        "            attention,\n",
        "            size=difficulty.shape[-2:],\n",
        "            mode='bilinear',\n",
        "            align_corners=False\n",
        "        )\n",
        "        attention_guidance = F.mse_loss(attention_upscaled, difficulty.detach())\n",
        "\n",
        "        total_loss = reconstruction_loss + 0.1 * entropy_reg + 0.1 * consistency + 0.1 * attention_guidance\n",
        "\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        return {\n",
        "            'reconstruction_loss': reconstruction_loss.item(),\n",
        "            'entropy_loss': entropy_reg.item(),\n",
        "            'consistency_loss': consistency.item(),\n",
        "            'attention_guidance': attention_guidance.item(),\n",
        "            'total_loss': total_loss.item()\n",
        "        }\n",
        "\n",
        "def entropy_loss(attention_maps):\n",
        "    \"\"\"Entropy-based regularization for attention maps\"\"\"\n",
        "    eps = 1e-8\n",
        "    entropy = -(attention_maps * torch.log(attention_maps + eps))\n",
        "    return entropy.mean()\n",
        "\n",
        "def spatial_consistency_loss(attention_maps):\n",
        "    \"\"\"Spatial consistency loss for attention maps\"\"\"\n",
        "    horizontal = F.l1_loss(attention_maps[..., :, 1:], attention_maps[..., :, :-1])\n",
        "    vertical = F.l1_loss(attention_maps[..., 1:, :], attention_maps[..., :-1, :])\n",
        "    return horizontal + vertical\n",
        "\n",
        "def get_reconstruction_difficulty(sr_output, hr_target):\n",
        "    \"\"\"Calculate pixel-wise reconstruction difficulty\"\"\"\n",
        "    with torch.no_grad():\n",
        "        diff = torch.abs(sr_output - hr_target)\n",
        "        # Normalize to [0, 1] range\n",
        "        diff = (diff - diff.min()) / (diff.max() - diff.min() + 1e-8)\n",
        "        return diff.mean(dim=1, keepdim=True)\n",
        "\n",
        "def visualize_results(model, val_loader, device, save_path=None):\n",
        "    \"\"\"Visualize and compare results\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        lr_img, hr_img = next(iter(val_loader))\n",
        "        lr_img, hr_img = lr_img.to(device), hr_img.to(device)\n",
        "\n",
        "        # Get outputs\n",
        "        bicubic = F.interpolate(lr_img, scale_factor=4, mode='bicubic', align_corners=False)\n",
        "        rcan_output = model.rcan(lr_img)\n",
        "        sr_output, attention = model(lr_img)\n",
        "\n",
        "        # Normalize attention for visualization\n",
        "        attention = (attention - attention.min()) / (attention.max() - attention.min() + 1e-8)\n",
        "\n",
        "        # Convert to images\n",
        "        def tensor_to_image(x):\n",
        "            x = x.clamp(0, 1)\n",
        "            x = x.cpu().squeeze(0).permute(1, 2, 0).numpy()\n",
        "            return x\n",
        "\n",
        "        # Plot\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "\n",
        "        axes[0, 0].imshow(tensor_to_image(lr_img))\n",
        "        axes[0, 0].set_title('LR Input')\n",
        "        axes[0, 1].imshow(tensor_to_image(bicubic))\n",
        "        axes[0, 1].set_title('Bicubic')\n",
        "        axes[0, 2].imshow(tensor_to_image(hr_img))\n",
        "        axes[0, 2].set_title('HR Ground Truth')\n",
        "        axes[1, 0].imshow(tensor_to_image(rcan_output))\n",
        "        axes[1, 0].set_title('RCAN Output')\n",
        "        axes[1, 1].imshow(tensor_to_image(sr_output))\n",
        "        axes[1, 1].set_title('Our Model Output')\n",
        "        axes[1, 2].imshow(tensor_to_image(attention.repeat(1, 3, 1, 1)))\n",
        "        axes[1, 2].set_title('Attention Map')\n",
        "\n",
        "        for ax in axes.flat:\n",
        "            ax.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        if save_path:\n",
        "            plt.savefig(save_path)\n",
        "        plt.show()\n",
        "\n",
        "# Training configuration\n",
        "config = {\n",
        "    'pre_train_epochs': 50,\n",
        "    'fine_tune_epochs': 100,\n",
        "    'pre_train_lr': 1e-4,\n",
        "    'fine_tune_lr': 1e-5,\n",
        "    'val_frequency': 5\n",
        "}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Setup\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load RCAN model\n",
        "    print(\"Loading base RCAN model...\")\n",
        "    base_model = load_rcan_model()\n",
        "    base_model = base_model.to(device)\n",
        "\n",
        "    # Create augmented model\n",
        "    print(\"Creating attention-augmented model...\")\n",
        "    model = AttentionAugmentedRCAN(base_model, freeze_base=True)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Setup data\n",
        "    print(\"Setting up datasets...\")\n",
        "    train_loader, val_loader = setup_datasets(batch_size=16)\n",
        "\n",
        "    # Create trainer\n",
        "    trainer = Trainer(model, train_loader, val_loader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rK6L0fRKoY7g"
      },
      "source": [
        "Let's check that our baseline RCAN implementation works properly by comparing it to the official RCAN repo results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "angTdCcMoeY1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from math import log10\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import cv2\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "\n",
        "def calc_psnr(sr, hr):\n",
        "    \"\"\"Calculate PSNR (Peak Signal-to-Noise Ratio)\"\"\"\n",
        "    # Convert to numpy arrays in range [0, 255]\n",
        "    sr = sr.mul(255).clamp(0, 255).round().cpu().numpy().transpose(1, 2, 0)\n",
        "    hr = hr.mul(255).clamp(0, 255).round().cpu().numpy().transpose(1, 2, 0)\n",
        "\n",
        "    # Calculate MSE\n",
        "    mse = np.mean((sr - hr) ** 2)\n",
        "    if mse == 0:\n",
        "        return 100\n",
        "\n",
        "    # Calculate PSNR\n",
        "    psnr = 20 * log10(255.0 / np.sqrt(mse))\n",
        "    return psnr\n",
        "\n",
        "def calc_ssim(sr, hr):\n",
        "    \"\"\"Calculate SSIM (Structural Similarity Index)\"\"\"\n",
        "    # Convert to numpy arrays in range [0, 255]\n",
        "    sr = sr.mul(255).clamp(0, 255).round().cpu().numpy().transpose(1, 2, 0)\n",
        "    hr = hr.mul(255).clamp(0, 255).round().cpu().numpy().transpose(1, 2, 0)\n",
        "\n",
        "    return ssim(sr, hr, channel_axis=2, data_range=255)\n",
        "\n",
        "def evaluate_rcan():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load RCAN model\n",
        "    print(\"Loading RCAN model...\")\n",
        "    model = load_rcan_model()\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Setup validation data\n",
        "    _, val_loader = setup_datasets(batch_size=1)  # Using Set14\n",
        "\n",
        "    # Evaluation metrics\n",
        "    psnr_values = []\n",
        "    ssim_values = []\n",
        "\n",
        "    print(\"\\nEvaluating RCAN on Set14...\")\n",
        "    with torch.no_grad():\n",
        "        for i, (lr_imgs, hr_imgs) in enumerate(val_loader):\n",
        "            lr_imgs = lr_imgs.to(device)\n",
        "            hr_imgs = hr_imgs.to(device)\n",
        "\n",
        "            # Get RCAN output\n",
        "            sr_output = model(lr_imgs)\n",
        "\n",
        "            # Calculate metrics for each image\n",
        "            for j in range(len(lr_imgs)):\n",
        "                psnr = calc_psnr(sr_output[j], hr_imgs[j])\n",
        "                ssim_val = calc_ssim(sr_output[j], hr_imgs[j])\n",
        "\n",
        "                psnr_values.append(psnr)\n",
        "                ssim_values.append(ssim_val)\n",
        "\n",
        "                print(f\"Image {i+1}: PSNR: {psnr:.2f} dB, SSIM: {ssim_val:.4f}\")\n",
        "\n",
        "    # Calculate average metrics\n",
        "    avg_psnr = sum(psnr_values) / len(psnr_values)\n",
        "    avg_ssim = sum(ssim_values) / len(ssim_values)\n",
        "\n",
        "    print(\"\\nOverall Results on Set14:\")\n",
        "    print(f\"Average PSNR: {avg_psnr:.2f} dB\")\n",
        "    print(f\"Average SSIM: {avg_ssim:.4f}\")\n",
        "\n",
        "    # Reference results from RCAN paper on Set14 (x4):\n",
        "    print(\"\\nReference Results from RCAN paper:\")\n",
        "    print(\"PSNR: 28.87 dB\")\n",
        "    print(\"SSIM: 0.7889\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    evaluate_rcan()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIXUqar7ougE"
      },
      "source": [
        "Performance is far below what it should be compared to our reference, so we'll need to debug."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNSioHevrA-3"
      },
      "outputs": [],
      "source": [
        "def check_image_ranges():\n",
        "    _, val_loader = setup_datasets(batch_size=1)\n",
        "    for lr_imgs, hr_imgs in val_loader:\n",
        "        print(f\"LR range: [{lr_imgs.min():.4f}, {lr_imgs.max():.4f}]\")\n",
        "        print(f\"HR range: [{hr_imgs.min():.4f}, {hr_imgs.max():.4f}]\")\n",
        "        break\n",
        "\n",
        "def print_rcan_args():\n",
        "    # Print the args we're using for RCAN\n",
        "    args = {\n",
        "        'n_resgroups': 10,\n",
        "        'n_resblocks': 20,\n",
        "        'n_feats': 64,\n",
        "        'scale': [4],\n",
        "        'rgb_range': 255,  # This might be the issue\n",
        "        'n_colors': 3,\n",
        "        'res_scale': 1,\n",
        "        'reduction': 16\n",
        "    }\n",
        "    print(\"Current RCAN args:\")\n",
        "    for k, v in args.items():\n",
        "        print(f\"{k}: {v}\")\n",
        "\n",
        "def check_model_output_range():\n",
        "    device = torch.device('cuda')\n",
        "    model = load_rcan_model().to(device)\n",
        "    model.eval()\n",
        "\n",
        "    _, val_loader = setup_datasets(batch_size=1)\n",
        "    with torch.no_grad():\n",
        "        lr_imgs, _ = next(iter(val_loader))\n",
        "        lr_imgs = lr_imgs.to(device)\n",
        "        output = model(lr_imgs)\n",
        "        print(f\"RCAN output range: [{output.min():.4f}, {output.max():.4f}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86fCva7NrLcz"
      },
      "outputs": [],
      "source": [
        "check_image_ranges()\n",
        "print_rcan_args()\n",
        "check_model_output_range()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7--CPIkVsIzz"
      },
      "source": [
        "RGB range is set to 255 in the RCAN args, but we're feeding in 0,1 images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_muiCUGsMNb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from math import log10\n",
        "import glob\n",
        "import os\n",
        "from google.colab import drive\n",
        "from tqdm import tqdm\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "\n",
        "class SRDataset(Dataset):\n",
        "    def __init__(self, root_dir, scale=4, patch_size=96, train=True):\n",
        "        self.scale = scale\n",
        "        self.patch_size = patch_size\n",
        "        self.train = train\n",
        "\n",
        "        # Mount Google Drive if not already mounted\n",
        "        if not os.path.exists('/content/drive'):\n",
        "            drive.mount('/content/drive')\n",
        "\n",
        "        # Find all images in the directory\n",
        "        self.image_files = sorted(glob.glob(os.path.join(root_dir, '*.png')))\n",
        "        if len(self.image_files) == 0:\n",
        "            raise RuntimeError(f\"No PNG images found in {root_dir}\")\n",
        "\n",
        "        print(f\"Found {len(self.image_files)} images in {root_dir}\")\n",
        "\n",
        "        # Basic augmentations for training\n",
        "        self.augment = transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomVerticalFlip()\n",
        "        ]) if train else None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load HR image\n",
        "        img_path = self.image_files[idx]\n",
        "        hr_image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        # Handle training vs evaluation\n",
        "        if self.train:\n",
        "            # Random crop for training\n",
        "            i, j, h, w = transforms.RandomCrop.get_params(\n",
        "                hr_image, output_size=(self.patch_size, self.patch_size))\n",
        "            hr_image = TF.crop(hr_image, i, j, h, w)\n",
        "\n",
        "            # Apply augmentations\n",
        "            if self.augment:\n",
        "                hr_image = self.augment(hr_image)\n",
        "        else:\n",
        "            # For validation, ensure dimensions are divisible by scale\n",
        "            w, h = hr_image.size\n",
        "            w = w - w % self.scale\n",
        "            h = h - h % self.scale\n",
        "            hr_image = hr_image.crop((0, 0, w, h))\n",
        "\n",
        "        # Convert to tensor and scale to [0, 255]\n",
        "        hr_tensor = TF.to_tensor(hr_image) * 255.0\n",
        "\n",
        "        # Create LR image using bicubic downsampling\n",
        "        lr_tensor = TF.resize(\n",
        "            hr_tensor,\n",
        "            size=[s // self.scale for s in hr_tensor.shape[-2:]],\n",
        "            interpolation=TF.InterpolationMode.BICUBIC\n",
        "        )\n",
        "\n",
        "        return lr_tensor, hr_tensor\n",
        "\n",
        "def setup_datasets(batch_size=16):\n",
        "    # Create datasets\n",
        "    train_dataset = SRDataset(\n",
        "        root_dir='DIV2K_train_HR',\n",
        "        scale=4,\n",
        "        patch_size=96,\n",
        "        train=True\n",
        "    )\n",
        "\n",
        "    val_dataset = SRDataset(\n",
        "        root_dir='/content/drive/MyDrive/E82/finalproject/Set14',\n",
        "        scale=4,\n",
        "        patch_size=96,\n",
        "        train=False\n",
        "    )\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=1,\n",
        "        shuffle=False,\n",
        "        num_workers=1,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "def calc_psnr(sr, hr):\n",
        "    \"\"\"Calculate PSNR (Peak Signal-to-Noise Ratio)\"\"\"\n",
        "    # Convert to numpy arrays (values already in range [0, 255])\n",
        "    sr = sr.clamp(0, 255).round().cpu().numpy().transpose(1, 2, 0)\n",
        "    hr = hr.clamp(0, 255).round().cpu().numpy().transpose(1, 2, 0)\n",
        "\n",
        "    # Calculate MSE\n",
        "    mse = np.mean((sr - hr) ** 2)\n",
        "    if mse == 0:\n",
        "        return 100\n",
        "\n",
        "    # Calculate PSNR\n",
        "    psnr = 20 * log10(255.0 / np.sqrt(mse))\n",
        "    return psnr\n",
        "\n",
        "def calc_ssim(sr, hr):\n",
        "    \"\"\"Calculate SSIM (Structural Similarity Index)\"\"\"\n",
        "    # Convert to numpy arrays (values already in range [0, 255])\n",
        "    sr = sr.clamp(0, 255).round().cpu().numpy().transpose(1, 2, 0)\n",
        "    hr = hr.clamp(0, 255).round().cpu().numpy().transpose(1, 2, 0)\n",
        "\n",
        "    return ssim(sr, hr, channel_axis=2, data_range=255)\n",
        "\n",
        "def evaluate_rcan():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Print RCAN configuration\n",
        "    args = {\n",
        "        'n_resgroups': 10,\n",
        "        'n_resblocks': 20,\n",
        "        'n_feats': 64,\n",
        "        'scale': [4],\n",
        "        'rgb_range': 255,\n",
        "        'n_colors': 3,\n",
        "        'res_scale': 1,\n",
        "        'reduction': 16\n",
        "    }\n",
        "    print(\"\\nRCAN Configuration:\")\n",
        "    for k, v in args.items():\n",
        "        print(f\"{k}: {v}\")\n",
        "\n",
        "    # Load RCAN model\n",
        "    print(\"\\nLoading RCAN model...\")\n",
        "    model = load_rcan_model()\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Setup validation data\n",
        "    _, val_loader = setup_datasets(batch_size=1)\n",
        "\n",
        "    # Check data ranges\n",
        "    print(\"\\nChecking data ranges...\")\n",
        "    lr_imgs, hr_imgs = next(iter(val_loader))\n",
        "    print(f\"LR range: [{lr_imgs.min():.4f}, {lr_imgs.max():.4f}]\")\n",
        "    print(f\"HR range: [{hr_imgs.min():.4f}, {hr_imgs.max():.4f}]\")\n",
        "\n",
        "    # Evaluation metrics\n",
        "    psnr_values = []\n",
        "    ssim_values = []\n",
        "\n",
        "    print(\"\\nEvaluating RCAN on Set14...\")\n",
        "    with torch.no_grad():\n",
        "        for i, (lr_imgs, hr_imgs) in enumerate(val_loader):\n",
        "            lr_imgs = lr_imgs.to(device)\n",
        "            hr_imgs = hr_imgs.to(device)\n",
        "\n",
        "            # Get RCAN output\n",
        "            sr_output = model(lr_imgs)\n",
        "\n",
        "            # Check output range for first image\n",
        "            if i == 0:\n",
        "                print(f\"SR output range: [{sr_output.min():.4f}, {sr_output.max():.4f}]\")\n",
        "\n",
        "            # Calculate metrics for each image\n",
        "            for j in range(len(lr_imgs)):\n",
        "                psnr = calc_psnr(sr_output[j], hr_imgs[j])\n",
        "                ssim_val = calc_ssim(sr_output[j], hr_imgs[j])\n",
        "\n",
        "                psnr_values.append(psnr)\n",
        "                ssim_values.append(ssim_val)\n",
        "\n",
        "                print(f\"Image {i+1}: PSNR: {psnr:.2f} dB, SSIM: {ssim_val:.4f}\")\n",
        "\n",
        "    # Calculate average metrics\n",
        "    avg_psnr = sum(psnr_values) / len(psnr_values)\n",
        "    avg_ssim = sum(ssim_values) / len(ssim_values)\n",
        "\n",
        "    print(\"\\nOverall Results on Set14:\")\n",
        "    print(f\"Average PSNR: {avg_psnr:.2f} dB\")\n",
        "    print(f\"Average SSIM: {avg_ssim:.4f}\")\n",
        "\n",
        "    # Reference results from RCAN paper\n",
        "    print(\"\\nReference Results from RCAN paper:\")\n",
        "    print(\"PSNR: 28.87 dB\")\n",
        "    print(\"SSIM: 0.7889\")\n",
        "\n",
        "    # Calculate difference\n",
        "    print(\"\\nDifference from paper results:\")\n",
        "    print(f\"PSNR diff: {28.87 - avg_psnr:.2f} dB\")\n",
        "    print(f\"SSIM diff: {0.7889 - avg_ssim:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    evaluate_rcan()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjcdyRSNs3K9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from math import log10\n",
        "import glob\n",
        "import os\n",
        "from google.colab import drive\n",
        "from tqdm import tqdm\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "\n",
        "class SRDataset(Dataset):\n",
        "    def __init__(self, root_dir, scale=4, patch_size=96, train=True):\n",
        "        self.scale = scale\n",
        "        self.patch_size = patch_size\n",
        "        self.train = train\n",
        "\n",
        "        # Mount Google Drive if not already mounted\n",
        "        if not os.path.exists('/content/drive'):\n",
        "            drive.mount('/content/drive')\n",
        "\n",
        "        # Find all images in the directory\n",
        "        self.image_files = sorted(glob.glob(os.path.join(root_dir, '*.png')))\n",
        "        if len(self.image_files) == 0:\n",
        "            raise RuntimeError(f\"No PNG images found in {root_dir}\")\n",
        "\n",
        "        print(f\"Found {len(self.image_files)} images in {root_dir}\")\n",
        "\n",
        "        # Basic augmentations for training\n",
        "        self.augment = transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomVerticalFlip()\n",
        "        ]) if train else None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load HR image\n",
        "        img_path = self.image_files[idx]\n",
        "        hr_image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        # Handle training vs evaluation\n",
        "        if self.train:\n",
        "            # Random crop for training\n",
        "            i, j, h, w = transforms.RandomCrop.get_params(\n",
        "                hr_image, output_size=(self.patch_size, self.patch_size))\n",
        "            hr_image = TF.crop(hr_image, i, j, h, w)\n",
        "\n",
        "            # Apply augmentations\n",
        "            if self.augment:\n",
        "                hr_image = self.augment(hr_image)\n",
        "        else:\n",
        "            # For validation, ensure dimensions are divisible by scale\n",
        "            w, h = hr_image.size\n",
        "            w = w - w % self.scale\n",
        "            h = h - h % self.scale\n",
        "            hr_image = hr_image.crop((0, 0, w, h))\n",
        "\n",
        "        # Convert to tensor keeping in [0,255] range for metrics\n",
        "        hr_tensor = TF.to_tensor(hr_image) * 255.0\n",
        "\n",
        "        # Create LR image using bicubic downsampling\n",
        "        lr_tensor = TF.resize(\n",
        "            hr_tensor,\n",
        "            size=[s // self.scale for s in hr_tensor.shape[-2:]],\n",
        "            interpolation=TF.InterpolationMode.BICUBIC\n",
        "        )\n",
        "\n",
        "        return lr_tensor, hr_tensor\n",
        "\n",
        "def setup_datasets(batch_size=16):\n",
        "    \"\"\"Setup training and validation dataloaders\"\"\"\n",
        "    # Create datasets\n",
        "    train_dataset = SRDataset(\n",
        "        root_dir='DIV2K_train_HR',\n",
        "        scale=4,\n",
        "        patch_size=96,\n",
        "        train=True\n",
        "    )\n",
        "\n",
        "    val_dataset = SRDataset(\n",
        "        root_dir='/content/drive/MyDrive/E82/finalproject/Set14',\n",
        "        scale=4,\n",
        "        patch_size=96,\n",
        "        train=False\n",
        "    )\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=1,\n",
        "        shuffle=False,\n",
        "        num_workers=1,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "def calc_psnr(sr, hr):\n",
        "    \"\"\"Calculate PSNR (Peak Signal-to-Noise Ratio)\"\"\"\n",
        "    # Values should already be in range [0, 255]\n",
        "    sr = sr.clamp(0, 255).round().cpu().numpy().transpose(1, 2, 0)\n",
        "    hr = hr.clamp(0, 255).round().cpu().numpy().transpose(1, 2, 0)\n",
        "\n",
        "    # Calculate MSE\n",
        "    mse = np.mean((sr - hr) ** 2)\n",
        "    if mse == 0:\n",
        "        return 100\n",
        "\n",
        "    # Calculate PSNR\n",
        "    psnr = 20 * log10(255.0 / np.sqrt(mse))\n",
        "    return psnr\n",
        "\n",
        "def calc_ssim(sr, hr):\n",
        "    \"\"\"Calculate SSIM (Structural Similarity Index)\"\"\"\n",
        "    # Values should already be in range [0, 255]\n",
        "    sr = sr.clamp(0, 255).round().cpu().numpy().transpose(1, 2, 0)\n",
        "    hr = hr.clamp(0, 255).round().cpu().numpy().transpose(1, 2, 0)\n",
        "\n",
        "    return ssim(sr, hr, channel_axis=2, data_range=255)\n",
        "\n",
        "def evaluate_rcan():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Print RCAN configuration\n",
        "    args = {\n",
        "        'n_resgroups': 10,\n",
        "        'n_resblocks': 20,\n",
        "        'n_feats': 64,\n",
        "        'scale': [4],\n",
        "        'rgb_range': 255,\n",
        "        'n_colors': 3,\n",
        "        'res_scale': 1,\n",
        "        'reduction': 16\n",
        "    }\n",
        "    print(\"\\nRCAN Configuration:\")\n",
        "    for k, v in args.items():\n",
        "        print(f\"{k}: {v}\")\n",
        "\n",
        "    # Load RCAN model\n",
        "    print(\"\\nLoading RCAN model...\")\n",
        "    model = load_rcan_model()\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Setup validation data\n",
        "    _, val_loader = setup_datasets(batch_size=1)\n",
        "\n",
        "    # Evaluation metrics\n",
        "    psnr_values = []\n",
        "    ssim_values = []\n",
        "\n",
        "    print(\"\\nEvaluating RCAN on Set14...\")\n",
        "    with torch.no_grad():\n",
        "        for i, (lr_imgs, hr_imgs) in enumerate(val_loader):\n",
        "            # Move to device and normalize LR input to [0,1] range\n",
        "            lr_imgs = lr_imgs.to(device) / 255.0\n",
        "            hr_imgs = hr_imgs.to(device)  # Keep HR in [0,255] for metrics\n",
        "\n",
        "            # Get RCAN output and scale back to [0,255] range\n",
        "            sr_output = model(lr_imgs) * 255.0\n",
        "\n",
        "            # Print ranges for first image\n",
        "            if i == 0:\n",
        "                print(f\"\\nFirst image ranges:\")\n",
        "                print(f\"LR input range (after norm): [{lr_imgs.min():.4f}, {lr_imgs.max():.4f}]\")\n",
        "                print(f\"SR output range (after scale): [{sr_output.min():.4f}, {sr_output.max():.4f}]\")\n",
        "                print(f\"HR target range: [{hr_imgs.min():.4f}, {hr_imgs.max():.4f}]\")\n",
        "\n",
        "            # Calculate metrics\n",
        "            psnr = calc_psnr(sr_output[0], hr_imgs[0])\n",
        "            ssim_val = calc_ssim(sr_output[0], hr_imgs[0])\n",
        "\n",
        "            psnr_values.append(psnr)\n",
        "            ssim_values.append(ssim_val)\n",
        "            print(f\"Image {i+1}: PSNR: {psnr:.2f} dB, SSIM: {ssim_val:.4f}\")\n",
        "\n",
        "    # Calculate average metrics\n",
        "    avg_psnr = sum(psnr_values) / len(psnr_values)\n",
        "    avg_ssim = sum(ssim_values) / len(ssim_values)\n",
        "\n",
        "    print(\"\\nOverall Results on Set14:\")\n",
        "    print(f\"Average PSNR: {avg_psnr:.2f} dB\")\n",
        "    print(f\"Average SSIM: {avg_ssim:.4f}\")\n",
        "\n",
        "    # Reference results from RCAN paper\n",
        "    print(\"\\nReference Results from RCAN paper:\")\n",
        "    print(\"PSNR: 28.87 dB\")\n",
        "    print(\"SSIM: 0.7889\")\n",
        "\n",
        "    # Calculate difference\n",
        "    print(\"\\nDifference from paper results:\")\n",
        "    print(f\"PSNR diff: {28.87 - avg_psnr:.2f} dB\")\n",
        "    print(f\"SSIM diff: {0.7889 - avg_ssim:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    evaluate_rcan()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNaZdFPZtJFz"
      },
      "outputs": [],
      "source": [
        "def evaluate_rcan_y_channel():\n",
        "    \"\"\"Evaluate RCAN using only Y channel (as done in paper)\"\"\"\n",
        "    device = torch.device('cuda')\n",
        "    model = load_rcan_model().to(device)\n",
        "    model.eval()\n",
        "\n",
        "    def rgb_to_y(img):\n",
        "        \"\"\"Convert RGB to Y channel (BT.709)\"\"\"\n",
        "        if len(img.shape) == 4:\n",
        "            img = img.squeeze(0)\n",
        "        r, g, b = img[0], img[1], img[2]\n",
        "        y = 0.2126 * r + 0.7152 * g + 0.0722 * b\n",
        "        return y.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    _, val_loader = setup_datasets(batch_size=1)\n",
        "    psnr_values = []\n",
        "\n",
        "    print(\"\\nEvaluating RCAN on Set14 (Y channel):\")\n",
        "    with torch.no_grad():\n",
        "        for i, (lr_imgs, hr_imgs) in enumerate(val_loader):\n",
        "            lr_imgs = lr_imgs.to(device) / 255.0\n",
        "            hr_imgs = hr_imgs.to(device)  # Keep in [0,255]\n",
        "\n",
        "            sr_output = model(lr_imgs) * 255.0\n",
        "\n",
        "            # Convert to Y channel\n",
        "            sr_y = rgb_to_y(sr_output[0])\n",
        "            hr_y = rgb_to_y(hr_imgs[0])\n",
        "\n",
        "            # Calculate PSNR on Y channel\n",
        "            mse = torch.mean((sr_y - hr_y) ** 2)\n",
        "            psnr = 20 * torch.log10(255.0 / torch.sqrt(mse))\n",
        "            psnr_values.append(psnr.item())\n",
        "\n",
        "            print(f\"Image {i+1} Y-PSNR: {psnr:.2f} dB\")\n",
        "\n",
        "    avg_psnr = sum(psnr_values) / len(psnr_values)\n",
        "    print(f\"\\nAverage Y-PSNR: {avg_psnr:.2f} dB\")\n",
        "\n",
        "evaluate_rcan_y_channel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6TRmNBpAuz2"
      },
      "outputs": [],
      "source": [
        "def get_official_weights():\n",
        "    print(\"Downloading official RCAN weights...\")\n",
        "    !wget https://www.dropbox.com/s/mjbcqkd4nwhr6nu/models_ECCV2018RCAN.zip\n",
        "    !unzip models_ECCV2018RCAN.zip\n",
        "    return './models_ECCV2018RCAN/RCAN_BIX4.pt'  # Path to official weights\n",
        "\n",
        "# Use these weights instead\n",
        "model_path = get_official_weights()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JerNoaitMA3s"
      },
      "source": [
        "Fixed the RCAN implementation. Hopefully."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxKs3zTsMDNL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import math\n",
        "from math import log10\n",
        "import glob\n",
        "import os\n",
        "from google.colab import drive\n",
        "from skimage.color import rgb2ycbcr\n",
        "\n",
        "class MeanShift(nn.Conv2d):\n",
        "    def __init__(self, rgb_range, rgb_mean, rgb_std, sign=-1):\n",
        "        super(MeanShift, self).__init__(3, 3, kernel_size=1)\n",
        "        std = torch.Tensor(rgb_std)\n",
        "        self.weight.data = torch.eye(3).view(3, 3, 1, 1)\n",
        "        self.weight.data.div_(std.view(3, 1, 1, 1))\n",
        "        self.bias.data = sign * rgb_range * torch.Tensor(rgb_mean)\n",
        "        self.bias.data.div_(std)\n",
        "        self.requires_grad = False\n",
        "\n",
        "class SRDataset(Dataset):\n",
        "    def __init__(self, root_dir, scale=4, train=False):\n",
        "        self.scale = scale\n",
        "\n",
        "        # Find all images\n",
        "        self.image_files = sorted(glob.glob(os.path.join(root_dir, '*.png')))\n",
        "        if len(self.image_files) == 0:\n",
        "            raise RuntimeError(f\"No PNG images found in {root_dir}\")\n",
        "\n",
        "        print(f\"Found {len(self.image_files)} images in {root_dir}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load HR image\n",
        "        img_path = self.image_files[idx]\n",
        "        hr_image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        # Ensure dimensions are divisible by scale\n",
        "        w, h = hr_image.size\n",
        "        w = w - w % self.scale\n",
        "        h = h - h % self.scale\n",
        "        hr_image = hr_image.crop((0, 0, w, h))\n",
        "\n",
        "        # Convert to tensor (keeping in [0, 255] initially)\n",
        "        hr_tensor = TF.to_tensor(hr_image) * 255.0\n",
        "\n",
        "        # Create LR using bicubic downsampling\n",
        "        lr_tensor = TF.resize(\n",
        "            hr_tensor,\n",
        "            size=[s // self.scale for s in hr_tensor.shape[-2:]],\n",
        "            interpolation=TF.InterpolationMode.BICUBIC\n",
        "        )\n",
        "\n",
        "        return lr_tensor, hr_tensor\n",
        "\n",
        "def calc_psnr(sr, hr):\n",
        "    \"\"\"Calculate PSNR for Y channel\"\"\"\n",
        "    def convert_rgb_to_y(img):\n",
        "        if type(img) == np.ndarray:\n",
        "            y = 16. + (64.738 * img[:, :, 0] + 129.057 * img[:, :, 1] + 25.064 * img[:, :, 2]) / 256.\n",
        "            return y\n",
        "        elif type(img) == torch.Tensor:\n",
        "            if len(img.shape) == 4:\n",
        "                img = img.squeeze(0)\n",
        "            img = img.permute(1, 2, 0).cpu().numpy()\n",
        "            y = 16. + (64.738 * img[:, :, 0] + 129.057 * img[:, :, 1] + 25.064 * img[:, :, 2]) / 256.\n",
        "            return y\n",
        "        else:\n",
        "            raise Exception('Unknown Type', type(img))\n",
        "\n",
        "    sr = convert_rgb_to_y(sr)\n",
        "    hr = convert_rgb_to_y(hr)\n",
        "\n",
        "    diff = (sr - hr) / 255.0\n",
        "    mse = np.mean(diff ** 2)\n",
        "    if mse == 0:\n",
        "        return 100\n",
        "    return -10 * log10(mse)\n",
        "\n",
        "def evaluate_rcan():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Setup mean shift layers\n",
        "    rgb_mean = (0.4488, 0.4371, 0.4040)\n",
        "    rgb_std = (1.0, 1.0, 1.0)\n",
        "    rgb_range = 255\n",
        "\n",
        "    # Load RCAN model\n",
        "    print(\"Loading RCAN model...\")\n",
        "    model = load_rcan_model()\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Setup validation data\n",
        "    val_dataset = SRDataset('/content/drive/MyDrive/E82/finalproject/Set14', scale=4)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "    psnr_values = []\n",
        "    print(\"\\nEvaluating RCAN on Set14:\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (lr_imgs, hr_imgs) in enumerate(val_loader):\n",
        "            lr_imgs = lr_imgs.to(device)\n",
        "            hr_imgs = hr_imgs.to(device)\n",
        "\n",
        "            # Debug prints for first image\n",
        "            if i == 0:\n",
        "                print(\"\\nTracking first image processing:\")\n",
        "                print(f\"1. Initial LR range: [{lr_imgs.min():.4f}, {lr_imgs.max():.4f}]\")\n",
        "\n",
        "            # Normalize to [0, 1]\n",
        "            lr_imgs = lr_imgs / 255.0\n",
        "\n",
        "            if i == 0:\n",
        "                print(f\"2. After [0,1] norm: [{lr_imgs.min():.4f}, {lr_imgs.max():.4f}]\")\n",
        "\n",
        "            # Process through model (including its internal mean shifts)\n",
        "            sr_output = model(lr_imgs)\n",
        "\n",
        "            if i == 0:\n",
        "                print(f\"3. Model output: [{sr_output.min():.4f}, {sr_output.max():.4f}]\")\n",
        "\n",
        "            # Scale back to [0, 255]\n",
        "            sr_output = sr_output * 255.0\n",
        "\n",
        "            if i == 0:\n",
        "                print(f\"4. Final SR range: [{sr_output.min():.4f}, {sr_output.max():.4f}]\")\n",
        "                print(f\"5. HR target range: [{hr_imgs.min():.4f}, {hr_imgs.max():.4f}]\")\n",
        "\n",
        "            # Calculate PSNR on Y channel\n",
        "            psnr = calc_psnr(sr_output, hr_imgs)\n",
        "            psnr_values.append(psnr)\n",
        "            print(f\"Image {i+1}: PSNR: {psnr:.2f} dB\")\n",
        "\n",
        "    avg_psnr = sum(psnr_values) / len(psnr_values)\n",
        "    print(f\"\\nAverage PSNR: {avg_psnr:.2f} dB\")\n",
        "    print(f\"Reference PSNR from paper: 28.87 dB\")\n",
        "    print(f\"Difference: {28.87 - avg_psnr:.2f} dB\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    evaluate_rcan()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "---rCNwGNc_e"
      },
      "source": [
        "Getting significantly closer to the reference results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkPVPX4uOhld"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from math import log10\n",
        "import glob\n",
        "import os\n",
        "from google.colab import drive\n",
        "import math\n",
        "\n",
        "class SRDataset(Dataset):\n",
        "    def __init__(self, root_dir, scale=4, train=False):\n",
        "        self.scale = scale\n",
        "\n",
        "        # Mount Google Drive if not already mounted\n",
        "        if not os.path.exists('/content/drive'):\n",
        "            drive.mount('/content/drive')\n",
        "\n",
        "        # Find all images\n",
        "        self.image_files = sorted(glob.glob(os.path.join(root_dir, '*.png')))\n",
        "        if len(self.image_files) == 0:\n",
        "            raise RuntimeError(f\"No PNG images found in {root_dir}\")\n",
        "\n",
        "        print(f\"Found {len(self.image_files)} images in {root_dir}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load HR image\n",
        "        img_path = self.image_files[idx]\n",
        "        hr_image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        # Ensure dimensions are divisible by scale\n",
        "        w, h = hr_image.size\n",
        "        w = w - w % self.scale\n",
        "        h = h - h % self.scale\n",
        "        hr_image = hr_image.crop((0, 0, w, h))\n",
        "\n",
        "        # Convert to tensor (keeping in [0, 255] initially)\n",
        "        hr_tensor = TF.to_tensor(hr_image) * 255.0\n",
        "\n",
        "        # Create LR using bicubic downsampling\n",
        "        lr_tensor = TF.resize(\n",
        "            hr_tensor,\n",
        "            size=[s // self.scale for s in hr_tensor.shape[-2:]],\n",
        "            interpolation=TF.InterpolationMode.BICUBIC\n",
        "        )\n",
        "\n",
        "        return lr_tensor, hr_tensor\n",
        "\n",
        "def setup_datasets(batch_size=16):\n",
        "    \"\"\"Setup training and validation dataloaders\"\"\"\n",
        "    # Create datasets\n",
        "    train_dataset = SRDataset(\n",
        "        root_dir='DIV2K_train_HR',\n",
        "        scale=4,\n",
        "        train=True\n",
        "    )\n",
        "\n",
        "    val_dataset = SRDataset(\n",
        "        root_dir='/content/drive/MyDrive/E82/finalproject/Set14',\n",
        "        scale=4,\n",
        "        train=False\n",
        "    )\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=1,\n",
        "        shuffle=False,\n",
        "        num_workers=1,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "def evaluate_rcan():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    def quantize(img, rgb_range):\n",
        "        pixel_range = 255 / rgb_range\n",
        "        return img.mul(pixel_range).clamp(0, 255).round().div(pixel_range)\n",
        "\n",
        "    def calc_psnr(sr, hr, scale=4, rgb_range=255):\n",
        "        # Convert inputs from [0, 255] to [0, 1] by dividing by rgb_range\n",
        "        diff = (sr - hr).data.div(rgb_range)\n",
        "        shave = scale\n",
        "\n",
        "        # Convert to Y\n",
        "        if diff.size(1) > 1:\n",
        "            convert = diff.new(1, 3, 1, 1)\n",
        "            convert[0, 0, 0, 0] = 65.738\n",
        "            convert[0, 1, 0, 0] = 129.057\n",
        "            convert[0, 2, 0, 0] = 25.064\n",
        "            diff.mul_(convert).div_(256)\n",
        "            diff = diff.sum(dim=1, keepdim=True)\n",
        "\n",
        "        # Shave borders\n",
        "        valid = diff[:, :, shave:-shave, shave:-shave]\n",
        "        mse = valid.pow(2).mean()\n",
        "        return -10 * math.log10(mse)\n",
        "\n",
        "    # Load RCAN model\n",
        "    print(\"Loading RCAN model...\")\n",
        "    model = load_rcan_model()\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Setup validation data\n",
        "    _, val_loader = setup_datasets(batch_size=1)\n",
        "\n",
        "    psnr_values = []\n",
        "    print(\"\\nEvaluating RCAN on Set14:\")\n",
        "    with torch.no_grad():\n",
        "        for i, (lr_imgs, hr_imgs) in enumerate(val_loader):\n",
        "            lr_imgs = lr_imgs.to(device)\n",
        "            hr_imgs = hr_imgs.to(device)\n",
        "\n",
        "            # Scale to [0,1] range for model input\n",
        "            lr_imgs = lr_imgs / 255.0\n",
        "\n",
        "            # Get SR output\n",
        "            sr_output = model(lr_imgs)\n",
        "\n",
        "            # Scale back and quantize\n",
        "            sr_output = sr_output * 255.0\n",
        "            sr_output = quantize(sr_output, rgb_range=255)\n",
        "\n",
        "            # Calculate PSNR using their method\n",
        "            psnr = calc_psnr(sr_output, hr_imgs, scale=4, rgb_range=255)\n",
        "            psnr_values.append(psnr)  # Removed .item() since psnr is already a float\n",
        "\n",
        "            if i == 0:\n",
        "                print(f\"\\nFirst image ranges:\")\n",
        "                print(f\"LR input range (normalized): [{lr_imgs.min():.4f}, {lr_imgs.max():.4f}]\")\n",
        "                print(f\"SR output range (after quantize): [{sr_output.min():.4f}, {sr_output.max():.4f}]\")\n",
        "                print(f\"HR target range: [{hr_imgs.min():.4f}, {hr_imgs.max():.4f}]\\n\")\n",
        "\n",
        "            print(f\"Image {i+1}: PSNR: {psnr:.2f} dB\")\n",
        "\n",
        "    avg_psnr = sum(psnr_values) / len(psnr_values)\n",
        "    print(f\"\\nAverage PSNR: {avg_psnr:.2f} dB\")\n",
        "    print(f\"Reference PSNR from paper: 28.87 dB\")\n",
        "    print(f\"Difference: {28.87 - avg_psnr:.2f} dB\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    evaluate_rcan()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzDyM0zbPsS4"
      },
      "outputs": [],
      "source": [
        "def evaluate_rcan():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    def quantize(img, rgb_range):\n",
        "        pixel_range = 255 / rgb_range\n",
        "        return img.mul(pixel_range).clamp(0, 255).round().div(pixel_range)\n",
        "\n",
        "    def calc_psnr(sr, hr, scale=4, rgb_range=255):\n",
        "        # Convert inputs from [0, 255] to [0, 1] by dividing by rgb_range\n",
        "        diff = (sr - hr).data.div(rgb_range)\n",
        "        shave = scale\n",
        "\n",
        "        # Convert to Y\n",
        "        if diff.size(1) > 1:\n",
        "            convert = diff.new(1, 3, 1, 1)\n",
        "            convert[0, 0, 0, 0] = 65.738\n",
        "            convert[0, 1, 0, 0] = 129.057\n",
        "            convert[0, 2, 0, 0] = 25.064\n",
        "            diff.mul_(convert).div_(256)\n",
        "            diff = diff.sum(dim=1, keepdim=True)\n",
        "\n",
        "        # Shave borders\n",
        "        valid = diff[:, :, shave:-shave, shave:-shave]\n",
        "        mse = valid.pow(2).mean()\n",
        "        return -10 * math.log10(mse)\n",
        "\n",
        "    # Load RCAN model\n",
        "    print(\"Loading RCAN model...\")\n",
        "    model = load_rcan_model()\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Setup validation data\n",
        "    _, val_loader = setup_datasets(batch_size=1)\n",
        "\n",
        "    print(\"\\nStarting detailed evaluation:\")\n",
        "    with torch.no_grad():\n",
        "        for i, (lr_imgs, hr_imgs) in enumerate(val_loader):\n",
        "            lr_imgs = lr_imgs.to(device)\n",
        "            hr_imgs = hr_imgs.to(device)\n",
        "\n",
        "            print(f\"\\nStep 1 - Initial ranges:\")\n",
        "            print(f\"LR range: [{lr_imgs.min():.4f}, {lr_imgs.max():.4f}]\")\n",
        "            print(f\"HR range: [{hr_imgs.min():.4f}, {hr_imgs.max():.4f}]\")\n",
        "\n",
        "            # Scale to [0,1] range for model input\n",
        "            lr_imgs = lr_imgs / 255.0\n",
        "            print(f\"\\nStep 2 - After scaling LR to [0,1]:\")\n",
        "            print(f\"LR range: [{lr_imgs.min():.4f}, {lr_imgs.max():.4f}]\")\n",
        "\n",
        "            # Get SR output\n",
        "            sr_output = model(lr_imgs)\n",
        "            print(f\"\\nStep 3 - Raw model output:\")\n",
        "            print(f\"SR range: [{sr_output.min():.4f}, {sr_output.max():.4f}]\")\n",
        "\n",
        "            # Scale back and quantize\n",
        "            sr_output = sr_output * 255.0\n",
        "            print(f\"\\nStep 4 - After scaling back to [0,255]:\")\n",
        "            print(f\"SR range: [{sr_output.min():.4f}, {sr_output.max():.4f}]\")\n",
        "\n",
        "            sr_output = quantize(sr_output, rgb_range=255)\n",
        "            print(f\"\\nStep 5 - After quantization:\")\n",
        "            print(f\"SR range: [{sr_output.min():.4f}, {sr_output.max():.4f}]\")\n",
        "\n",
        "            # Calculate PSNR\n",
        "            print(\"\\nStep 6 - Calculate PSNR:\")\n",
        "            print(f\"Input shapes:\")\n",
        "            print(f\"SR shape: {sr_output.shape}\")\n",
        "            print(f\"HR shape: {hr_imgs.shape}\")\n",
        "            psnr = calc_psnr(sr_output, hr_imgs, scale=4, rgb_range=255)\n",
        "            print(f\"PSNR: {psnr:.2f} dB\")\n",
        "\n",
        "            # Check specific values\n",
        "            print(\"\\nStep 7 - Sample pixel values:\")\n",
        "            print(\"SR center pixel values:\", sr_output[0, :, sr_output.shape[2]//2, sr_output.shape[3]//2])\n",
        "            print(\"HR center pixel values:\", hr_imgs[0, :, hr_imgs.shape[2]//2, hr_imgs.shape[3]//2])\n",
        "\n",
        "            break  # Just examine first image\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    evaluate_rcan()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dR71eefuAys4"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from math import log10\n",
        "import glob\n",
        "import os\n",
        "from google.colab import drive\n",
        "import math\n",
        "\n",
        "class SRDataset(Dataset):\n",
        "    def __init__(self, root_dir, scale=4, train=False):\n",
        "        self.scale = scale\n",
        "\n",
        "        # Mount Google Drive if not already mounted\n",
        "        if not os.path.exists('/content/drive'):\n",
        "            drive.mount('/content/drive')\n",
        "\n",
        "        # Find all images\n",
        "        self.image_files = sorted(glob.glob(os.path.join(root_dir, '*.png')))\n",
        "        if len(self.image_files) == 0:\n",
        "            raise RuntimeError(f\"No PNG images found in {root_dir}\")\n",
        "\n",
        "        print(f\"Found {len(self.image_files)} images in {root_dir}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load HR image\n",
        "        img_path = self.image_files[idx]\n",
        "        hr_image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        # Ensure dimensions are divisible by scale\n",
        "        w, h = hr_image.size\n",
        "        w = w - w % self.scale\n",
        "        h = h - h % self.scale\n",
        "        hr_image = hr_image.crop((0, 0, w, h))\n",
        "\n",
        "        # Convert to tensor (keeping in [0, 255] initially)\n",
        "        hr_tensor = TF.to_tensor(hr_image) * 255.0\n",
        "\n",
        "        # Create LR using bicubic downsampling\n",
        "        lr_tensor = TF.resize(\n",
        "            hr_tensor,\n",
        "            size=[s // self.scale for s in hr_tensor.shape[-2:]],\n",
        "            interpolation=TF.InterpolationMode.BICUBIC\n",
        "        )\n",
        "\n",
        "        return lr_tensor, hr_tensor\n",
        "\n",
        "def setup_datasets():\n",
        "    \"\"\"Setup validation dataloader for Set14\"\"\"\n",
        "    val_dataset = SRDataset(\n",
        "        root_dir='/content/drive/MyDrive/E82/finalproject/Set14',\n",
        "        scale=4,\n",
        "        train=False\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=1,\n",
        "        shuffle=False,\n",
        "        num_workers=1,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    return val_loader\n",
        "\n",
        "def evaluate_rcan():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    def quantize(img, rgb_range):\n",
        "        return img.clamp(0, 255).round()\n",
        "\n",
        "    def calc_psnr(sr, hr, scale=4, rgb_range=255, benchmark=True):\n",
        "        if hr.nelement() == 1:\n",
        "            return 0\n",
        "\n",
        "        diff = (sr - hr)\n",
        "        shave = scale + 6 if not benchmark else scale\n",
        "        if diff.size(1) > 1:\n",
        "            gray_coeffs = [65.738, 129.057, 25.064]\n",
        "            convert = diff.new_tensor(gray_coeffs).view(1, 3, 1, 1) / 256\n",
        "            diff = diff.mul(convert).sum(dim=1, keepdim=True)\n",
        "\n",
        "        valid = diff[:, :, shave:-shave, shave:-shave]\n",
        "        mse = valid.pow(2).mean()\n",
        "\n",
        "        return -10 * math.log10(mse/(rgb_range**2))\n",
        "\n",
        "    # Load RCAN model\n",
        "    print(\"Loading RCAN model...\")\n",
        "    model = load_rcan_model()\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Setup validation data\n",
        "    val_loader = setup_datasets()\n",
        "\n",
        "    print(\"\\nStarting evaluation on Set14:\")\n",
        "    psnr_values = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (lr_imgs, hr_imgs) in enumerate(val_loader):\n",
        "            if i == 0:\n",
        "                print(f\"\\nStep 1 - Initial ranges:\")\n",
        "                print(f\"LR range: [{lr_imgs.min():.4f}, {lr_imgs.max():.4f}]\")\n",
        "                print(f\"HR range: [{hr_imgs.min():.4f}, {hr_imgs.max():.4f}]\")\n",
        "\n",
        "            lr_imgs = lr_imgs.to(device)\n",
        "            hr_imgs = hr_imgs.to(device)\n",
        "\n",
        "            # Scale to [0,1] range for model input\n",
        "            lr_imgs = lr_imgs / 255.0\n",
        "\n",
        "            if i == 0:\n",
        "                print(f\"\\nStep 2 - After scaling LR to [0,1]:\")\n",
        "                print(f\"LR range: [{lr_imgs.min():.4f}, {lr_imgs.max():.4f}]\")\n",
        "\n",
        "            # Get SR output\n",
        "            sr_output = model(lr_imgs)\n",
        "\n",
        "            if i == 0:\n",
        "                print(f\"\\nStep 3 - Raw model output:\")\n",
        "                print(f\"SR range: [{sr_output.min():.4f}, {sr_output.max():.4f}]\")\n",
        "\n",
        "            # Scale back to [0,255] and quantize\n",
        "            sr_output = sr_output * 255.0\n",
        "            sr_output = quantize(sr_output, rgb_range=255)\n",
        "\n",
        "            if i == 0:\n",
        "                print(f\"\\nStep 4 - After quantization:\")\n",
        "                print(f\"SR range: [{sr_output.min():.4f}, {sr_output.max():.4f}]\")\n",
        "                print(f\"Input shapes:\")\n",
        "                print(f\"SR shape: {sr_output.shape}\")\n",
        "                print(f\"HR shape: {hr_imgs.shape}\")\n",
        "\n",
        "            # Calculate PSNR\n",
        "            psnr = calc_psnr(sr_output, hr_imgs, scale=4, rgb_range=255)\n",
        "            psnr_values.append(psnr)\n",
        "\n",
        "            if i == 0:\n",
        "                print(\"\\nStep 5 - Sample pixel values:\")\n",
        "                print(\"SR center pixel values:\", sr_output[0, :, sr_output.shape[2]//2, sr_output.shape[3]//2])\n",
        "                print(\"HR center pixel values:\", hr_imgs[0, :, hr_imgs.shape[2]//2, hr_imgs.shape[3]//2])\n",
        "\n",
        "            print(f\"Image {i+1}: PSNR: {psnr:.2f} dB\")\n",
        "\n",
        "    avg_psnr = sum(psnr_values) / len(psnr_values)\n",
        "    print(f\"\\nAverage PSNR: {avg_psnr:.2f} dB\")\n",
        "    print(f\"Reference PSNR from paper: 28.87 dB\")\n",
        "    print(f\"Difference: {28.87 - avg_psnr:.2f} dB\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    evaluate_rcan()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jSjP4CACIMG"
      },
      "source": [
        "We've fixed the RCAN implementation as much as we reasonably can here. It uses the official implementation code wherever possible, so we can reasonably call it an optimized RCAN and move on to trying to beat it with our architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTucmK-QCX3G"
      },
      "outputs": [],
      "source": [
        "class SelfSupervisedAttention(nn.Module):\n",
        "    \"\"\"Self-supervised auxiliary network for dynamic pixel importance prediction\"\"\"\n",
        "    def __init__(self, in_channels=64):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        # Spatial feature extraction with BatchNorm for training stability\n",
        "        self.conv1 = nn.Conv2d(in_channels, in_channels, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
        "        self.conv2 = nn.Conv2d(in_channels, in_channels, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(in_channels)\n",
        "\n",
        "        # Channel attention\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.channel_attention = nn.Sequential(\n",
        "            nn.Linear(in_channels, in_channels//4),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(in_channels//4, in_channels),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Final attention prediction\n",
        "        self.conv3 = nn.Conv2d(in_channels, in_channels//2, 3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(in_channels//2)\n",
        "        self.conv4 = nn.Conv2d(in_channels//2, 1, 1)\n",
        "\n",
        "        # Initialize weights\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.ones_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Spatial features\n",
        "        feat = F.relu(self.bn1(self.conv1(x)))\n",
        "        feat = F.relu(self.bn2(self.conv2(feat)))\n",
        "\n",
        "        # Channel attention\n",
        "        channel_weights = self.avg_pool(feat).squeeze(-1).squeeze(-1)\n",
        "        channel_weights = self.channel_attention(channel_weights)\n",
        "        channel_weights = channel_weights.view(-1, self.in_channels, 1, 1)\n",
        "\n",
        "        # Apply channel attention\n",
        "        feat = feat * channel_weights\n",
        "\n",
        "        # Generate attention map that enhances important features\n",
        "        feat = F.relu(self.bn3(self.conv3(feat)))\n",
        "        attention = torch.sigmoid(self.conv4(feat))\n",
        "\n",
        "        # Scale attention to maintain feature magnitudes\n",
        "        attention = attention + 1  # Range [1,2] to enhance features without destroying them\n",
        "\n",
        "        return attention\n",
        "\n",
        "class AttentionAugmentedRCAN(nn.Module):\n",
        "    def __init__(self, base_rcan, freeze_base=True):\n",
        "        super().__init__()\n",
        "        self.rcan = base_rcan\n",
        "        self.attention_net = SelfSupervisedAttention(64)  # RCAN uses 64 channels\n",
        "\n",
        "        if freeze_base:\n",
        "            for param in self.rcan.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, x, mode='inference'):\n",
        "        if mode == 'pre_training':\n",
        "            # For attention network pre-training\n",
        "            feats = self.rcan.extract_features(x)\n",
        "            attention = self.attention_net(feats[1])  # Apply to body features\n",
        "            return attention\n",
        "\n",
        "        # Normal inference with attention\n",
        "        input_feat, body_feat = self.rcan.extract_features(x)\n",
        "        attention = self.attention_net(body_feat)\n",
        "        weighted_feat = body_feat * attention\n",
        "\n",
        "        # Complete super-resolution\n",
        "        sr_output = self.rcan.complete_sr((input_feat, weighted_feat))\n",
        "\n",
        "        return sr_output, attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osli5NtrCkBF"
      },
      "source": [
        "testing script to verify our attention mechanism is working correctly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31u2S1x7CmoX"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import figure\n",
        "\n",
        "def test_attention_mechanism():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load base RCAN model\n",
        "    print(\"Loading RCAN model...\")\n",
        "    base_model = load_rcan_model()\n",
        "    base_model = base_model.to(device)\n",
        "\n",
        "    # Create augmented model\n",
        "    print(\"Creating attention-augmented model...\")\n",
        "    model = AttentionAugmentedRCAN(base_model, freeze_base=True)\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Create dummy input\n",
        "    print(\"\\nTesting with dummy input:\")\n",
        "    x = torch.randn(1, 3, 32, 32).to(device)\n",
        "    x = (x - x.min()) / (x.max() - x.min())  # Normalize to [0,1]\n",
        "    print(f\"Input shape: {x.shape}\")\n",
        "    print(f\"Input range: [{x.min():.4f}, {x.max():.4f}]\")\n",
        "\n",
        "    # Test pre-training mode\n",
        "    print(\"\\nTesting pre-training mode:\")\n",
        "    with torch.no_grad():\n",
        "        attention = model(x, mode='pre_training')\n",
        "        print(f\"Attention shape: {attention.shape}\")\n",
        "        print(f\"Attention range: [{attention.min():.4f}, {attention.max():.4f}]\")\n",
        "\n",
        "    # Test inference mode\n",
        "    print(\"\\nTesting inference mode:\")\n",
        "    with torch.no_grad():\n",
        "        sr_output, attention = model(x)\n",
        "        print(f\"SR output shape: {sr_output.shape}\")\n",
        "        print(f\"SR output range: [{sr_output.min():.4f}, {sr_output.max():.4f}]\")\n",
        "        print(f\"Attention shape: {attention.shape}\")\n",
        "        print(f\"Attention range: [{attention.min():.4f}, {attention.max():.4f}]\")\n",
        "\n",
        "    # Visualize attention map\n",
        "    print(\"\\nVisualizing attention map...\")\n",
        "    fig = plt.figure(figsize=(10, 5))\n",
        "\n",
        "    # Heatmap\n",
        "    plt.subplot(1, 2, 1)\n",
        "    im = plt.imshow(attention[0, 0].cpu().numpy(), cmap='viridis')\n",
        "    plt.colorbar(im)\n",
        "    plt.title(\"Attention Map - Heatmap\")\n",
        "\n",
        "    # Histogram\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.hist(attention.cpu().numpy().flatten(), bins=50)\n",
        "    plt.title(\"Attention Map - Value Distribution\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Test gradients\n",
        "    print(\"\\nTesting gradient flow:\")\n",
        "    model.train()\n",
        "    sr_output, attention = model(x)\n",
        "    loss = sr_output.mean() + attention.mean()  # Dummy loss\n",
        "    loss.backward()\n",
        "\n",
        "    has_grad = lambda p: p.grad is not None and torch.abs(p.grad).sum().item() > 0\n",
        "\n",
        "    print(\"Attention network gradients:\")\n",
        "    attention_grads = [has_grad(p) for p in model.attention_net.parameters()]\n",
        "    print(f\"Parameters with gradients: {sum(attention_grads)}/{len(attention_grads)}\")\n",
        "\n",
        "    print(\"\\nRCAN gradients (should be zero if frozen):\")\n",
        "    rcan_grads = [has_grad(p) for p in model.rcan.parameters()]\n",
        "    print(f\"Parameters with gradients: {sum(rcan_grads)}/{len(rcan_grads)}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model = test_attention_mechanism()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lCkU-HSDNeR"
      },
      "source": [
        "Attention map shapes are correct (32x32 matching input features)\n",
        "\n",
        "Attention values are in a reasonable range [1.26, 1.56], providing meaningful scaling\n",
        "\n",
        "Gradient flow is working correctly:\n",
        "\n",
        "All attention parameters (18/18) receive gradients\n",
        "\n",
        "RCAN parameters (0/1630) are properly frozen\n",
        "\n",
        "\n",
        "The attention distribution looks roughly Gaussian (from histogram)\n",
        "\n",
        "Issues to address:\n",
        "\n",
        "SR output has values outside [0,1] range: [-0.1375, 1.1809]\n",
        "\n",
        "The attention map shows some vertical striping (visible in heatmap)\n",
        "\n",
        "The attention range could be wider for stronger feature enhancement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_MYmTsmDUFd"
      },
      "outputs": [],
      "source": [
        "class SelfSupervisedAttention(nn.Module):\n",
        "    def __init__(self, in_channels=64):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        # Deeper feature extraction\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels, 3, padding=1),\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(in_channels, in_channels, 3, padding=1),\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        # Channel attention with higher reduction ratio\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.channel_attention = nn.Sequential(\n",
        "            nn.Linear(in_channels, in_channels//8),  # Increased reduction\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(in_channels//8, in_channels),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Final attention prediction with smoother transition\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels//2, 3, padding=1),\n",
        "            nn.BatchNorm2d(in_channels//2),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(in_channels//2, 1, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Feature extraction\n",
        "        feat = self.conv1(x)\n",
        "\n",
        "        # Channel attention\n",
        "        channel_weights = self.avg_pool(feat).squeeze(-1).squeeze(-1)\n",
        "        channel_weights = self.channel_attention(channel_weights)\n",
        "        channel_weights = channel_weights.view(-1, self.in_channels, 1, 1)\n",
        "        feat = feat * channel_weights\n",
        "\n",
        "        # Generate attention map\n",
        "        attention = self.conv2(feat)\n",
        "\n",
        "        # Scale to [1.0, 2.0] with smoother activation\n",
        "        attention = torch.tanh(attention) * 0.5 + 1.5\n",
        "\n",
        "        return attention\n",
        "\n",
        "class AttentionAugmentedRCAN(nn.Module):\n",
        "    def __init__(self, base_rcan, freeze_base=True):\n",
        "        super().__init__()\n",
        "        self.rcan = base_rcan\n",
        "        self.attention_net = SelfSupervisedAttention(64)\n",
        "\n",
        "        if freeze_base:\n",
        "            for param in self.rcan.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, x, mode='inference'):\n",
        "        if mode == 'pre_training':\n",
        "            feats = self.rcan.extract_features(x)\n",
        "            attention = self.attention_net(feats[1])\n",
        "            return attention\n",
        "\n",
        "        # Normal inference with attention\n",
        "        input_feat, body_feat = self.rcan.extract_features(x)\n",
        "        attention = self.attention_net(body_feat)\n",
        "        weighted_feat = body_feat * attention\n",
        "\n",
        "        # Complete super-resolution with clamping\n",
        "        sr_output = self.rcan.complete_sr((input_feat, weighted_feat))\n",
        "        sr_output = torch.clamp(sr_output, 0, 1)\n",
        "\n",
        "        return sr_output, attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gs9wP_2PDd6I"
      },
      "outputs": [],
      "source": [
        "def test_updated_attention():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load base RCAN model\n",
        "    print(\"Loading RCAN model...\")\n",
        "    base_model = load_rcan_model()\n",
        "    base_model = base_model.to(device)\n",
        "\n",
        "    # Create augmented model\n",
        "    print(\"Creating attention-augmented model...\")\n",
        "    model = AttentionAugmentedRCAN(base_model, freeze_base=True)\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Test with real data from Set14\n",
        "    print(\"\\nTesting with real data:\")\n",
        "    val_loader = setup_datasets()\n",
        "    lr_img, hr_img = next(iter(val_loader))\n",
        "    lr_img, hr_img = lr_img.to(device), hr_img.to(device)\n",
        "\n",
        "    # Scale input to [0,1]\n",
        "    lr_img = lr_img / 255.0\n",
        "\n",
        "    print(f\"Input shape: {lr_img.shape}\")\n",
        "    print(f\"Input range: [{lr_img.min():.4f}, {lr_img.max():.4f}]\")\n",
        "\n",
        "    # Test both modes\n",
        "    print(\"\\nTesting pre-training mode:\")\n",
        "    with torch.no_grad():\n",
        "        attention = model(lr_img, mode='pre_training')\n",
        "        print(f\"Attention shape: {attention.shape}\")\n",
        "        print(f\"Attention range: [{attention.min():.4f}, {attention.max():.4f}]\")\n",
        "\n",
        "    print(\"\\nTesting inference mode:\")\n",
        "    with torch.no_grad():\n",
        "        sr_output, attention = model(lr_img)\n",
        "        print(f\"SR output shape: {sr_output.shape}\")\n",
        "        print(f\"SR output range: [{sr_output.min():.4f}, {sr_output.max():.4f}]\")\n",
        "        print(f\"Attention shape: {attention.shape}\")\n",
        "        print(f\"Attention range: [{attention.min():.4f}, {attention.max():.4f}]\")\n",
        "\n",
        "    # Visualize attention map\n",
        "    print(\"\\nVisualizing attention map...\")\n",
        "    fig = plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Original LR image\n",
        "    plt.subplot(1, 3, 1)\n",
        "    lr_img_vis = lr_img[0].cpu().permute(1, 2, 0).numpy()\n",
        "    plt.imshow(lr_img_vis)\n",
        "    plt.title(\"LR Input\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Attention heatmap\n",
        "    plt.subplot(1, 3, 2)\n",
        "    im = plt.imshow(attention[0, 0].cpu().numpy(), cmap='viridis')\n",
        "    plt.colorbar(im)\n",
        "    plt.title(\"Attention Map\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Attention distribution\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.hist(attention.cpu().numpy().flatten(), bins=50)\n",
        "    plt.title(\"Attention Distribution\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Test gradients\n",
        "    print(\"\\nTesting gradient flow:\")\n",
        "    model.train()\n",
        "    sr_output, attention = model(lr_img)\n",
        "\n",
        "    # Use L1 loss as an example\n",
        "    loss = F.l1_loss(sr_output, hr_img/255.0) + 0.1 * attention.mean()\n",
        "    loss.backward()\n",
        "\n",
        "    has_grad = lambda p: p.grad is not None and torch.abs(p.grad).sum().item() > 0\n",
        "\n",
        "    print(\"Attention network gradients:\")\n",
        "    attention_grads = [has_grad(p) for p in model.attention_net.parameters()]\n",
        "    print(f\"Parameters with gradients: {sum(attention_grads)}/{len(attention_grads)}\")\n",
        "\n",
        "    print(\"\\nRCAN gradients (should be zero if frozen):\")\n",
        "    rcan_grads = [has_grad(p) for p in model.rcan.parameters()]\n",
        "    print(f\"Parameters with gradients: {sum(rcan_grads)}/{len(rcan_grads)}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model = test_updated_attention()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHzOi9fRDv1N"
      },
      "source": [
        "Looking at the results, we have some interesting observations and potential improvements needed:\n",
        "Good signs:\n",
        "\n",
        "SR output is now properly bounded [0.0000, 1.0000]\n",
        "The attention map shows clear structure matching the image content (you can see the baboon's features)\n",
        "All attention parameters receive gradients (18/18)\n",
        "RCAN remains properly frozen (0/1630)\n",
        "\n",
        "Issues to address:\n",
        "\n",
        "Attention range is very narrow [1.5407, 1.5523]\n",
        "\n",
        "Our target was [1.0, 2.0] but we're only using about 1% of that range\n",
        "This suggests the attention is not providing much feature enhancement\n",
        "\n",
        "\n",
        "The attention distribution is highly peaked\n",
        "\n",
        "Most values are clustered around 1.547\n",
        "We want more variance to differentiate important features\n",
        "\n",
        "\n",
        "\n",
        "Let's modify the attention network to address these:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sEptswYD1FJ"
      },
      "outputs": [],
      "source": [
        "class SelfSupervisedAttention(nn.Module):\n",
        "    def __init__(self, in_channels=64):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        # Deeper feature extraction with more channels\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels*2, 3, padding=1),\n",
        "            nn.BatchNorm2d(in_channels*2),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(in_channels*2, in_channels, 3, padding=1),\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        # Channel attention with temperature scaling\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.channel_attention = nn.Sequential(\n",
        "            nn.Linear(in_channels, in_channels//4),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(in_channels//4, in_channels),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Final attention prediction with temperature\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels//2, 3, padding=1),\n",
        "            nn.BatchNorm2d(in_channels//2),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(in_channels//2, 1, 1),\n",
        "        )\n",
        "\n",
        "        self.temperature = nn.Parameter(torch.ones(1) * 0.1)  # Learnable temperature\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Feature extraction\n",
        "        feat = self.conv1(x)\n",
        "\n",
        "        # Channel attention with temperature\n",
        "        channel_weights = self.avg_pool(feat).squeeze(-1).squeeze(-1)\n",
        "        channel_weights = self.channel_attention(channel_weights)\n",
        "        channel_weights = channel_weights.view(-1, self.in_channels, 1, 1)\n",
        "        feat = feat * channel_weights\n",
        "\n",
        "        # Generate attention map with temperature scaling\n",
        "        attention = self.conv2(feat)\n",
        "        attention = torch.tanh(attention / self.temperature) * 0.5 + 1.5\n",
        "\n",
        "        return attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voIIGfJPENXo"
      },
      "outputs": [],
      "source": [
        "def test_updated_attention():\n",
        "   device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "   print(f\"Using device: {device}\")\n",
        "\n",
        "   # Load base RCAN model\n",
        "   print(\"Loading RCAN model...\")\n",
        "   base_model = load_rcan_model()\n",
        "   base_model = base_model.to(device)\n",
        "\n",
        "   # Create augmented model\n",
        "   print(\"Creating attention-augmented model...\")\n",
        "   model = AttentionAugmentedRCAN(base_model, freeze_base=True)\n",
        "   model = model.to(device)\n",
        "   model.eval()\n",
        "\n",
        "   # Test with real data from Set14\n",
        "   print(\"\\nTesting with real data:\")\n",
        "   val_loader = setup_datasets()\n",
        "\n",
        "   # Test multiple images\n",
        "   all_attention_values = []\n",
        "\n",
        "   with torch.no_grad():\n",
        "       for i, (lr_img, hr_img) in enumerate(val_loader):\n",
        "           if i >= 3:  # Test first 3 images\n",
        "               break\n",
        "\n",
        "           lr_img = lr_img.to(device)\n",
        "           lr_img = lr_img / 255.0  # Scale to [0,1]\n",
        "\n",
        "           # Get model outputs\n",
        "           sr_output, attention = model(lr_img)\n",
        "           all_attention_values.append(attention.cpu().numpy())\n",
        "\n",
        "           # Print stats for each image\n",
        "           print(f\"\\nImage {i+1}:\")\n",
        "           print(f\"Input shape: {lr_img.shape}\")\n",
        "           print(f\"SR output shape: {sr_output.shape}\")\n",
        "           print(f\"Attention range: [{attention.min():.4f}, {attention.max():.4f}]\")\n",
        "           print(f\"Attention mean: {attention.mean():.4f}\")\n",
        "           print(f\"Attention std: {attention.std():.4f}\")\n",
        "\n",
        "           # Visualize\n",
        "           fig = plt.figure(figsize=(15, 5))\n",
        "\n",
        "           # LR input\n",
        "           plt.subplot(1, 3, 1)\n",
        "           lr_img_vis = lr_img[0].cpu().permute(1, 2, 0).numpy()\n",
        "           plt.imshow(lr_img_vis)\n",
        "           plt.title(f\"LR Input {i+1}\")\n",
        "           plt.axis('off')\n",
        "\n",
        "           # Attention heatmap\n",
        "           plt.subplot(1, 3, 2)\n",
        "           attention_map = attention[0, 0].cpu().numpy()\n",
        "           im = plt.imshow(attention_map, cmap='viridis')\n",
        "           plt.colorbar(im)\n",
        "           plt.title(f\"Attention Map (={attention.mean():.3f}, ={attention.std():.3f})\")\n",
        "           plt.axis('off')\n",
        "\n",
        "           # Attention histogram\n",
        "           plt.subplot(1, 3, 3)\n",
        "           plt.hist(attention.cpu().numpy().flatten(), bins=50, density=True)\n",
        "           plt.title(\"Attention Distribution\")\n",
        "           plt.axvline(attention.mean().item(), color='r', linestyle='--', label='Mean')\n",
        "           plt.axvline(attention.mean().item() + attention.std().item(), color='g', linestyle='--', label='+1 std')\n",
        "           plt.axvline(attention.mean().item() - attention.std().item(), color='g', linestyle='--', label='-1 std')\n",
        "           plt.legend()\n",
        "\n",
        "           plt.tight_layout()\n",
        "           plt.show()\n",
        "\n",
        "   # Print overall statistics\n",
        "   all_attention_values = np.concatenate([a.flatten() for a in all_attention_values])\n",
        "   print(\"\\nOverall Attention Statistics:\")\n",
        "   print(f\"Global range: [{np.min(all_attention_values):.4f}, {np.max(all_attention_values):.4f}]\")\n",
        "   print(f\"Global mean: {np.mean(all_attention_values):.4f}\")\n",
        "   print(f\"Global std: {np.std(all_attention_values):.4f}\")\n",
        "\n",
        "   # Test gradients\n",
        "   print(\"\\nTesting gradient flow...\")\n",
        "   model.train()\n",
        "   lr_img, hr_img = next(iter(val_loader))\n",
        "   lr_img = lr_img.to(device) / 255.0\n",
        "   hr_img = hr_img.to(device) / 255.0\n",
        "\n",
        "   sr_output, attention = model(lr_img)\n",
        "\n",
        "   # Test with reconstruction loss and attention regularization\n",
        "   recon_loss = F.l1_loss(sr_output, hr_img)\n",
        "   attention_reg = 0.1 * (attention.mean() - 1.5).abs()  # Center around 1.5\n",
        "   attention_std_reg = 0.1 * (0.1 - attention.std()).clamp(min=0)  # Encourage std > 0.1\n",
        "\n",
        "   loss = recon_loss + attention_reg + attention_std_reg\n",
        "   loss.backward()\n",
        "\n",
        "   print(\"\\nLoss components:\")\n",
        "   print(f\"Reconstruction loss: {recon_loss.item():.4f}\")\n",
        "   print(f\"Attention reg loss: {attention_reg.item():.4f}\")\n",
        "   print(f\"Attention std reg loss: {attention_std_reg.item():.4f}\")\n",
        "\n",
        "   has_grad = lambda p: p.grad is not None and torch.abs(p.grad).sum().item() > 0\n",
        "\n",
        "   print(\"\\nGradient check:\")\n",
        "   print(\"Attention network gradients:\")\n",
        "   attention_grads = [has_grad(p) for p in model.attention_net.parameters()]\n",
        "   print(f\"Parameters with gradients: {sum(attention_grads)}/{len(attention_grads)}\")\n",
        "\n",
        "   # Check temperature gradient specifically\n",
        "   temp_grad = model.attention_net.temperature.grad\n",
        "   print(f\"Temperature gradient: {temp_grad.item() if temp_grad is not None else 'None'}\")\n",
        "\n",
        "   return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "   model = test_updated_attention()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qyNLmcAEtGD"
      },
      "source": [
        "The attention mechanism is still not working as intended. There are several issues:\n",
        "\n",
        "Attention Range:\n",
        "\n",
        "Current: [1.023, 1.035] (variance of only ~0.012)\n",
        "Target: [1.0, 2.0] (we want much more variance)\n",
        "The attention maps are barely modulating the features\n",
        "\n",
        "\n",
        "Standard Deviation:\n",
        "\n",
        "Current:   0.001 (extremely small)\n",
        "We need much larger variation to meaningfully enhance different features\n",
        "\n",
        "\n",
        "Feature Detection:\n",
        "\n",
        "The attention maps show some structure (visible in the heatmaps)\n",
        "But the effect is too weak to meaningfully impact super-resolution\n",
        "\n",
        "\n",
        "\n",
        "Let's modify the attention mechanism to be more aggressive:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoB0MT6XEzxV"
      },
      "outputs": [],
      "source": [
        "class SelfSupervisedAttention(nn.Module):\n",
        "    def __init__(self, in_channels=64):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        # Increase capacity for feature detection\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels*2, 3, padding=1),\n",
        "            nn.BatchNorm2d(in_channels*2),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(in_channels*2, in_channels*2, 3, padding=1),\n",
        "            nn.BatchNorm2d(in_channels*2),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        # Stronger channel attention\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)  # Add max pooling\n",
        "        self.channel_attention = nn.Sequential(\n",
        "            nn.Linear(in_channels*4, in_channels),  # Combine avg and max features\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(in_channels, in_channels*2),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Final attention with stronger modulation\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels*2, in_channels, 3, padding=1),\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(in_channels, 1, 1),\n",
        "        )\n",
        "\n",
        "        # Learnable scaling parameters\n",
        "        self.scale = nn.Parameter(torch.ones(1) * 0.5)  # Control output range\n",
        "        self.temperature = nn.Parameter(torch.ones(1) * 0.1)  # Control sharpness\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Feature extraction\n",
        "        feat = self.conv1(x)\n",
        "\n",
        "        # Enhanced channel attention\n",
        "        avg_pool = self.avg_pool(feat).squeeze(-1).squeeze(-1)\n",
        "        max_pool = self.max_pool(feat).squeeze(-1).squeeze(-1)\n",
        "        channel_feats = torch.cat([avg_pool, max_pool], dim=1)\n",
        "        channel_weights = self.channel_attention(channel_feats)\n",
        "        channel_weights = channel_weights.view(-1, self.in_channels*2, 1, 1)\n",
        "\n",
        "        feat = feat * channel_weights\n",
        "\n",
        "        # Generate attention map with learnable scaling\n",
        "        attention = self.conv2(feat)\n",
        "        attention = torch.tanh(attention / self.temperature) * self.scale + 1.5\n",
        "\n",
        "        return attention\n",
        "\n",
        "class AttentionAugmentedRCAN(nn.Module):\n",
        "    def __init__(self, base_rcan, freeze_base=True):\n",
        "        super().__init__()\n",
        "        self.rcan = base_rcan\n",
        "        self.attention_net = SelfSupervisedAttention(64)\n",
        "\n",
        "        if freeze_base:\n",
        "            for param in self.rcan.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        # Initialize with stronger attention\n",
        "        self.attention_net.scale.data.fill_(1.0)\n",
        "        self.attention_net.temperature.data.fill_(0.05)\n",
        "\n",
        "    def forward(self, x, mode='inference'):\n",
        "        if mode == 'pre_training':\n",
        "            feats = self.rcan.extract_features(x)\n",
        "            attention = self.attention_net(feats[1])\n",
        "            return attention\n",
        "\n",
        "        input_feat, body_feat = self.rcan.extract_features(x)\n",
        "        attention = self.attention_net(body_feat)\n",
        "        weighted_feat = body_feat * attention\n",
        "        sr_output = self.rcan.complete_sr((input_feat, weighted_feat))\n",
        "        return sr_output, attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfDsfM3cFA2n"
      },
      "source": [
        "Testing again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTM6Q3S9FDW5"
      },
      "outputs": [],
      "source": [
        "def test_updated_attention():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load base RCAN model\n",
        "    print(\"Loading RCAN model...\")\n",
        "    base_model = load_rcan_model()\n",
        "    base_model = base_model.to(device)\n",
        "\n",
        "    # Create augmented model\n",
        "    print(\"Creating attention-augmented model...\")\n",
        "    model = AttentionAugmentedRCAN(base_model, freeze_base=True)\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Test with real data from Set14\n",
        "    print(\"\\nTesting with real data:\")\n",
        "    val_loader = setup_datasets()\n",
        "\n",
        "    # Test multiple images\n",
        "    all_attention_values = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (lr_img, hr_img) in enumerate(val_loader):\n",
        "            if i >= 3:  # Test first 3 images\n",
        "                break\n",
        "\n",
        "            lr_img = lr_img.to(device)\n",
        "            lr_img = lr_img / 255.0  # Scale to [0,1]\n",
        "\n",
        "            # Get model outputs\n",
        "            sr_output, attention = model(lr_img)\n",
        "            all_attention_values.append(attention.cpu().numpy())\n",
        "\n",
        "            # Print stats for each image\n",
        "            print(f\"\\nImage {i+1}:\")\n",
        "            print(f\"Input shape: {lr_img.shape}\")\n",
        "            print(f\"SR output shape: {sr_output.shape}\")\n",
        "            print(f\"Attention range: [{attention.min():.4f}, {attention.max():.4f}]\")\n",
        "            print(f\"Attention mean: {attention.mean():.4f}\")\n",
        "            print(f\"Attention std: {attention.std():.4f}\")\n",
        "\n",
        "            # Visualize\n",
        "            fig = plt.figure(figsize=(15, 5))\n",
        "\n",
        "            # LR input\n",
        "            plt.subplot(1, 3, 1)\n",
        "            lr_img_vis = lr_img[0].cpu().permute(1, 2, 0).numpy()\n",
        "            plt.imshow(lr_img_vis)\n",
        "            plt.title(f\"LR Input {i+1}\")\n",
        "            plt.axis('off')\n",
        "\n",
        "            # Attention heatmap\n",
        "            plt.subplot(1, 3, 2)\n",
        "            attention_map = attention[0, 0].cpu().numpy()\n",
        "            im = plt.imshow(attention_map, cmap='viridis')\n",
        "            plt.colorbar(im)\n",
        "            plt.title(f\"Attention Map (={attention.mean():.3f}, ={attention.std():.3f})\")\n",
        "            plt.axis('off')\n",
        "\n",
        "            # Attention histogram\n",
        "            plt.subplot(1, 3, 3)\n",
        "            plt.hist(attention.cpu().numpy().flatten(), bins=50, density=True)\n",
        "            plt.title(\"Attention Distribution\")\n",
        "            plt.axvline(attention.mean().item(), color='r', linestyle='--', label='Mean')\n",
        "            plt.axvline(attention.mean().item() + attention.std().item(), color='g', linestyle='--', label='+1 std')\n",
        "            plt.axvline(attention.mean().item() - attention.std().item(), color='g', linestyle='--', label='-1 std')\n",
        "            plt.legend()\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "    # Print overall statistics\n",
        "    all_attention_values = np.concatenate([a.flatten() for a in all_attention_values])\n",
        "    print(\"\\nOverall Attention Statistics:\")\n",
        "    print(f\"Global range: [{np.min(all_attention_values):.4f}, {np.max(all_attention_values):.4f}]\")\n",
        "    print(f\"Global mean: {np.mean(all_attention_values):.4f}\")\n",
        "    print(f\"Global std: {np.std(all_attention_values):.4f}\")\n",
        "\n",
        "    # Test gradients\n",
        "    print(\"\\nTesting gradient flow...\")\n",
        "    model.train()\n",
        "    lr_img, hr_img = next(iter(val_loader))\n",
        "    lr_img = lr_img.to(device) / 255.0\n",
        "    hr_img = hr_img.to(device) / 255.0\n",
        "\n",
        "    sr_output, attention = model(lr_img)\n",
        "\n",
        "    # Test with reconstruction loss and attention regularization\n",
        "    recon_loss = F.l1_loss(sr_output, hr_img)\n",
        "    attention_reg = 0.1 * (attention.mean() - 1.5).abs()  # Center around 1.5\n",
        "    attention_std_reg = 0.1 * (0.1 - attention.std()).clamp(min=0)  # Encourage std > 0.1\n",
        "\n",
        "    loss = recon_loss + attention_reg + attention_std_reg\n",
        "    loss.backward()\n",
        "\n",
        "    print(\"\\nLoss components:\")\n",
        "    print(f\"Reconstruction loss: {recon_loss.item():.4f}\")\n",
        "    print(f\"Attention reg loss: {attention_reg.item():.4f}\")\n",
        "    print(f\"Attention std reg loss: {attention_std_reg.item():.4f}\")\n",
        "\n",
        "    has_grad = lambda p: p.grad is not None and torch.abs(p.grad).sum().item() > 0\n",
        "\n",
        "    print(\"\\nGradient check:\")\n",
        "    print(\"Attention network gradients:\")\n",
        "    attention_grads = [has_grad(p) for p in model.attention_net.parameters()]\n",
        "    print(f\"Parameters with gradients: {sum(attention_grads)}/{len(attention_grads)}\")\n",
        "\n",
        "    # Check scaling parameters specifically\n",
        "    print(\"\\nScaling parameters:\")\n",
        "    print(f\"Scale value: {model.attention_net.scale.item():.4f}\")\n",
        "    print(f\"Scale gradient: {model.attention_net.scale.grad.item() if model.attention_net.scale.grad is not None else 'None'}\")\n",
        "    print(f\"Temperature value: {model.attention_net.temperature.item():.4f}\")\n",
        "    print(f\"Temperature gradient: {model.attention_net.temperature.grad.item() if model.attention_net.temperature.grad is not None else 'None'}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model = test_updated_attention()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoTM1ybmFdIN"
      },
      "source": [
        "The attention values have shifted but we still have issues:\n",
        "\n",
        "Attention Range:\n",
        "\n",
        "\n",
        "Now [2.37, 2.44] - too high (outside our target [1.0, 2.0])\n",
        "Still very narrow range (~0.07 difference)\n",
        "Standard deviation remains tiny (  0.005)\n",
        "\n",
        "\n",
        "Feature Detection:\n",
        "\n",
        "\n",
        "Looking at the attention maps, we're getting similar feature detection as before\n",
        "But now we're amplifying everything too much (mean  2.4)\n",
        "\n",
        "Let's modify the attention mechanism to:\n",
        "\n",
        "Better control the output range\n",
        "Increase variance between important and less important regions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DE8ZqNviFfd5"
      },
      "outputs": [],
      "source": [
        "class SelfSupervisedAttention(nn.Module):\n",
        "    def __init__(self, in_channels=64):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        # Feature extraction\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels*2, 3, padding=1),\n",
        "            nn.BatchNorm2d(in_channels*2),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(in_channels*2, in_channels*2, 3, padding=1),\n",
        "            nn.BatchNorm2d(in_channels*2),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        # Channel attention\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "        self.channel_attention = nn.Sequential(\n",
        "            nn.Linear(in_channels*4, in_channels),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(in_channels, in_channels*2),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Final attention\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels*2, in_channels, 3, padding=1),\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(in_channels, 1, 1),\n",
        "        )\n",
        "\n",
        "        # Initialize for moderate attention range\n",
        "        self.base_attention = nn.Parameter(torch.ones(1) * 1.2)  # Base level\n",
        "        self.attention_range = nn.Parameter(torch.ones(1) * 0.3)  # Max deviation\n",
        "        self.temperature = nn.Parameter(torch.ones(1) * 0.1)\n",
        "\n",
        "        self.scale = nn.Parameter(torch.tensor(1.0))  # Default scale factor\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Feature extraction\n",
        "        feat = self.conv1(x)\n",
        "\n",
        "        # Enhanced channel attention\n",
        "        avg_pool = self.avg_pool(feat).squeeze(-1).squeeze(-1)\n",
        "        max_pool = self.max_pool(feat).squeeze(-1).squeeze(-1)\n",
        "        channel_feats = torch.cat([avg_pool, max_pool], dim=1)\n",
        "        channel_weights = self.channel_attention(channel_feats)\n",
        "        channel_weights = channel_weights.view(-1, self.in_channels*2, 1, 1)\n",
        "\n",
        "        feat = feat * channel_weights\n",
        "\n",
        "        # Generate attention map\n",
        "        attention_logits = self.conv2(feat)\n",
        "        attention = torch.tanh(attention_logits / self.temperature)  # Range [-1, 1]\n",
        "\n",
        "        # Scale to desired range around base_attention\n",
        "        attention = self.base_attention + (attention * self.attention_range)\n",
        "\n",
        "        return attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGNXI8zZFho3"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAnkhp39Fiy4"
      },
      "outputs": [],
      "source": [
        "def has_grad(param):\n",
        "    return param.grad is not None\n",
        "\n",
        "\n",
        "def test_updated_attention():\n",
        "   device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "   print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "\n",
        "   # Load base RCAN model\n",
        "   print(\"Loading RCAN model...\")\n",
        "   base_model = load_rcan_model()\n",
        "   base_model = base_model.to(device)\n",
        "\n",
        "   # Create augmented model\n",
        "   print(\"Creating attention-augmented model...\")\n",
        "   model = AttentionAugmentedRCAN(base_model, freeze_base=True)\n",
        "   model = model.to(device)\n",
        "   model.eval()\n",
        "\n",
        "   # Test with real data from Set14\n",
        "   print(\"\\nTesting with real data:\")\n",
        "   val_loader = setup_datasets()\n",
        "\n",
        "   # Test multiple images\n",
        "   all_attention_values = []\n",
        "\n",
        "   with torch.no_grad():\n",
        "       for i, (lr_img, hr_img) in enumerate(val_loader):\n",
        "           if i >= 3:  # Test first 3 images\n",
        "               break\n",
        "\n",
        "           lr_img = lr_img.to(device)\n",
        "           lr_img = lr_img / 255.0  # Scale to [0,1]\n",
        "\n",
        "           # Get model outputs\n",
        "           sr_output, attention = model(lr_img)\n",
        "           all_attention_values.append(attention.cpu().numpy())\n",
        "\n",
        "           # Print stats for each image\n",
        "           print(f\"\\nImage {i+1}:\")\n",
        "           print(f\"Input shape: {lr_img.shape}\")\n",
        "           print(f\"SR output shape: {sr_output.shape}\")\n",
        "           print(f\"Attention range: [{attention.min():.4f}, {attention.max():.4f}]\")\n",
        "           print(f\"Attention mean: {attention.mean():.4f}\")\n",
        "           print(f\"Attention std: {attention.std():.4f}\")\n",
        "\n",
        "           # Visualize\n",
        "           fig = plt.figure(figsize=(20, 5))\n",
        "\n",
        "           # LR input\n",
        "           plt.subplot(1, 4, 1)\n",
        "           lr_img_vis = lr_img[0].cpu().permute(1, 2, 0).numpy()\n",
        "           plt.imshow(lr_img_vis)\n",
        "           plt.title(f\"LR Input {i+1}\")\n",
        "           plt.axis('off')\n",
        "\n",
        "           # SR output\n",
        "           plt.subplot(1, 4, 2)\n",
        "           sr_img_vis = sr_output[0].cpu().permute(1, 2, 0).numpy()\n",
        "           plt.imshow(np.clip(sr_img_vis, 0, 1))\n",
        "           plt.title(\"SR Output\")\n",
        "           plt.axis('off')\n",
        "\n",
        "           # Attention heatmap\n",
        "           plt.subplot(1, 4, 3)\n",
        "           attention_map = attention[0, 0].cpu().numpy()\n",
        "           im = plt.imshow(attention_map, cmap='viridis')\n",
        "           plt.colorbar(im)\n",
        "           plt.title(f\"Attention Map (={attention.mean():.3f}, ={attention.std():.3f})\")\n",
        "           plt.axis('off')\n",
        "\n",
        "           # Attention histogram\n",
        "           plt.subplot(1, 4, 4)\n",
        "           plt.hist(attention.cpu().numpy().flatten(), bins=50, density=True)\n",
        "           plt.title(\"Attention Distribution\")\n",
        "           plt.axvline(attention.mean().item(), color='r', linestyle='--', label='Mean')\n",
        "           plt.axvline(attention.mean().item() + attention.std().item(), color='g', linestyle='--', label='+1 std')\n",
        "           plt.axvline(attention.mean().item() - attention.std().item(), color='g', linestyle='--', label='-1 std')\n",
        "           plt.axvline(model.attention_net.base_attention.item(), color='b', linestyle='--', label='Base Level')\n",
        "           plt.legend()\n",
        "\n",
        "           plt.tight_layout()\n",
        "           plt.show()\n",
        "\n",
        "   # Print overall statistics\n",
        "   all_attention_values = np.concatenate([a.flatten() for a in all_attention_values])\n",
        "   print(\"\\nOverall Attention Statistics:\")\n",
        "   print(f\"Global range: [{np.min(all_attention_values):.4f}, {np.max(all_attention_values):.4f}]\")\n",
        "   print(f\"Global mean: {np.mean(all_attention_values):.4f}\")\n",
        "   print(f\"Global std: {np.std(all_attention_values):.4f}\")\n",
        "\n",
        "   # Print attention parameters\n",
        "   print(\"\\nAttention Parameters:\")\n",
        "   print(f\"Base attention: {model.attention_net.base_attention.item():.4f}\")\n",
        "   print(f\"Attention range: {model.attention_net.attention_range.item():.4f}\")\n",
        "   print(f\"Temperature: {model.attention_net.temperature.item():.4f}\")\n",
        "\n",
        "   # Test gradients\n",
        "   print(\"\\nTesting gradient flow...\")\n",
        "   model.train()\n",
        "   lr_img, hr_img = next(iter(val_loader))\n",
        "   lr_img = lr_img.to(device) / 255.0\n",
        "   hr_img = hr_img.to(device) / 255.0\n",
        "\n",
        "   sr_output, attention = model(lr_img)\n",
        "\n",
        "   # Test with reconstruction loss and attention regularization\n",
        "   recon_loss = F.l1_loss(sr_output, hr_img)\n",
        "   attention_reg = 0.1 * (attention.mean() - model.attention_net.base_attention).abs()\n",
        "   attention_std_reg = 0.1 * (0.1 - attention.std()).clamp(min=0)\n",
        "\n",
        "   loss = recon_loss + attention_reg + attention_std_reg\n",
        "   loss.backward()\n",
        "\n",
        "   print(\"\\nLoss components:\")\n",
        "   print(f\"Reconstruction loss: {recon_loss.item():.4f}\")\n",
        "   print(f\"Attention reg loss: {attention_reg.item():.4f}\")\n",
        "   print(f\"Attention std reg loss: {attention_std_reg.item():.4f}\")\n",
        "\n",
        "   print(\"\\nGradient check:\")\n",
        "   print(\"Attention network gradients:\")\n",
        "   attention_grads = [has_grad(p) for p in model.attention_net.parameters()]\n",
        "   print(f\"Parameters with gradients: {sum(attention_grads)}/{len(attention_grads)}\")\n",
        "\n",
        "   # Check parameter gradients\n",
        "   print(\"\\nParameter gradients:\")\n",
        "   print(f\"Base attention grad: {model.attention_net.base_attention.grad.item():.4f}\")\n",
        "   print(f\"Attention range grad: {model.attention_net.attention_range.grad.item():.4f}\")\n",
        "   print(f\"Temperature grad: {model.attention_net.temperature.grad.item():.4f}\")\n",
        "\n",
        "   return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "   model = test_updated_attention()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3o34Mn99LTTx"
      },
      "source": [
        "Temperature:\n",
        "\n",
        "Increased from 0.05  0.2 to smooth the attention map values, leading to less concentration around the base level.\n",
        "Attention Regularization:\n",
        "\n",
        "Reduced regularization weight (0.1  0.05) to allow the attention values to deviate more dynamically.\n",
        "Attention Std Regularization:\n",
        "\n",
        "Encourages a higher standard deviation (0.05 target) to ensure more variation in the attention map."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJlLiRSyK3Oc"
      },
      "outputs": [],
      "source": [
        "class SelfSupervisedAttention(nn.Module):\n",
        "    def __init__(self, in_channels=64):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        # Feature extraction\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels*2, 3, padding=1),\n",
        "            nn.BatchNorm2d(in_channels*2),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(in_channels*2, in_channels*2, 3, padding=1),\n",
        "            nn.BatchNorm2d(in_channels*2),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        # Channel attention\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "        self.channel_attention = nn.Sequential(\n",
        "            nn.Linear(in_channels*4, in_channels),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(in_channels, in_channels*2),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Final attention\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels*2, in_channels, 3, padding=1),\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(in_channels, 1, 1),\n",
        "        )\n",
        "\n",
        "        # Initialize for moderate attention range\n",
        "        self.base_attention = nn.Parameter(torch.ones(1) * 1.2)\n",
        "        self.attention_range = nn.Parameter(torch.ones(1) * 0.3)\n",
        "        self.temperature = nn.Parameter(torch.ones(1) * 0.2)  # Increased temperature\n",
        "        self.scale = nn.Parameter(torch.tensor(1.0))  # Default scale\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Feature extraction\n",
        "        feat = self.conv1(x)\n",
        "\n",
        "        # Enhanced channel attention\n",
        "        avg_pool = self.avg_pool(feat).squeeze(-1).squeeze(-1)\n",
        "        max_pool = self.max_pool(feat).squeeze(-1).squeeze(-1)\n",
        "        channel_feats = torch.cat([avg_pool, max_pool], dim=1)\n",
        "        channel_weights = self.channel_attention(channel_feats)\n",
        "        channel_weights = channel_weights.view(-1, self.in_channels*2, 1, 1)\n",
        "\n",
        "        feat = feat * channel_weights\n",
        "\n",
        "        # Generate attention map\n",
        "        attention_logits = self.conv2(feat)\n",
        "        attention = torch.tanh(attention_logits / self.temperature)  # Range [-1, 1]\n",
        "\n",
        "        # Scale to desired range around base_attention\n",
        "        attention = self.base_attention + (attention * self.attention_range)\n",
        "\n",
        "        return attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4URqsbuLAZd"
      },
      "outputs": [],
      "source": [
        "def has_grad(param):\n",
        "    return param.grad is not None\n",
        "\n",
        "\n",
        "def test_updated_attention():\n",
        "   device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "   print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "\n",
        "   # Load base RCAN model\n",
        "   print(\"Loading RCAN model...\")\n",
        "   base_model = load_rcan_model()\n",
        "   base_model = base_model.to(device)\n",
        "\n",
        "   # Create augmented model\n",
        "   print(\"Creating attention-augmented model...\")\n",
        "   model = AttentionAugmentedRCAN(base_model, freeze_base=True)\n",
        "   model = model.to(device)\n",
        "   model.eval()\n",
        "\n",
        "   # Test with real data from Set14\n",
        "   print(\"\\nTesting with real data:\")\n",
        "   val_loader = setup_datasets()\n",
        "\n",
        "   # Test multiple images\n",
        "   all_attention_values = []\n",
        "\n",
        "   with torch.no_grad():\n",
        "       for i, (lr_img, hr_img) in enumerate(val_loader):\n",
        "           if i >= 3:  # Test first 3 images\n",
        "               break\n",
        "\n",
        "           lr_img = lr_img.to(device)\n",
        "           lr_img = lr_img / 255.0  # Scale to [0,1]\n",
        "\n",
        "           # Get model outputs\n",
        "           sr_output, attention = model(lr_img)\n",
        "           all_attention_values.append(attention.cpu().numpy())\n",
        "\n",
        "           # Print stats for each image\n",
        "           print(f\"\\nImage {i+1}:\")\n",
        "           print(f\"Input shape: {lr_img.shape}\")\n",
        "           print(f\"SR output shape: {sr_output.shape}\")\n",
        "           print(f\"Attention range: [{attention.min():.4f}, {attention.max():.4f}]\")\n",
        "           print(f\"Attention mean: {attention.mean():.4f}\")\n",
        "           print(f\"Attention std: {attention.std():.4f}\")\n",
        "\n",
        "           # Visualize\n",
        "           fig = plt.figure(figsize=(20, 5))\n",
        "\n",
        "           # LR input\n",
        "           plt.subplot(1, 4, 1)\n",
        "           lr_img_vis = lr_img[0].cpu().permute(1, 2, 0).numpy()\n",
        "           plt.imshow(lr_img_vis)\n",
        "           plt.title(f\"LR Input {i+1}\")\n",
        "           plt.axis('off')\n",
        "\n",
        "           # SR output\n",
        "           plt.subplot(1, 4, 2)\n",
        "           sr_img_vis = sr_output[0].cpu().permute(1, 2, 0).numpy()\n",
        "           plt.imshow(np.clip(sr_img_vis, 0, 1))\n",
        "           plt.title(\"SR Output\")\n",
        "           plt.axis('off')\n",
        "\n",
        "           # Attention heatmap\n",
        "           plt.subplot(1, 4, 3)\n",
        "           attention_map = attention[0, 0].cpu().numpy()\n",
        "           im = plt.imshow(attention_map, cmap='viridis')\n",
        "           plt.colorbar(im)\n",
        "           plt.title(f\"Attention Map (={attention.mean():.3f}, ={attention.std():.3f})\")\n",
        "           plt.axis('off')\n",
        "\n",
        "           # Attention histogram\n",
        "           plt.subplot(1, 4, 4)\n",
        "           plt.hist(attention.cpu().numpy().flatten(), bins=50, density=True)\n",
        "           plt.title(\"Attention Distribution\")\n",
        "           plt.axvline(attention.mean().item(), color='r', linestyle='--', label='Mean')\n",
        "           plt.axvline(attention.mean().item() + attention.std().item(), color='g', linestyle='--', label='+1 std')\n",
        "           plt.axvline(attention.mean().item() - attention.std().item(), color='g', linestyle='--', label='-1 std')\n",
        "           plt.axvline(model.attention_net.base_attention.item(), color='b', linestyle='--', label='Base Level')\n",
        "           plt.legend()\n",
        "\n",
        "           plt.tight_layout()\n",
        "           plt.show()\n",
        "\n",
        "   # Print overall statistics\n",
        "   all_attention_values = np.concatenate([a.flatten() for a in all_attention_values])\n",
        "   print(\"\\nOverall Attention Statistics:\")\n",
        "   print(f\"Global range: [{np.min(all_attention_values):.4f}, {np.max(all_attention_values):.4f}]\")\n",
        "   print(f\"Global mean: {np.mean(all_attention_values):.4f}\")\n",
        "   print(f\"Global std: {np.std(all_attention_values):.4f}\")\n",
        "\n",
        "   # Print attention parameters\n",
        "   print(\"\\nAttention Parameters:\")\n",
        "   print(f\"Base attention: {model.attention_net.base_attention.item():.4f}\")\n",
        "   print(f\"Attention range: {model.attention_net.attention_range.item():.4f}\")\n",
        "   print(f\"Temperature: {model.attention_net.temperature.item():.4f}\")\n",
        "\n",
        "   # Test gradients\n",
        "   print(\"\\nTesting gradient flow...\")\n",
        "   model.train()\n",
        "   lr_img, hr_img = next(iter(val_loader))\n",
        "   lr_img = lr_img.to(device) / 255.0\n",
        "   hr_img = hr_img.to(device) / 255.0\n",
        "\n",
        "   sr_output, attention = model(lr_img)\n",
        "\n",
        "   # Test with reconstruction loss and attention regularization\n",
        "   recon_loss = F.l1_loss(sr_output, hr_img)\n",
        "   attention_reg = 0.1 * (attention.mean() - model.attention_net.base_attention).abs()\n",
        "   attention_std_reg = 0.1 * (0.1 - attention.std()).clamp(min=0)\n",
        "\n",
        "   loss = recon_loss + attention_reg + attention_std_reg\n",
        "   loss.backward()\n",
        "\n",
        "   print(\"\\nLoss components:\")\n",
        "   print(f\"Reconstruction loss: {recon_loss.item():.4f}\")\n",
        "   print(f\"Attention reg loss: {attention_reg.item():.4f}\")\n",
        "   print(f\"Attention std reg loss: {attention_std_reg.item():.4f}\")\n",
        "\n",
        "   print(\"\\nGradient check:\")\n",
        "   print(\"Attention network gradients:\")\n",
        "   attention_grads = [has_grad(p) for p in model.attention_net.parameters()]\n",
        "   print(f\"Parameters with gradients: {sum(attention_grads)}/{len(attention_grads)}\")\n",
        "\n",
        "   # Check parameter gradients\n",
        "   print(\"\\nParameter gradients:\")\n",
        "   print(f\"Base attention grad: {model.attention_net.base_attention.grad.item():.4f}\")\n",
        "   print(f\"Attention range grad: {model.attention_net.attention_range.grad.item():.4f}\")\n",
        "   print(f\"Temperature grad: {model.attention_net.temperature.grad.item():.4f}\")\n",
        "\n",
        "   return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "   model = test_updated_attention()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PRm67LI9SqK"
      },
      "outputs": [],
      "source": [
        "class SelfSupervisedAttention(nn.Module):\n",
        "    def __init__(self, in_channels=64):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        # Feature extraction\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels*2, 3, padding=1),\n",
        "            nn.BatchNorm2d(in_channels*2),\n",
        "            nn.LeakyReLU(0.2, True),  # LeakyReLU for better gradients\n",
        "            nn.Conv2d(in_channels*2, in_channels*2, 3, padding=1),\n",
        "            nn.BatchNorm2d(in_channels*2),\n",
        "            nn.LeakyReLU(0.2, True)\n",
        "        )\n",
        "\n",
        "        # Channel attention with stronger modulation\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "        self.channel_attention = nn.Sequential(\n",
        "            nn.Linear(in_channels*4, in_channels),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Linear(in_channels, in_channels*2),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Final attention with direct range control\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels*2, in_channels, 3, padding=1),\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(in_channels, 1, 1),\n",
        "        )\n",
        "\n",
        "        # More direct control parameters\n",
        "        self.min_attention = nn.Parameter(torch.ones(1) * 0.8)  # Minimum attention\n",
        "        self.max_attention = nn.Parameter(torch.ones(1) * 1.6)  # Maximum attention\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Feature extraction\n",
        "        feat = self.conv1(x)\n",
        "\n",
        "        # Enhanced channel attention\n",
        "        avg_pool = self.avg_pool(feat).squeeze(-1).squeeze(-1)\n",
        "        max_pool = self.max_pool(feat).squeeze(-1).squeeze(-1)\n",
        "        channel_feats = torch.cat([avg_pool, max_pool], dim=1)\n",
        "        channel_weights = self.channel_attention(channel_feats)\n",
        "        channel_weights = channel_weights.view(-1, self.in_channels*2, 1, 1)\n",
        "\n",
        "        feat = feat * channel_weights\n",
        "\n",
        "        # Generate attention map with direct range control\n",
        "        attention_logits = self.conv2(feat)\n",
        "        attention = torch.sigmoid(attention_logits)  # [0,1]\n",
        "\n",
        "        # Scale directly to desired range\n",
        "        attention = self.min_attention + (attention * (self.max_attention - self.min_attention))\n",
        "\n",
        "        return attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHdIM3jK_LX6"
      },
      "outputs": [],
      "source": [
        "def setup_datasets():\n",
        "    \"\"\"Setup validation dataloader for Set14 only\"\"\"\n",
        "    val_dataset = SRDataset(\n",
        "        root_dir='/content/drive/MyDrive/E82/finalproject/Set14',\n",
        "        scale=4,\n",
        "        train=False\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=1,\n",
        "        shuffle=False,\n",
        "        num_workers=1,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    return val_loader\n",
        "\n",
        "class SRDataset(Dataset):\n",
        "    def __init__(self, root_dir, scale=4, train=False):\n",
        "        self.scale = scale\n",
        "\n",
        "        # Mount Google Drive if not already mounted\n",
        "        if not os.path.exists('/content/drive'):\n",
        "            drive.mount('/content/drive')\n",
        "\n",
        "        # Find all images\n",
        "        self.image_files = sorted(glob.glob(os.path.join(root_dir, '*.png')))\n",
        "        if len(self.image_files) == 0:\n",
        "            raise RuntimeError(f\"No PNG images found in {root_dir}\")\n",
        "\n",
        "        print(f\"Found {len(self.image_files)} images in {root_dir}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load HR image\n",
        "        img_path = self.image_files[idx]\n",
        "        hr_image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        # Ensure dimensions are divisible by scale\n",
        "        w, h = hr_image.size\n",
        "        w = w - w % self.scale\n",
        "        h = h - h % self.scale\n",
        "        hr_image = hr_image.crop((0, 0, w, h))\n",
        "\n",
        "        # Convert to tensor (keeping in [0, 255] initially)\n",
        "        hr_tensor = TF.to_tensor(hr_image) * 255.0\n",
        "\n",
        "        # Create LR using bicubic downsampling\n",
        "        lr_tensor = TF.resize(\n",
        "            hr_tensor,\n",
        "            size=[s // self.scale for s in hr_tensor.shape[-2:]],\n",
        "            interpolation=TF.InterpolationMode.BICUBIC\n",
        "        )\n",
        "\n",
        "        return lr_tensor, hr_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rPolkf3-xtW"
      },
      "outputs": [],
      "source": [
        "def test_updated_attention():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load base RCAN model\n",
        "    print(\"Loading RCAN model...\")\n",
        "    base_model = load_rcan_model()\n",
        "    base_model = base_model.to(device)\n",
        "\n",
        "    # Create augmented model\n",
        "    print(\"Creating attention-augmented model...\")\n",
        "    model = AttentionAugmentedRCAN(base_model, freeze_base=True)\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Test with real data from Set14\n",
        "    print(\"\\nTesting with real data:\")\n",
        "    val_loader = setup_datasets()\n",
        "\n",
        "    # Test multiple images\n",
        "    all_attention_values = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (lr_img, hr_img) in enumerate(val_loader):\n",
        "            if i >= 3:  # Test first 3 images\n",
        "                break\n",
        "\n",
        "            lr_img = lr_img.to(device)\n",
        "            lr_img = lr_img / 255.0  # Scale to [0,1]\n",
        "\n",
        "            # Get model outputs\n",
        "            sr_output, attention = model(lr_img)\n",
        "            all_attention_values.append(attention.cpu().numpy())\n",
        "\n",
        "            # Print stats for each image\n",
        "            print(f\"\\nImage {i+1}:\")\n",
        "            print(f\"Input shape: {lr_img.shape}\")\n",
        "            print(f\"SR output shape: {sr_output.shape}\")\n",
        "            print(f\"Attention range: [{attention.min():.4f}, {attention.max():.4f}]\")\n",
        "            print(f\"Attention mean: {attention.mean():.4f}\")\n",
        "            print(f\"Attention std: {attention.std():.4f}\")\n",
        "\n",
        "            # Visualize\n",
        "            fig = plt.figure(figsize=(20, 5))\n",
        "\n",
        "            # LR input\n",
        "            plt.subplot(1, 4, 1)\n",
        "            lr_img_vis = lr_img[0].cpu().permute(1, 2, 0).numpy()\n",
        "            plt.imshow(lr_img_vis)\n",
        "            plt.title(f\"LR Input {i+1}\")\n",
        "            plt.axis('off')\n",
        "\n",
        "            # SR output\n",
        "            plt.subplot(1, 4, 2)\n",
        "            sr_img_vis = sr_output[0].cpu().permute(1, 2, 0).numpy()\n",
        "            plt.imshow(np.clip(sr_img_vis, 0, 1))\n",
        "            plt.title(\"SR Output\")\n",
        "            plt.axis('off')\n",
        "\n",
        "            # Attention heatmap\n",
        "            plt.subplot(1, 4, 3)\n",
        "            attention_map = attention[0, 0].cpu().numpy()\n",
        "            im = plt.imshow(attention_map, cmap='viridis')\n",
        "            plt.colorbar(im)\n",
        "            plt.title(f\"Attention Map (={attention.mean():.3f}, ={attention.std():.3f})\")\n",
        "            plt.axis('off')\n",
        "\n",
        "            # Attention histogram\n",
        "            plt.subplot(1, 4, 4)\n",
        "            plt.hist(attention.cpu().numpy().flatten(), bins=50, density=True)\n",
        "            plt.title(\"Attention Distribution\")\n",
        "            plt.axvline(attention.mean().item(), color='r', linestyle='--', label='Mean')\n",
        "            plt.axvline(attention.mean().item() + attention.std().item(), color='g', linestyle='--', label='+1 std')\n",
        "            plt.axvline(attention.mean().item() - attention.std().item(), color='g', linestyle='--', label='-1 std')\n",
        "            plt.axvline(model.attention_net.min_attention.item(), color='b', linestyle='--', label='Min Level')\n",
        "            plt.axvline(model.attention_net.max_attention.item(), color='b', linestyle='--', label='Max Level')\n",
        "            plt.legend()\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "    # Print overall statistics\n",
        "    all_attention_values = np.concatenate([a.flatten() for a in all_attention_values])\n",
        "    print(\"\\nOverall Attention Statistics:\")\n",
        "    print(f\"Global range: [{np.min(all_attention_values):.4f}, {np.max(all_attention_values):.4f}]\")\n",
        "    print(f\"Global mean: {np.mean(all_attention_values):.4f}\")\n",
        "    print(f\"Global std: {np.std(all_attention_values):.4f}\")\n",
        "\n",
        "    # Print attention parameters\n",
        "    print(\"\\nAttention Parameters:\")\n",
        "    print(f\"Min attention: {model.attention_net.min_attention.item():.4f}\")\n",
        "    print(f\"Max attention: {model.attention_net.max_attention.item():.4f}\")\n",
        "\n",
        "    # Test gradients\n",
        "    print(\"\\nTesting gradient flow...\")\n",
        "    model.train()\n",
        "    lr_img, hr_img = next(iter(val_loader))\n",
        "    lr_img = lr_img.to(device) / 255.0\n",
        "    hr_img = hr_img.to(device) / 255.0\n",
        "\n",
        "    sr_output, attention = model(lr_img)\n",
        "\n",
        "    # Test with reconstruction loss and attention regularization\n",
        "    recon_loss = F.l1_loss(sr_output, hr_img)\n",
        "    attention_std_reg = 0.1 * (0.1 - attention.std()).clamp(min=0)  # Encourage std > 0.1\n",
        "    min_max_reg = 0.1 * F.l1_loss(attention, torch.ones_like(attention) * 1.2)  # Center around 1.2\n",
        "\n",
        "    loss = recon_loss + attention_std_reg + min_max_reg\n",
        "    loss.backward()\n",
        "\n",
        "    print(\"\\nLoss components:\")\n",
        "    print(f\"Reconstruction loss: {recon_loss.item():.4f}\")\n",
        "    print(f\"Attention std reg loss: {attention_std_reg.item():.4f}\")\n",
        "    print(f\"Min-max reg loss: {min_max_reg.item():.4f}\")\n",
        "\n",
        "    print(\"\\nGradient check:\")\n",
        "    print(\"Attention network gradients:\")\n",
        "    attention_grads = [has_grad(p) for p in model.attention_net.parameters()]\n",
        "    print(f\"Parameters with gradients: {sum(attention_grads)}/{len(attention_grads)}\")\n",
        "\n",
        "    # Check parameter gradients\n",
        "    print(\"\\nParameter gradients:\")\n",
        "    print(f\"Min attention grad: {model.attention_net.min_attention.grad.item():.4f}\")\n",
        "    print(f\"Max attention grad: {model.attention_net.max_attention.grad.item():.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model = test_updated_attention()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNWq-YqZ_e6y"
      },
      "source": [
        "The attention is still not working effectively. Even with our min/max range set to [0.8, 1.6], the actual values are tightly clustered around 1.19 with a tiny standard deviation (0.0004-0.0005). This suggests the sigmoid activation is saturating.\n",
        "Let's modify the attention mechanism to be more direct:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRRPyqpI_h9a"
      },
      "outputs": [],
      "source": [
        "class SelfSupervisedAttention(nn.Module):\n",
        "    def __init__(self, in_channels=64):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        # Feature extraction with residual connections\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels*2, 3, padding=1),\n",
        "            nn.BatchNorm2d(in_channels*2),\n",
        "            nn.LeakyReLU(0.2, True)\n",
        "        )\n",
        "\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels*2, in_channels*2, 3, padding=1),\n",
        "            nn.BatchNorm2d(in_channels*2),\n",
        "            nn.LeakyReLU(0.2, True)\n",
        "        )\n",
        "\n",
        "        # Channel attention\n",
        "        self.channel_att = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(in_channels*2, in_channels//2, 1),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(in_channels//2, in_channels*2, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Direct attention prediction\n",
        "        self.attention_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels*2, in_channels, 3, padding=1),\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(in_channels, 1, 1)\n",
        "        )\n",
        "\n",
        "        # Explicit scale parameters\n",
        "        self.attention_scale = nn.Parameter(torch.ones(1) * 0.4)  # Controls variation range\n",
        "        self.attention_bias = nn.Parameter(torch.ones(1) * 1.2)   # Controls center point\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Feature extraction with residual\n",
        "        feat1 = self.conv1(x)\n",
        "        feat2 = self.conv2(feat1)\n",
        "        feat = feat1 + feat2\n",
        "\n",
        "        # Apply channel attention\n",
        "        channel_weights = self.channel_att(feat)\n",
        "        feat = feat * channel_weights\n",
        "\n",
        "        # Generate raw attention values\n",
        "        attention_logits = self.attention_conv(feat)\n",
        "\n",
        "        # Scale attention directly: bias  scale * tanh(logits)\n",
        "        attention = self.attention_bias + (self.attention_scale * torch.tanh(attention_logits))\n",
        "\n",
        "        return attention\n",
        "\n",
        "class AttentionAugmentedRCAN(nn.Module):\n",
        "    def __init__(self, base_rcan, freeze_base=True):\n",
        "        super().__init__()\n",
        "        self.rcan = base_rcan\n",
        "        self.attention_net = SelfSupervisedAttention(64)\n",
        "\n",
        "        if freeze_base:\n",
        "            for param in self.rcan.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, x, mode='inference'):\n",
        "        if mode == 'pre_training':\n",
        "            feats = self.rcan.extract_features(x)\n",
        "            attention = self.attention_net(feats[1])\n",
        "            return attention\n",
        "\n",
        "        input_feat, body_feat = self.rcan.extract_features(x)\n",
        "        attention = self.attention_net(body_feat)\n",
        "        weighted_feat = body_feat * attention\n",
        "        sr_output = self.rcan.complete_sr((input_feat, weighted_feat))\n",
        "        return sr_output, attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHoyQ0OY_nTH"
      },
      "source": [
        "Key changes:\n",
        "\n",
        "Removed sigmoid bottleneck\n",
        "Direct linear scaling using bias  scale * tanh\n",
        "Residual connections in feature extraction\n",
        "Simplified channel attention\n",
        "More explicit control over attention range\n",
        "\n",
        "With bias=1.2 and scale=0.4, we should get:\n",
        "\n",
        "Center point at 1.2\n",
        "Range of approximately [0.8, 1.6] (bias  scale)\n",
        "More pronounced feature differentiation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKVvICcs_sFr"
      },
      "outputs": [],
      "source": [
        "def test_updated_attention():\n",
        "   device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "   print(f\"Using device: {device}\")\n",
        "\n",
        "   # Load base RCAN model\n",
        "   print(\"Loading RCAN model...\")\n",
        "   base_model = load_rcan_model()\n",
        "   base_model = base_model.to(device)\n",
        "\n",
        "   # Create augmented model\n",
        "   print(\"Creating attention-augmented model...\")\n",
        "   model = AttentionAugmentedRCAN(base_model, freeze_base=True)\n",
        "   model = model.to(device)\n",
        "   model.eval()\n",
        "\n",
        "   # Test with real data from Set14\n",
        "   print(\"\\nTesting with real data:\")\n",
        "   val_loader = setup_datasets()\n",
        "\n",
        "   # Test multiple images\n",
        "   all_attention_values = []\n",
        "\n",
        "   with torch.no_grad():\n",
        "       for i, (lr_img, hr_img) in enumerate(val_loader):\n",
        "           if i >= 3:  # Test first 3 images\n",
        "               break\n",
        "\n",
        "           lr_img = lr_img.to(device)\n",
        "           lr_img = lr_img / 255.0  # Scale to [0,1]\n",
        "\n",
        "           # Get model outputs\n",
        "           sr_output, attention = model(lr_img)\n",
        "           all_attention_values.append(attention.cpu().numpy())\n",
        "\n",
        "           # Print stats for each image\n",
        "           print(f\"\\nImage {i+1}:\")\n",
        "           print(f\"Input shape: {lr_img.shape}\")\n",
        "           print(f\"SR output shape: {sr_output.shape}\")\n",
        "           print(f\"Attention range: [{attention.min():.4f}, {attention.max():.4f}]\")\n",
        "           print(f\"Attention mean: {attention.mean():.4f}\")\n",
        "           print(f\"Attention std: {attention.std():.4f}\")\n",
        "\n",
        "           # Visualize\n",
        "           fig = plt.figure(figsize=(20, 5))\n",
        "\n",
        "           # LR input\n",
        "           plt.subplot(1, 4, 1)\n",
        "           lr_img_vis = lr_img[0].cpu().permute(1, 2, 0).numpy()\n",
        "           plt.imshow(lr_img_vis)\n",
        "           plt.title(f\"LR Input {i+1}\")\n",
        "           plt.axis('off')\n",
        "\n",
        "           # SR output\n",
        "           plt.subplot(1, 4, 2)\n",
        "           sr_img_vis = sr_output[0].cpu().permute(1, 2, 0).numpy()\n",
        "           plt.imshow(np.clip(sr_img_vis, 0, 1))\n",
        "           plt.title(\"SR Output\")\n",
        "           plt.axis('off')\n",
        "\n",
        "           # Attention heatmap\n",
        "           plt.subplot(1, 4, 3)\n",
        "           attention_map = attention[0, 0].cpu().numpy()\n",
        "           im = plt.imshow(attention_map, cmap='viridis')\n",
        "           plt.colorbar(im)\n",
        "           plt.title(f\"Attention Map (={attention.mean():.3f}, ={attention.std():.3f})\")\n",
        "           plt.axis('off')\n",
        "\n",
        "           # Attention histogram\n",
        "           plt.subplot(1, 4, 4)\n",
        "           plt.hist(attention.cpu().numpy().flatten(), bins=50, density=True)\n",
        "           plt.title(\"Attention Distribution\")\n",
        "           plt.axvline(attention.mean().item(), color='r', linestyle='--', label='Mean')\n",
        "           plt.axvline(attention.mean().item() + attention.std().item(), color='g', linestyle='--', label='+1 std')\n",
        "           plt.axvline(attention.mean().item() - attention.std().item(), color='g', linestyle='--', label='-1 std')\n",
        "           plt.axvline(model.attention_net.attention_bias.item(), color='b', linestyle='--', label='Bias')\n",
        "           plt.legend()\n",
        "\n",
        "           plt.tight_layout()\n",
        "           plt.show()\n",
        "\n",
        "   # Print overall statistics\n",
        "   all_attention_values = np.concatenate([a.flatten() for a in all_attention_values])\n",
        "   print(\"\\nOverall Attention Statistics:\")\n",
        "   print(f\"Global range: [{np.min(all_attention_values):.4f}, {np.max(all_attention_values):.4f}]\")\n",
        "   print(f\"Global mean: {np.mean(all_attention_values):.4f}\")\n",
        "   print(f\"Global std: {np.std(all_attention_values):.4f}\")\n",
        "\n",
        "   # Print attention parameters\n",
        "   print(\"\\nAttention Parameters:\")\n",
        "   print(f\"Bias: {model.attention_net.attention_bias.item():.4f}\")\n",
        "   print(f\"Scale: {model.attention_net.attention_scale.item():.4f}\")\n",
        "\n",
        "   # Test gradients\n",
        "   print(\"\\nTesting gradient flow...\")\n",
        "   model.train()\n",
        "   lr_img, hr_img = next(iter(val_loader))\n",
        "   lr_img = lr_img.to(device) / 255.0\n",
        "   hr_img = hr_img.to(device) / 255.0\n",
        "\n",
        "   sr_output, attention = model(lr_img)\n",
        "\n",
        "   # Test with reconstruction loss and attention regularization\n",
        "   recon_loss = F.l1_loss(sr_output, hr_img)\n",
        "   attention_std_reg = 0.1 * (0.1 - attention.std()).clamp(min=0)  # Encourage std > 0.1\n",
        "   attention_reg = 0.1 * F.l1_loss(attention, torch.ones_like(attention) * 1.2)  # Center around target\n",
        "\n",
        "   loss = recon_loss + attention_std_reg + attention_reg\n",
        "   loss.backward()\n",
        "\n",
        "   print(\"\\nLoss components:\")\n",
        "   print(f\"Reconstruction loss: {recon_loss.item():.4f}\")\n",
        "   print(f\"Attention std reg loss: {attention_std_reg.item():.4f}\")\n",
        "   print(f\"Attention reg loss: {attention_reg.item():.4f}\")\n",
        "\n",
        "   print(\"\\nGradient check:\")\n",
        "   print(\"Attention network gradients:\")\n",
        "   attention_grads = [has_grad(p) for p in model.attention_net.parameters()]\n",
        "   print(f\"Parameters with gradients: {sum(attention_grads)}/{len(attention_grads)}\")\n",
        "\n",
        "   # Check parameter gradients\n",
        "   print(\"\\nParameter gradients:\")\n",
        "   print(f\"Bias gradient: {model.attention_net.attention_bias.grad.item():.4f}\")\n",
        "   print(f\"Scale gradient: {model.attention_net.attention_scale.grad.item():.4f}\")\n",
        "\n",
        "   return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "   model = test_updated_attention()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not getting anywhere.  We will try an ensemble approach that combines multiple attention maps."
      ],
      "metadata": {
        "id": "6hTNgzzYADxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_ensemble_attention():\n",
        "   device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "   print(f\"Using device: {device}\")\n",
        "\n",
        "   class SelfSupervisedAttention(nn.Module):\n",
        "       def __init__(self, in_channels=64):\n",
        "           super().__init__()\n",
        "           self.in_channels = in_channels\n",
        "\n",
        "           # Stronger feature extraction\n",
        "           self.feat_conv = nn.Sequential(\n",
        "               nn.Conv2d(in_channels, in_channels*2, 3, padding=1),\n",
        "               nn.LeakyReLU(0.2, True),\n",
        "               nn.Conv2d(in_channels*2, in_channels*2, 3, padding=1),\n",
        "               nn.LeakyReLU(0.2, True)\n",
        "           )\n",
        "\n",
        "           # Multiple attention heads with different kernel sizes\n",
        "           self.heads = nn.ModuleList([\n",
        "               # Head 1: Local features (3x3)\n",
        "               nn.Sequential(\n",
        "                   nn.Conv2d(in_channels*2, in_channels, 3, padding=1),\n",
        "                   nn.LeakyReLU(0.2, True),\n",
        "                   nn.Conv2d(in_channels, 1, 3, padding=1)\n",
        "               ),\n",
        "               # Head 2: Medium features (5x5)\n",
        "               nn.Sequential(\n",
        "                   nn.Conv2d(in_channels*2, in_channels, 5, padding=2),\n",
        "                   nn.LeakyReLU(0.2, True),\n",
        "                   nn.Conv2d(in_channels, 1, 5, padding=2)\n",
        "               ),\n",
        "               # Head 3: Global features (7x7)\n",
        "               nn.Sequential(\n",
        "                   nn.Conv2d(in_channels*2, in_channels, 7, padding=3),\n",
        "                   nn.LeakyReLU(0.2, True),\n",
        "                   nn.Conv2d(in_channels, 1, 7, padding=3)\n",
        "               )\n",
        "           ])\n",
        "\n",
        "           # Initialize head weights for different scales\n",
        "           self.head_weights = nn.Parameter(torch.tensor([1.0, 0.7, 0.4]))\n",
        "\n",
        "           # Range control\n",
        "           self.min_attention = nn.Parameter(torch.tensor(0.8))\n",
        "           self.max_attention = nn.Parameter(torch.tensor(1.6))\n",
        "\n",
        "       def forward(self, x):\n",
        "           # Extract features\n",
        "           feat = self.feat_conv(x)\n",
        "\n",
        "           # Get attention from each head\n",
        "           attention_maps = [head(feat) for head in self.heads]\n",
        "\n",
        "           # Combine with softmax weights\n",
        "           weights = F.softmax(self.head_weights, dim=0)\n",
        "           combined = sum(w * torch.tanh(m) for w, m in zip(weights, attention_maps))  # Use tanh for each head\n",
        "\n",
        "           # Scale to desired range\n",
        "           attention_range = self.max_attention - self.min_attention\n",
        "           attention = self.min_attention + (torch.sigmoid(combined) * attention_range)\n",
        "\n",
        "           return attention, attention_maps\n",
        "\n",
        "   class AttentionAugmentedRCAN(nn.Module):\n",
        "       def __init__(self, base_rcan, freeze_base=True):\n",
        "           super().__init__()\n",
        "           self.rcan = base_rcan\n",
        "           self.attention_net = SelfSupervisedAttention(64)\n",
        "\n",
        "           if freeze_base:\n",
        "               for param in self.rcan.parameters():\n",
        "                   param.requires_grad = False\n",
        "\n",
        "       def forward(self, x, mode='inference'):\n",
        "           if mode == 'pre_training':\n",
        "               feats = self.rcan.extract_features(x)\n",
        "               attention, _ = self.attention_net(feats[1])\n",
        "               return attention\n",
        "\n",
        "           input_feat, body_feat = self.rcan.extract_features(x)\n",
        "           attention, attention_maps = self.attention_net(body_feat)\n",
        "           weighted_feat = body_feat * attention\n",
        "           sr_output = self.rcan.complete_sr((input_feat, weighted_feat))\n",
        "           return sr_output, attention, attention_maps\n",
        "\n",
        "   # Load RCAN model\n",
        "   print(\"Loading RCAN model...\")\n",
        "   base_model = load_rcan_model()\n",
        "   base_model = base_model.to(device)\n",
        "\n",
        "   # Create augmented model\n",
        "   print(\"Creating attention-augmented model...\")\n",
        "   model = AttentionAugmentedRCAN(base_model, freeze_base=True)\n",
        "   model = model.to(device)\n",
        "   model.eval()\n",
        "\n",
        "   # Test with real data from Set14\n",
        "   print(\"\\nTesting with real data:\")\n",
        "   val_loader = setup_datasets()\n",
        "\n",
        "   # Test multiple images\n",
        "   all_attention_values = []\n",
        "\n",
        "   with torch.no_grad():\n",
        "       for i, (lr_img, hr_img) in enumerate(val_loader):\n",
        "           if i >= 3:  # Test first 3 images\n",
        "               break\n",
        "\n",
        "           lr_img = lr_img.to(device)\n",
        "           lr_img = lr_img / 255.0  # Scale to [0,1]\n",
        "\n",
        "           # Get model outputs\n",
        "           sr_output, attention, attention_maps = model(lr_img)\n",
        "           all_attention_values.append(attention.cpu().numpy())\n",
        "\n",
        "           # Print stats for each image\n",
        "           print(f\"\\nImage {i+1}:\")\n",
        "           print(f\"SR output shape: {sr_output.shape}\")\n",
        "           print(f\"Attention range: [{attention.min():.4f}, {attention.max():.4f}]\")\n",
        "           print(f\"Attention mean: {attention.mean():.4f}\")\n",
        "           print(f\"Attention std: {attention.std():.4f}\")\n",
        "\n",
        "           # Print head weights and ranges\n",
        "           weights = F.softmax(model.attention_net.head_weights, dim=0)\n",
        "           print(f\"Head weights: {weights.detach().cpu().numpy()}\")\n",
        "           for j, maps in enumerate(attention_maps):\n",
        "               print(f\"Head {j+1} range: [{maps.min():.4f}, {maps.max():.4f}]\")\n",
        "\n",
        "           # Visualize results\n",
        "           fig = plt.figure(figsize=(20, 8))\n",
        "\n",
        "           # LR input\n",
        "           plt.subplot(2, 3, 1)\n",
        "           lr_img_vis = lr_img[0].cpu().permute(1, 2, 0).numpy()\n",
        "           plt.imshow(lr_img_vis)\n",
        "           plt.title(f\"LR Input {i+1}\")\n",
        "           plt.axis('off')\n",
        "\n",
        "           # SR output\n",
        "           plt.subplot(2, 3, 2)\n",
        "           sr_img_vis = sr_output[0].cpu().permute(1, 2, 0).numpy()\n",
        "           plt.imshow(np.clip(sr_img_vis, 0, 1))\n",
        "           plt.title(\"SR Output\")\n",
        "           plt.axis('off')\n",
        "\n",
        "           # Combined attention heatmap\n",
        "           plt.subplot(2, 3, 3)\n",
        "           attention_map = attention[0, 0].cpu().numpy()\n",
        "           im = plt.imshow(attention_map, cmap='viridis')\n",
        "           plt.colorbar(im)\n",
        "           plt.title(f\"Combined Attention (={attention.mean():.3f}, ={attention.std():.3f})\")\n",
        "           plt.axis('off')\n",
        "\n",
        "           # Individual head outputs\n",
        "           for j, head_map in enumerate(attention_maps):\n",
        "               plt.subplot(2, 3, 4+j)\n",
        "               head_viz = head_map[0, 0].cpu().numpy()\n",
        "               plt.imshow(head_viz, cmap='viridis')\n",
        "               plt.colorbar()\n",
        "               plt.title(f\"Head {j+1} - {['3x3', '5x5', '7x7'][j]} (w: {weights[j]:.2f})\")\n",
        "               plt.axis('off')\n",
        "\n",
        "           plt.tight_layout()\n",
        "           plt.show()\n",
        "\n",
        "   # Print overall statistics\n",
        "   all_attention_values = np.concatenate([a.flatten() for a in all_attention_values])\n",
        "   print(\"\\nOverall Attention Statistics:\")\n",
        "   print(f\"Global range: [{np.min(all_attention_values):.4f}, {np.max(all_attention_values):.4f}]\")\n",
        "   print(f\"Global mean: {np.mean(all_attention_values):.4f}\")\n",
        "   print(f\"Global std: {np.std(all_attention_values):.4f}\")\n",
        "\n",
        "   # Print attention parameters\n",
        "   print(\"\\nAttention Parameters:\")\n",
        "   print(f\"Min attention: {model.attention_net.min_attention.item():.4f}\")\n",
        "   print(f\"Max attention: {model.attention_net.max_attention.item():.4f}\")\n",
        "\n",
        "   return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "   model = test_ensemble_attention()"
      ],
      "metadata": {
        "id": "T45cFBX9AJIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results show that we're still not getting enough variation in the attention maps:\n",
        "\n",
        "Attention Range Issues:\n",
        "\n",
        "Range is narrow: [1.1985, 1.2043] (spread of only ~0.006)\n",
        "\n",
        "Standard deviation is tiny: 0.0005\n",
        "\n",
        "We're not using nearly the full range we set (0.8 to 1.6)\n",
        "\n",
        "\n",
        "Head Behavior:\n",
        "\n",
        "Head weights are working (different contributions: 44%, 32%, 24%)\n",
        "Individual head ranges are very small (around 0.04)\n",
        "All heads seem to be producing similar patterns\n",
        "\n",
        "\n",
        "Problems to Fix:\n",
        "\n",
        "The sigmoid/tanh activations might be squashing our values too much\n",
        "The range control isn't effective\n",
        "The heads aren't learning sufficiently different features"
      ],
      "metadata": {
        "id": "KTumPOyKBpAx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key changes:\n",
        "\n",
        "Removed sigmoid/tanh activations\n",
        "Direct scaling of head outputs\n",
        "Different initial scales for each head (0.3, 0.2, 0.1)\n",
        "Hard clamping to ensure reasonable range\n",
        "Simpler combination strategy"
      ],
      "metadata": {
        "id": "EhKJIYGaCVH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_direct_attention():\n",
        "   device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "   print(f\"Using device: {device}\")\n",
        "\n",
        "   class SelfSupervisedAttention(nn.Module):\n",
        "       def __init__(self, in_channels=64):\n",
        "           super().__init__()\n",
        "           self.in_channels = in_channels\n",
        "\n",
        "           # Feature extraction\n",
        "           self.feat_conv = nn.Sequential(\n",
        "               nn.Conv2d(in_channels, in_channels*2, 3, padding=1),\n",
        "               nn.LeakyReLU(0.2, True),\n",
        "               nn.Conv2d(in_channels*2, in_channels*2, 3, padding=1),\n",
        "               nn.LeakyReLU(0.2, True)\n",
        "           )\n",
        "\n",
        "           # Three heads with different receptive fields\n",
        "           self.heads = nn.ModuleList([\n",
        "               # Local features\n",
        "               nn.Sequential(\n",
        "                   nn.Conv2d(in_channels*2, in_channels, 3, padding=1),\n",
        "                   nn.LeakyReLU(0.2, True),\n",
        "                   nn.Conv2d(in_channels, 1, 1)\n",
        "               ),\n",
        "               # Medium features\n",
        "               nn.Sequential(\n",
        "                   nn.Conv2d(in_channels*2, in_channels, 5, padding=2),\n",
        "                   nn.LeakyReLU(0.2, True),\n",
        "                   nn.Conv2d(in_channels, 1, 1)\n",
        "               ),\n",
        "               # Global features\n",
        "               nn.Sequential(\n",
        "                   nn.Conv2d(in_channels*2, in_channels, 7, padding=3),\n",
        "                   nn.LeakyReLU(0.2, True),\n",
        "                   nn.Conv2d(in_channels, 1, 1)\n",
        "               )\n",
        "           ])\n",
        "\n",
        "           # Direct scaling parameters for each head\n",
        "           self.head_scales = nn.Parameter(torch.tensor([0.3, 0.2, 0.1]))  # Different initial scales\n",
        "           self.base_attention = nn.Parameter(torch.tensor(1.2))  # Base attention level\n",
        "\n",
        "       def forward(self, x):\n",
        "           # Extract features\n",
        "           feat = self.feat_conv(x)\n",
        "\n",
        "           # Get raw attention from each head\n",
        "           attention_maps = [head(feat) for head in self.heads]\n",
        "\n",
        "           # Scale and combine heads directly\n",
        "           scaled_maps = []\n",
        "           for map, scale in zip(attention_maps, self.head_scales):\n",
        "               scaled_maps.append(map * scale)\n",
        "\n",
        "           # Sum maps and add to base attention\n",
        "           attention = self.base_attention + sum(scaled_maps)\n",
        "\n",
        "           # Ensure reasonable range\n",
        "           attention = attention.clamp(0.8, 1.6)\n",
        "\n",
        "           return attention, attention_maps\n",
        "\n",
        "   class AttentionAugmentedRCAN(nn.Module):\n",
        "       def __init__(self, base_rcan, freeze_base=True):\n",
        "           super().__init__()\n",
        "           self.rcan = base_rcan\n",
        "           self.attention_net = SelfSupervisedAttention(64)\n",
        "\n",
        "           if freeze_base:\n",
        "               for param in self.rcan.parameters():\n",
        "                   param.requires_grad = False\n",
        "\n",
        "       def forward(self, x, mode='inference'):\n",
        "           if mode == 'pre_training':\n",
        "               feats = self.rcan.extract_features(x)\n",
        "               attention, _ = self.attention_net(feats[1])\n",
        "               return attention\n",
        "\n",
        "           input_feat, body_feat = self.rcan.extract_features(x)\n",
        "           attention, attention_maps = self.attention_net(body_feat)\n",
        "           weighted_feat = body_feat * attention\n",
        "           sr_output = self.rcan.complete_sr((input_feat, weighted_feat))\n",
        "           return sr_output, attention, attention_maps\n",
        "\n",
        "   # Load RCAN model\n",
        "   print(\"Loading RCAN model...\")\n",
        "   base_model = load_rcan_model()\n",
        "   base_model = base_model.to(device)\n",
        "\n",
        "   # Create augmented model\n",
        "   print(\"Creating attention-augmented model...\")\n",
        "   model = AttentionAugmentedRCAN(base_model, freeze_base=True)\n",
        "   model = model.to(device)\n",
        "   model.eval()\n",
        "\n",
        "   # Test with real data\n",
        "   print(\"\\nTesting with real data:\")\n",
        "   val_loader = setup_datasets()\n",
        "\n",
        "   # Test multiple images\n",
        "   all_attention_values = []\n",
        "\n",
        "   with torch.no_grad():\n",
        "       for i, (lr_img, hr_img) in enumerate(val_loader):\n",
        "           if i >= 3:  # Test first 3 images\n",
        "               break\n",
        "\n",
        "           lr_img = lr_img.to(device)\n",
        "           lr_img = lr_img / 255.0\n",
        "\n",
        "           # Get model outputs\n",
        "           sr_output, attention, attention_maps = model(lr_img)\n",
        "           all_attention_values.append(attention.cpu().numpy())\n",
        "\n",
        "           # Print stats\n",
        "           print(f\"\\nImage {i+1}:\")\n",
        "           print(f\"SR output shape: {sr_output.shape}\")\n",
        "           print(f\"Attention range: [{attention.min():.4f}, {attention.max():.4f}]\")\n",
        "           print(f\"Attention mean: {attention.mean():.4f}\")\n",
        "           print(f\"Attention std: {attention.std():.4f}\")\n",
        "\n",
        "           # Print head scales and ranges\n",
        "           print(f\"Head scales: {model.attention_net.head_scales.detach().cpu().numpy()}\")\n",
        "           for j, maps in enumerate(attention_maps):\n",
        "               print(f\"Head {j+1} raw range: [{maps.min():.4f}, {maps.max():.4f}]\")\n",
        "\n",
        "           # Visualize\n",
        "           fig = plt.figure(figsize=(20, 8))\n",
        "\n",
        "           # Input and output\n",
        "           plt.subplot(2, 3, 1)\n",
        "           lr_img_vis = lr_img[0].cpu().permute(1, 2, 0).numpy()\n",
        "           plt.imshow(lr_img_vis)\n",
        "           plt.title(f\"LR Input {i+1}\")\n",
        "           plt.axis('off')\n",
        "\n",
        "           plt.subplot(2, 3, 2)\n",
        "           sr_img_vis = sr_output[0].cpu().permute(1, 2, 0).numpy()\n",
        "           plt.imshow(np.clip(sr_img_vis, 0, 1))\n",
        "           plt.title(\"SR Output\")\n",
        "           plt.axis('off')\n",
        "\n",
        "           # Attention maps\n",
        "           plt.subplot(2, 3, 3)\n",
        "           attention_map = attention[0, 0].cpu().numpy()\n",
        "           im = plt.imshow(attention_map, cmap='viridis')\n",
        "           plt.colorbar(im)\n",
        "           plt.title(f\"Combined Attention\\n={attention.mean():.3f}, ={attention.std():.3f}\")\n",
        "           plt.axis('off')\n",
        "\n",
        "           # Individual head outputs\n",
        "           for j, head_map in enumerate(attention_maps):\n",
        "               plt.subplot(2, 3, 4+j)\n",
        "               head_viz = head_map[0, 0].cpu().numpy()\n",
        "               scale = model.attention_net.head_scales[j].item()\n",
        "               plt.imshow(head_viz, cmap='viridis')\n",
        "               plt.colorbar()\n",
        "               plt.title(f\"Head {j+1} (scale: {scale:.3f})\")\n",
        "               plt.axis('off')\n",
        "\n",
        "           plt.tight_layout()\n",
        "           plt.show()\n",
        "\n",
        "   print(\"\\nOverall Attention Statistics:\")\n",
        "   all_attention_values = np.concatenate([a.flatten() for a in all_attention_values])\n",
        "   print(f\"Global range: [{np.min(all_attention_values):.4f}, {np.max(all_attention_values):.4f}]\")\n",
        "   print(f\"Global mean: {np.mean(all_attention_values):.4f}\")\n",
        "   print(f\"Global std: {np.std(all_attention_values):.4f}\")\n",
        "\n",
        "   print(\"\\nAttention Parameters:\")\n",
        "   print(f\"Base attention: {model.attention_net.base_attention.item():.4f}\")\n",
        "\n",
        "   return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "   model = test_direct_attention()"
      ],
      "metadata": {
        "id": "G7F4lUvVCCNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Still having issues with very narrow attention range and std.\n",
        "\n",
        "\n",
        "Key changes:\n",
        "\n",
        "More channels in feature extraction (64128256)\n",
        "Tanh activation for full [-1,1] range from heads\n",
        "Doubled the scaling factors (0.30.6, etc.)\n",
        "Wider clamping range (0.5 to 2.0)\n",
        "Base attention at 1.0 for more room to move up/down\n",
        "\n",
        "This should give us:\n",
        "\n",
        "Much larger attention variations\n",
        "Stronger feature enhancement\n",
        "Clearer differentiation between regions"
      ],
      "metadata": {
        "id": "JgDzf6sKARLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_direct_attention():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    class SelfSupervisedAttention(nn.Module):\n",
        "        def __init__(self, in_channels=64):\n",
        "            super().__init__()\n",
        "            self.in_channels = in_channels\n",
        "\n",
        "            # Stronger feature extraction\n",
        "            self.feat_conv = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, in_channels*2, 3, padding=1),\n",
        "                nn.LeakyReLU(0.2, True),\n",
        "                nn.Conv2d(in_channels*2, in_channels*4, 3, padding=1),  # More channels\n",
        "                nn.LeakyReLU(0.2, True)\n",
        "            )\n",
        "\n",
        "            # Three heads with different receptive fields and stronger outputs\n",
        "            self.heads = nn.ModuleList([\n",
        "                # Local features (aggressive)\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(in_channels*4, in_channels*2, 3, padding=1),\n",
        "                    nn.LeakyReLU(0.2, True),\n",
        "                    nn.Conv2d(in_channels*2, 1, 1),\n",
        "                    nn.Tanh()  # Force full range\n",
        "                ),\n",
        "                # Medium features (balanced)\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(in_channels*4, in_channels*2, 5, padding=2),\n",
        "                    nn.LeakyReLU(0.2, True),\n",
        "                    nn.Conv2d(in_channels*2, 1, 1),\n",
        "                    nn.Tanh()\n",
        "                ),\n",
        "                # Global features (subtle)\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(in_channels*4, in_channels*2, 7, padding=3),\n",
        "                    nn.LeakyReLU(0.2, True),\n",
        "                    nn.Conv2d(in_channels*2, 1, 1),\n",
        "                    nn.Tanh()\n",
        "                )\n",
        "            ])\n",
        "\n",
        "            # Much larger scales for more dramatic effect\n",
        "            self.head_scales = nn.Parameter(torch.tensor([0.6, 0.4, 0.2]))  # Double the scales\n",
        "            self.base_attention = nn.Parameter(torch.tensor(1.0))  # Start at 1.0\n",
        "\n",
        "        def forward(self, x):\n",
        "            # Extract features\n",
        "            feat = self.feat_conv(x)\n",
        "\n",
        "            # Get raw attention from each head (now in [-1,1] range due to tanh)\n",
        "            attention_maps = [head(feat) for head in self.heads]\n",
        "\n",
        "            # Scale and combine heads directly\n",
        "            scaled_maps = []\n",
        "            for map, scale in zip(attention_maps, self.head_scales):\n",
        "                scaled_maps.append(map * scale)  # Will give 0.6, 0.4, 0.2\n",
        "\n",
        "            # Sum maps and add to base attention with wider range\n",
        "            attention = self.base_attention + sum(scaled_maps)  # Range should be [0.0, 2.0]\n",
        "\n",
        "            # Ensure reasonable range but allow more variation\n",
        "            attention = attention.clamp(0.5, 2.0)\n",
        "\n",
        "            return attention, attention_maps\n",
        "\n",
        "    class AttentionAugmentedRCAN(nn.Module):\n",
        "        def __init__(self, base_rcan, freeze_base=True):\n",
        "            super().__init__()\n",
        "            self.rcan = base_rcan\n",
        "            self.attention_net = SelfSupervisedAttention(64)\n",
        "\n",
        "            if freeze_base:\n",
        "                for param in self.rcan.parameters():\n",
        "                    param.requires_grad = False\n",
        "\n",
        "        def forward(self, x, mode='inference'):\n",
        "            if mode == 'pre_training':\n",
        "                feats = self.rcan.extract_features(x)\n",
        "                attention, _ = self.attention_net(feats[1])\n",
        "                return attention\n",
        "\n",
        "            input_feat, body_feat = self.rcan.extract_features(x)\n",
        "            attention, attention_maps = self.attention_net(body_feat)\n",
        "            weighted_feat = body_feat * attention\n",
        "            sr_output = self.rcan.complete_sr((input_feat, weighted_feat))\n",
        "            return sr_output, attention, attention_maps\n",
        "\n",
        "    # Load RCAN model\n",
        "    print(\"Loading RCAN model...\")\n",
        "    base_model = load_rcan_model()\n",
        "    base_model = base_model.to(device)\n",
        "\n",
        "    # Create augmented model\n",
        "    print(\"Creating attention-augmented model...\")\n",
        "    model = AttentionAugmentedRCAN(base_model, freeze_base=True)\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Test with real data\n",
        "    print(\"\\nTesting with real data:\")\n",
        "    val_loader = setup_datasets()\n",
        "\n",
        "    # Test multiple images\n",
        "    all_attention_values = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (lr_img, hr_img) in enumerate(val_loader):\n",
        "            if i >= 3:  # Test first 3 images\n",
        "                break\n",
        "\n",
        "            lr_img = lr_img.to(device)\n",
        "            lr_img = lr_img / 255.0\n",
        "\n",
        "            # Get model outputs\n",
        "            sr_output, attention, attention_maps = model(lr_img)\n",
        "            all_attention_values.append(attention.cpu().numpy())\n",
        "\n",
        "            # Print stats\n",
        "            print(f\"\\nImage {i+1}:\")\n",
        "            print(f\"SR output shape: {sr_output.shape}\")\n",
        "            print(f\"Attention range: [{attention.min():.4f}, {attention.max():.4f}]\")\n",
        "            print(f\"Attention mean: {attention.mean():.4f}\")\n",
        "            print(f\"Attention std: {attention.std():.4f}\")\n",
        "\n",
        "            # Print head scales and ranges\n",
        "            print(f\"Head scales: {model.attention_net.head_scales.detach().cpu().numpy()}\")\n",
        "            for j, maps in enumerate(attention_maps):\n",
        "                print(f\"Head {j+1} raw range: [{maps.min():.4f}, {maps.max():.4f}]\")\n",
        "\n",
        "            # Visualize\n",
        "            fig = plt.figure(figsize=(20, 8))\n",
        "\n",
        "            # Input and output\n",
        "            plt.subplot(2, 3, 1)\n",
        "            lr_img_vis = lr_img[0].cpu().permute(1, 2, 0).numpy()\n",
        "            plt.imshow(lr_img_vis)\n",
        "            plt.title(f\"LR Input {i+1}\")\n",
        "            plt.axis('off')\n",
        "\n",
        "            plt.subplot(2, 3, 2)\n",
        "            sr_img_vis = sr_output[0].cpu().permute(1, 2, 0).numpy()\n",
        "            plt.imshow(np.clip(sr_img_vis, 0, 1))\n",
        "            plt.title(\"SR Output\")\n",
        "            plt.axis('off')\n",
        "\n",
        "            # Attention maps\n",
        "            plt.subplot(2, 3, 3)\n",
        "            attention_map = attention[0, 0].cpu().numpy()\n",
        "            im = plt.imshow(attention_map, cmap='viridis')\n",
        "            plt.colorbar(im)\n",
        "            plt.title(f\"Combined Attention\\n={attention.mean():.3f}, ={attention.std():.3f}\")\n",
        "            plt.axis('off')\n",
        "\n",
        "            # Individual head outputs\n",
        "            for j, head_map in enumerate(attention_maps):\n",
        "                plt.subplot(2, 3, 4+j)\n",
        "                head_viz = head_map[0, 0].cpu().numpy()\n",
        "                scale = model.attention_net.head_scales[j].item()\n",
        "                plt.imshow(head_viz, cmap='viridis')\n",
        "                plt.colorbar()\n",
        "                plt.title(f\"Head {j+1} (scale: {scale:.3f})\")\n",
        "                plt.axis('off')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "    print(\"\\nOverall Attention Statistics:\")\n",
        "    all_attention_values = np.concatenate([a.flatten() for a in all_attention_values])\n",
        "    print(f\"Global range: [{np.min(all_attention_values):.4f}, {np.max(all_attention_values):.4f}]\")\n",
        "    print(f\"Global mean: {np.mean(all_attention_values):.4f}\")\n",
        "    print(f\"Global std: {np.std(all_attention_values):.4f}\")\n",
        "\n",
        "    print(\"\\nAttention Parameters:\")\n",
        "    print(f\"Base attention: {model.attention_net.base_attention.item():.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model = test_direct_attention()"
      ],
      "metadata": {
        "id": "weInjfHiGneP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some minor improvement, but let's do a full test of the model now."
      ],
      "metadata": {
        "id": "fb35P7_YHRr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_for_paper():\n",
        "   device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "   print(f\"Using device: {device}\")\n",
        "\n",
        "   class SelfSupervisedAttention(nn.Module):\n",
        "       def __init__(self, in_channels=64):\n",
        "           super().__init__()\n",
        "           self.in_channels = in_channels\n",
        "\n",
        "           # Feature extraction\n",
        "           self.feat_conv = nn.Sequential(\n",
        "               nn.Conv2d(in_channels, in_channels*2, 3, padding=1),\n",
        "               nn.LeakyReLU(0.2, True),\n",
        "               nn.Conv2d(in_channels*2, in_channels*2, 3, padding=1),\n",
        "               nn.LeakyReLU(0.2, True)\n",
        "           )\n",
        "\n",
        "           # Three heads with different receptive fields\n",
        "           self.heads = nn.ModuleList([\n",
        "               # Local features\n",
        "               nn.Sequential(\n",
        "                   nn.Conv2d(in_channels*2, in_channels, 3, padding=1),\n",
        "                   nn.LeakyReLU(0.2, True),\n",
        "                   nn.Conv2d(in_channels, 1, 1)\n",
        "               ),\n",
        "               # Medium features\n",
        "               nn.Sequential(\n",
        "                   nn.Conv2d(in_channels*2, in_channels, 5, padding=2),\n",
        "                   nn.LeakyReLU(0.2, True),\n",
        "                   nn.Conv2d(in_channels, 1, 1)\n",
        "               ),\n",
        "               # Global features\n",
        "               nn.Sequential(\n",
        "                   nn.Conv2d(in_channels*2, in_channels, 7, padding=3),\n",
        "                   nn.LeakyReLU(0.2, True),\n",
        "                   nn.Conv2d(in_channels, 1, 1)\n",
        "               )\n",
        "           ])\n",
        "\n",
        "           # Direct scaling parameters for each head\n",
        "           self.head_scales = nn.Parameter(torch.tensor([0.3, 0.2, 0.1]))  # Different initial scales\n",
        "           self.base_attention = nn.Parameter(torch.tensor(1.2))  # Base attention level\n",
        "\n",
        "       def forward(self, x):\n",
        "           # Extract features\n",
        "           feat = self.feat_conv(x)\n",
        "\n",
        "           # Get raw attention from each head\n",
        "           attention_maps = [head(feat) for head in self.heads]\n",
        "\n",
        "           # Scale and combine heads directly\n",
        "           scaled_maps = []\n",
        "           for map, scale in zip(attention_maps, self.head_scales):\n",
        "               scaled_maps.append(map * scale)\n",
        "\n",
        "           # Sum maps and add to base attention\n",
        "           attention = self.base_attention + sum(scaled_maps)\n",
        "\n",
        "           # Ensure reasonable range\n",
        "           attention = attention.clamp(0.8, 1.6)\n",
        "\n",
        "           return attention, attention_maps\n",
        "\n",
        "   class AttentionAugmentedRCAN(nn.Module):\n",
        "       def __init__(self, base_rcan, freeze_base=True):\n",
        "           super().__init__()\n",
        "           self.rcan = base_rcan\n",
        "           self.attention_net = SelfSupervisedAttention(64)\n",
        "\n",
        "           if freeze_base:\n",
        "               for param in self.rcan.parameters():\n",
        "                   param.requires_grad = False\n",
        "\n",
        "       def forward(self, x, mode='inference'):\n",
        "           if mode == 'pre_training':\n",
        "               feats = self.rcan.extract_features(x)\n",
        "               attention, _ = self.attention_net(feats[1])\n",
        "               return attention\n",
        "\n",
        "           input_feat, body_feat = self.rcan.extract_features(x)\n",
        "           attention, attention_maps = self.attention_net(body_feat)\n",
        "           weighted_feat = body_feat * attention\n",
        "           sr_output = self.rcan.complete_sr((input_feat, weighted_feat))\n",
        "           return sr_output, attention\n",
        "\n",
        "   # Load models\n",
        "   print(\"Loading base RCAN...\")\n",
        "   base_model = load_rcan_model()\n",
        "   base_model = base_model.to(device)\n",
        "   base_model.eval()\n",
        "\n",
        "   print(\"Loading attention-augmented RCAN...\")\n",
        "   att_model = AttentionAugmentedRCAN(base_model, freeze_base=True)\n",
        "   att_model = att_model.to(device)\n",
        "   att_model.eval()\n",
        "\n",
        "   # Load validation data\n",
        "   print(\"Loading Set14 dataset...\")\n",
        "   val_loader = setup_datasets()\n",
        "\n",
        "   # Results storage\n",
        "   results = []\n",
        "\n",
        "   print(\"\\nEvaluating models on Set14:\")\n",
        "   with torch.no_grad():\n",
        "       for i, (lr_imgs, hr_imgs) in enumerate(val_loader):\n",
        "           lr_imgs = lr_imgs.to(device)\n",
        "           lr_input = lr_imgs / 255.0  # Scale to [0,1] for model input\n",
        "           hr_imgs = hr_imgs.to(device)\n",
        "\n",
        "           # Get outputs from both models\n",
        "           base_sr = base_model(lr_input) * 255.0\n",
        "           att_sr, attention = att_model(lr_input)\n",
        "           att_sr = att_sr * 255.0\n",
        "\n",
        "           # Calculate PSNR\n",
        "           base_psnr = calc_psnr(base_sr, hr_imgs, scale=4, rgb_range=255)\n",
        "           att_psnr = calc_psnr(att_sr, hr_imgs, scale=4, rgb_range=255)\n",
        "\n",
        "           # Store results\n",
        "           results.append({\n",
        "               'image_idx': i+1,\n",
        "               'base_psnr': base_psnr,\n",
        "               'att_psnr': att_psnr,\n",
        "               'improvement': att_psnr - base_psnr,\n",
        "               'attention_stats': {\n",
        "                   'min': attention.min().item(),\n",
        "                   'max': attention.max().item(),\n",
        "                   'mean': attention.mean().item(),\n",
        "                   'std': attention.std().item()\n",
        "               }\n",
        "           })\n",
        "\n",
        "           print(f\"\\nImage {i+1}:\")\n",
        "           print(f\"Base RCAN PSNR: {base_psnr:.2f} dB\")\n",
        "           print(f\"Attention RCAN PSNR: {att_psnr:.2f} dB\")\n",
        "           print(f\"Improvement: {att_psnr - base_psnr:.2f} dB\")\n",
        "\n",
        "           # Save visual comparisons for first 3 images\n",
        "           if i < 3:\n",
        "               fig = plt.figure(figsize=(20, 5))\n",
        "\n",
        "               # LR input\n",
        "               plt.subplot(1, 4, 1)\n",
        "               lr_img_vis = lr_imgs[0].cpu().permute(1, 2, 0).numpy().astype(np.uint8)\n",
        "               plt.imshow(lr_img_vis)\n",
        "               plt.title(\"LR Input\")\n",
        "               plt.axis('off')\n",
        "\n",
        "               # Base RCAN output\n",
        "               plt.subplot(1, 4, 2)\n",
        "               base_sr_vis = base_sr[0].cpu().permute(1, 2, 0).numpy().astype(np.uint8)\n",
        "               plt.imshow(base_sr_vis)\n",
        "               plt.title(f\"Base RCAN\\nPSNR: {base_psnr:.2f} dB\")\n",
        "               plt.axis('off')\n",
        "\n",
        "               # Attention RCAN output\n",
        "               plt.subplot(1, 4, 3)\n",
        "               att_sr_vis = att_sr[0].cpu().permute(1, 2, 0).numpy().astype(np.uint8)\n",
        "               plt.imshow(att_sr_vis)\n",
        "               plt.title(f\"Attention RCAN\\nPSNR: {att_psnr:.2f} dB\")\n",
        "               plt.axis('off')\n",
        "\n",
        "               # Attention map\n",
        "               plt.subplot(1, 4, 4)\n",
        "               attention_map = attention[0, 0].cpu().numpy()\n",
        "               im = plt.imshow(attention_map, cmap='viridis')\n",
        "               plt.colorbar(im)\n",
        "               plt.title(f\"Attention Map\\nRange: [{attention.min():.3f}, {attention.max():.3f}]\")\n",
        "               plt.axis('off')\n",
        "\n",
        "               plt.tight_layout()\n",
        "               plt.savefig(f'comparison_image_{i+1}.png', dpi=300, bbox_inches='tight')\n",
        "               plt.show()\n",
        "\n",
        "   # Calculate overall statistics\n",
        "   base_psnrs = [r['base_psnr'] for r in results]\n",
        "   att_psnrs = [r['att_psnr'] for r in results]\n",
        "   improvements = [r['improvement'] for r in results]\n",
        "\n",
        "   avg_base = np.mean(base_psnrs)\n",
        "   avg_att = np.mean(att_psnrs)\n",
        "   avg_imp = np.mean(improvements)\n",
        "\n",
        "   print(\"\\nOverall Results on Set14:\")\n",
        "   print(f\"Average Base RCAN PSNR: {avg_base:.2f} dB\")\n",
        "   print(f\"Average Attention RCAN PSNR: {avg_att:.2f} dB\")\n",
        "   print(f\"Average Improvement: {avg_imp:.2f} dB\")\n",
        "\n",
        "   # Statistical significance test\n",
        "   import scipy.stats as stats\n",
        "   t_stat, p_value = stats.ttest_rel(att_psnrs, base_psnrs)\n",
        "   print(f\"\\nStatistical Analysis:\")\n",
        "   print(f\"T-statistic: {t_stat:.4f}\")\n",
        "   print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "   # Save detailed results to file\n",
        "   with open('evaluation_results.txt', 'w') as f:\n",
        "       f.write(\"Set14 Evaluation Results\\n\")\n",
        "       f.write(\"=======================\\n\\n\")\n",
        "       f.write(f\"Average Base RCAN PSNR: {avg_base:.2f} dB\\n\")\n",
        "       f.write(f\"Average Attention RCAN PSNR: {avg_att:.2f} dB\\n\")\n",
        "       f.write(f\"Average Improvement: {avg_imp:.2f} dB\\n\\n\")\n",
        "       f.write(\"Per-Image Results:\\n\")\n",
        "       f.write(\"----------------\\n\")\n",
        "       for r in results:\n",
        "           f.write(f\"\\nImage {r['image_idx']}:\\n\")\n",
        "           f.write(f\"Base PSNR: {r['base_psnr']:.2f} dB\\n\")\n",
        "           f.write(f\"Attention PSNR: {r['att_psnr']:.2f} dB\\n\")\n",
        "           f.write(f\"Improvement: {r['improvement']:.2f} dB\\n\")\n",
        "           f.write(\"Attention Stats:\\n\")\n",
        "           for k, v in r['attention_stats'].items():\n",
        "               f.write(f\"  {k}: {v:.4f}\\n\")\n",
        "\n",
        "   return results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "   results = evaluate_for_paper()"
      ],
      "metadata": {
        "id": "7FK3zKqDHUEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Slightly worse results, but we haven't fully optimzied by any means.\n",
        "\n",
        "\n",
        "The key changes to follow are:\n",
        "\n",
        "4x wider feature channels\n",
        "Deeper head networks\n",
        "\n",
        "Tanh activation for full [-1,1] range\n",
        "\n",
        "Much stronger scaling factors\n",
        "\n",
        "Wider attention range (0.5 to 2.0)"
      ],
      "metadata": {
        "id": "2xVNqOTwJKNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_for_paper():\n",
        "   device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "   print(f\"Using device: {device}\")\n",
        "\n",
        "   class SelfSupervisedAttention(nn.Module):\n",
        "       def __init__(self, in_channels=64):\n",
        "           super().__init__()\n",
        "           self.in_channels = in_channels\n",
        "\n",
        "           # More powerful feature extraction\n",
        "           self.feat_conv = nn.Sequential(\n",
        "               nn.Conv2d(in_channels, in_channels*4, 3, padding=1),\n",
        "               nn.LeakyReLU(0.2, True),\n",
        "               nn.Conv2d(in_channels*4, in_channels*4, 3, padding=1),\n",
        "               nn.LeakyReLU(0.2, True)\n",
        "           )\n",
        "\n",
        "           # Three heads with different receptive fields and stronger architecture\n",
        "           self.heads = nn.ModuleList([\n",
        "               # Local features (aggressive)\n",
        "               nn.Sequential(\n",
        "                   nn.Conv2d(in_channels*4, in_channels*2, 3, padding=1),\n",
        "                   nn.LeakyReLU(0.2, True),\n",
        "                   nn.Conv2d(in_channels*2, in_channels, 3, padding=1),\n",
        "                   nn.LeakyReLU(0.2, True),\n",
        "                   nn.Conv2d(in_channels, 1, 1),\n",
        "                   nn.Tanh()\n",
        "               ),\n",
        "               # Medium features\n",
        "               nn.Sequential(\n",
        "                   nn.Conv2d(in_channels*4, in_channels*2, 5, padding=2),\n",
        "                   nn.LeakyReLU(0.2, True),\n",
        "                   nn.Conv2d(in_channels*2, in_channels, 3, padding=1),\n",
        "                   nn.LeakyReLU(0.2, True),\n",
        "                   nn.Conv2d(in_channels, 1, 1),\n",
        "                   nn.Tanh()\n",
        "               ),\n",
        "               # Global features\n",
        "               nn.Sequential(\n",
        "                   nn.Conv2d(in_channels*4, in_channels*2, 7, padding=3),\n",
        "                   nn.LeakyReLU(0.2, True),\n",
        "                   nn.Conv2d(in_channels*2, in_channels, 3, padding=1),\n",
        "                   nn.LeakyReLU(0.2, True),\n",
        "                   nn.Conv2d(in_channels, 1, 1),\n",
        "                   nn.Tanh()\n",
        "               )\n",
        "           ])\n",
        "\n",
        "           # Much stronger scaling factors\n",
        "           self.head_scales = nn.Parameter(torch.tensor([1.0, 0.7, 0.4]))\n",
        "           self.base_attention = nn.Parameter(torch.tensor(1.0))\n",
        "\n",
        "       def forward(self, x):\n",
        "           # Extract features\n",
        "           feat = self.feat_conv(x)\n",
        "\n",
        "           # Get raw attention from each head (now in [-1,1] range due to tanh)\n",
        "           attention_maps = [head(feat) for head in self.heads]\n",
        "\n",
        "           # Scale and combine heads directly\n",
        "           scaled_maps = []\n",
        "           for map, scale in zip(attention_maps, self.head_scales):\n",
        "               scaled_maps.append(map * scale)\n",
        "\n",
        "           # Sum maps and add to base attention\n",
        "           attention = self.base_attention + sum(scaled_maps)\n",
        "\n",
        "           # Allow wider range\n",
        "           attention = attention.clamp(0.5, 2.0)\n",
        "\n",
        "           return attention, attention_maps\n",
        "\n",
        "   class AttentionAugmentedRCAN(nn.Module):\n",
        "       def __init__(self, base_rcan, freeze_base=True):\n",
        "           super().__init__()\n",
        "           self.rcan = base_rcan\n",
        "           self.attention_net = SelfSupervisedAttention(64)\n",
        "\n",
        "           if freeze_base:\n",
        "               for param in self.rcan.parameters():\n",
        "                   param.requires_grad = False\n",
        "\n",
        "       def forward(self, x, mode='inference'):\n",
        "           if mode == 'pre_training':\n",
        "               feats = self.rcan.extract_features(x)\n",
        "               attention, _ = self.attention_net(feats[1])\n",
        "               return attention\n",
        "\n",
        "           input_feat, body_feat = self.rcan.extract_features(x)\n",
        "           attention, attention_maps = self.attention_net(body_feat)\n",
        "           weighted_feat = body_feat * attention\n",
        "           sr_output = self.rcan.complete_sr((input_feat, weighted_feat))\n",
        "           return sr_output, attention\n",
        "\n",
        "   # Load models\n",
        "   print(\"Loading base RCAN...\")\n",
        "   base_model = load_rcan_model()\n",
        "   base_model = base_model.to(device)\n",
        "   base_model.eval()\n",
        "\n",
        "   print(\"Loading attention-augmented RCAN...\")\n",
        "   att_model = AttentionAugmentedRCAN(base_model, freeze_base=True)\n",
        "   att_model = att_model.to(device)\n",
        "   att_model.eval()\n",
        "\n",
        "   # Load validation data\n",
        "   print(\"Loading Set14 dataset...\")\n",
        "   val_loader = setup_datasets()\n",
        "\n",
        "   # Results storage\n",
        "   results = []\n",
        "\n",
        "   print(\"\\nEvaluating models on Set14:\")\n",
        "   with torch.no_grad():\n",
        "       for i, (lr_imgs, hr_imgs) in enumerate(val_loader):\n",
        "           lr_imgs = lr_imgs.to(device)\n",
        "           lr_input = lr_imgs / 255.0  # Scale to [0,1] for model input\n",
        "           hr_imgs = hr_imgs.to(device)\n",
        "\n",
        "           # Get outputs from both models\n",
        "           base_sr = base_model(lr_input) * 255.0\n",
        "           att_sr, attention = att_model(lr_input)\n",
        "           att_sr = att_sr * 255.0\n",
        "\n",
        "           # Calculate PSNR\n",
        "           base_psnr = calc_psnr(base_sr, hr_imgs, scale=4, rgb_range=255)\n",
        "           att_psnr = calc_psnr(att_sr, hr_imgs, scale=4, rgb_range=255)\n",
        "\n",
        "           # Store results\n",
        "           results.append({\n",
        "               'image_idx': i+1,\n",
        "               'base_psnr': base_psnr,\n",
        "               'att_psnr': att_psnr,\n",
        "               'improvement': att_psnr - base_psnr,\n",
        "               'attention_stats': {\n",
        "                   'min': attention.min().item(),\n",
        "                   'max': attention.max().item(),\n",
        "                   'mean': attention.mean().item(),\n",
        "                   'std': attention.std().item()\n",
        "               }\n",
        "           })\n",
        "\n",
        "           print(f\"\\nImage {i+1}:\")\n",
        "           print(f\"Base RCAN PSNR: {base_psnr:.2f} dB\")\n",
        "           print(f\"Attention RCAN PSNR: {att_psnr:.2f} dB\")\n",
        "           print(f\"Improvement: {att_psnr - base_psnr:.2f} dB\")\n",
        "\n",
        "           # Save visual comparisons for first 3 images\n",
        "           if i < 3:\n",
        "               fig = plt.figure(figsize=(20, 5))\n",
        "\n",
        "               # LR input\n",
        "               plt.subplot(1, 4, 1)\n",
        "               lr_img_vis = lr_imgs[0].cpu().permute(1, 2, 0).numpy().astype(np.uint8)\n",
        "               plt.imshow(lr_img_vis)\n",
        "               plt.title(\"LR Input\")\n",
        "               plt.axis('off')\n",
        "\n",
        "               # Base RCAN output\n",
        "               plt.subplot(1, 4, 2)\n",
        "               base_sr_vis = base_sr[0].cpu().permute(1, 2, 0).numpy().astype(np.uint8)\n",
        "               plt.imshow(base_sr_vis)\n",
        "               plt.title(f\"Base RCAN\\nPSNR: {base_psnr:.2f} dB\")\n",
        "               plt.axis('off')\n",
        "\n",
        "               # Attention RCAN output\n",
        "               plt.subplot(1, 4, 3)\n",
        "               att_sr_vis = att_sr[0].cpu().permute(1, 2, 0).numpy().astype(np.uint8)\n",
        "               plt.imshow(att_sr_vis)\n",
        "               plt.title(f\"Attention RCAN\\nPSNR: {att_psnr:.2f} dB\")\n",
        "               plt.axis('off')\n",
        "\n",
        "               # Attention map\n",
        "               plt.subplot(1, 4, 4)\n",
        "               attention_map = attention[0, 0].cpu().numpy()\n",
        "               im = plt.imshow(attention_map, cmap='viridis')\n",
        "               plt.colorbar(im)\n",
        "               plt.title(f\"Attention Map\\nRange: [{attention.min():.3f}, {attention.max():.3f}]\")\n",
        "               plt.axis('off')\n",
        "\n",
        "               plt.tight_layout()\n",
        "               plt.savefig(f'comparison_image_{i+1}.png', dpi=300, bbox_inches='tight')\n",
        "               plt.show()\n",
        "\n",
        "   # Calculate overall statistics\n",
        "   base_psnrs = [r['base_psnr'] for r in results]\n",
        "   att_psnrs = [r['att_psnr'] for r in results]\n",
        "   improvements = [r['improvement'] for r in results]\n",
        "\n",
        "   avg_base = np.mean(base_psnrs)\n",
        "   avg_att = np.mean(att_psnrs)\n",
        "   avg_imp = np.mean(improvements)\n",
        "\n",
        "   print(\"\\nOverall Results on Set14:\")\n",
        "   print(f\"Average Base RCAN PSNR: {avg_base:.2f} dB\")\n",
        "   print(f\"Average Attention RCAN PSNR: {avg_att:.2f} dB\")\n",
        "   print(f\"Average Improvement: {avg_imp:.2f} dB\")\n",
        "\n",
        "   # Statistical significance test\n",
        "   import scipy.stats as stats\n",
        "   t_stat, p_value = stats.ttest_rel(att_psnrs, base_psnrs)\n",
        "   print(f\"\\nStatistical Analysis:\")\n",
        "   print(f\"T-statistic: {t_stat:.4f}\")\n",
        "   print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "   # Save detailed results to file\n",
        "   with open('evaluation_results.txt', 'w') as f:\n",
        "       f.write(\"Set14 Evaluation Results\\n\")\n",
        "       f.write(\"=======================\\n\\n\")\n",
        "       f.write(f\"Average Base RCAN PSNR: {avg_base:.2f} dB\\n\")\n",
        "       f.write(f\"Average Attention RCAN PSNR: {avg_att:.2f} dB\\n\")\n",
        "       f.write(f\"Average Improvement: {avg_imp:.2f} dB\\n\\n\")\n",
        "       f.write(\"Per-Image Results:\\n\")\n",
        "       f.write(\"----------------\\n\")\n",
        "       for r in results:\n",
        "           f.write(f\"\\nImage {r['image_idx']}:\\n\")\n",
        "           f.write(f\"Base PSNR: {r['base_psnr']:.2f} dB\\n\")\n",
        "           f.write(f\"Attention PSNR: {r['att_psnr']:.2f} dB\\n\")\n",
        "           f.write(f\"Improvement: {r['improvement']:.2f} dB\\n\")\n",
        "           f.write(\"Attention Stats:\\n\")\n",
        "           for k, v in r['attention_stats'].items():\n",
        "               f.write(f\"  {k}: {v:.4f}\\n\")\n",
        "\n",
        "   return results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "   results = evaluate_for_paper()"
      ],
      "metadata": {
        "id": "XZFGlwqgJOqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We do see an improvement over our baseline RCAN model now."
      ],
      "metadata": {
        "id": "HuaNcVWaJkYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_for_paper():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    class SelfSupervisedAttention(nn.Module):\n",
        "        def __init__(self, in_channels=64):\n",
        "            super().__init__()\n",
        "            self.in_channels = in_channels\n",
        "\n",
        "            # More powerful feature extraction\n",
        "            self.feat_conv = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, in_channels*4, 3, padding=1),\n",
        "                nn.LeakyReLU(0.2, True),\n",
        "                nn.Conv2d(in_channels*4, in_channels*4, 3, padding=1),\n",
        "                nn.LeakyReLU(0.2, True)\n",
        "            )\n",
        "\n",
        "            # Three heads with different receptive fields and stronger architecture\n",
        "            self.heads = nn.ModuleList([\n",
        "                # Local features (aggressive)\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(in_channels*4, in_channels*2, 3, padding=1),\n",
        "                    nn.LeakyReLU(0.2, True),\n",
        "                    nn.Conv2d(in_channels*2, in_channels, 3, padding=1),\n",
        "                    nn.LeakyReLU(0.2, True),\n",
        "                    nn.Conv2d(in_channels, 1, 1),\n",
        "                    nn.Tanh()\n",
        "                ),\n",
        "                # Medium features\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(in_channels*4, in_channels*2, 5, padding=2),\n",
        "                    nn.LeakyReLU(0.2, True),\n",
        "                    nn.Conv2d(in_channels*2, in_channels, 3, padding=1),\n",
        "                    nn.LeakyReLU(0.2, True),\n",
        "                    nn.Conv2d(in_channels, 1, 1),\n",
        "                    nn.Tanh()\n",
        "                ),\n",
        "                # Global features\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(in_channels*4, in_channels*2, 7, padding=3),\n",
        "                    nn.LeakyReLU(0.2, True),\n",
        "                    nn.Conv2d(in_channels*2, in_channels, 3, padding=1),\n",
        "                    nn.LeakyReLU(0.2, True),\n",
        "                    nn.Conv2d(in_channels, 1, 1),\n",
        "                    nn.Tanh()\n",
        "                )\n",
        "            ])\n",
        "\n",
        "            # Much stronger scaling factors\n",
        "            self.head_scales = nn.Parameter(torch.tensor([1.0, 0.7, 0.4]))\n",
        "            self.base_attention = nn.Parameter(torch.tensor(1.0))\n",
        "\n",
        "        def forward(self, x):\n",
        "            # Extract features\n",
        "            feat = self.feat_conv(x)\n",
        "\n",
        "            # Get raw attention from each head (now in [-1,1] range due to tanh)\n",
        "            attention_maps = [head(feat) for head in self.heads]\n",
        "\n",
        "            # Scale and combine heads directly\n",
        "            scaled_maps = []\n",
        "            for map, scale in zip(attention_maps, self.head_scales):\n",
        "                scaled_maps.append(map * scale)\n",
        "\n",
        "            # Sum maps and add to base attention\n",
        "            attention = self.base_attention + sum(scaled_maps)\n",
        "\n",
        "            # Allow wider range\n",
        "            attention = attention.clamp(0.5, 2.0)\n",
        "\n",
        "            return attention, attention_maps\n",
        "\n",
        "    class AttentionAugmentedRCAN(nn.Module):\n",
        "        def __init__(self, base_rcan, freeze_base=True):\n",
        "            super().__init__()\n",
        "            self.rcan = base_rcan\n",
        "            self.attention_net = SelfSupervisedAttention(64)\n",
        "\n",
        "            if freeze_base:\n",
        "                for param in self.rcan.parameters():\n",
        "                    param.requires_grad = False\n",
        "\n",
        "        def forward(self, x, mode='inference'):\n",
        "            if mode == 'pre_training':\n",
        "                feats = self.rcan.extract_features(x)\n",
        "                attention, _ = self.attention_net(feats[1])\n",
        "                return attention\n",
        "\n",
        "            input_feat, body_feat = self.rcan.extract_features(x)\n",
        "            attention, attention_maps = self.attention_net(body_feat)\n",
        "            weighted_feat = body_feat * attention\n",
        "            sr_output = self.rcan.complete_sr((input_feat, weighted_feat))\n",
        "            return sr_output, attention\n",
        "\n",
        "    # Load models\n",
        "    print(\"Loading base RCAN...\")\n",
        "    base_model = load_rcan_model()\n",
        "    base_model = base_model.to(device)\n",
        "    base_model.eval()\n",
        "\n",
        "    print(\"Loading attention-augmented RCAN...\")\n",
        "    att_model = AttentionAugmentedRCAN(base_model, freeze_base=True)\n",
        "    att_model = att_model.to(device)\n",
        "    att_model.eval()\n",
        "\n",
        "    # Load validation data\n",
        "    print(\"Loading Set14 dataset...\")\n",
        "    val_loader = setup_datasets()\n",
        "\n",
        "    # Results storage\n",
        "    results = []\n",
        "\n",
        "    print(\"\\nEvaluating models on Set14:\")\n",
        "    with torch.no_grad():\n",
        "        for i, (lr_imgs, hr_imgs) in enumerate(val_loader):\n",
        "            lr_imgs = lr_imgs.to(device)\n",
        "            lr_input = lr_imgs / 255.0  # Scale to [0,1] for model input\n",
        "            hr_imgs = hr_imgs.to(device)\n",
        "\n",
        "            # Get outputs from both models\n",
        "            base_sr = base_model(lr_input) * 255.0\n",
        "            att_sr, attention = att_model(lr_input)\n",
        "            att_sr = att_sr * 255.0\n",
        "\n",
        "            # Calculate PSNR\n",
        "            base_psnr = calc_psnr(base_sr, hr_imgs, scale=4, rgb_range=255)\n",
        "            att_psnr = calc_psnr(att_sr, hr_imgs, scale=4, rgb_range=255)\n",
        "\n",
        "            # Store results\n",
        "            results.append({\n",
        "                'image_idx': i+1,\n",
        "                'base_psnr': base_psnr,\n",
        "                'att_psnr': att_psnr,\n",
        "                'improvement': att_psnr - base_psnr,\n",
        "                'attention_stats': {\n",
        "                    'min': attention.min().item(),\n",
        "                    'max': attention.max().item(),\n",
        "                    'mean': attention.mean().item(),\n",
        "                    'std': attention.std().item()\n",
        "                }\n",
        "            })\n",
        "\n",
        "            print(f\"\\nImage {i+1}:\")\n",
        "            print(f\"Base RCAN PSNR: {base_psnr:.2f} dB\")\n",
        "            print(f\"Attention RCAN PSNR: {att_psnr:.2f} dB\")\n",
        "            print(f\"Improvement: {att_psnr - base_psnr:.2f} dB\")\n",
        "\n",
        "            # Save visual comparisons for all images\n",
        "            fig = plt.figure(figsize=(20, 5))\n",
        "\n",
        "            # LR input\n",
        "            plt.subplot(1, 4, 1)\n",
        "            lr_img_vis = lr_imgs[0].cpu().permute(1, 2, 0).numpy().astype(np.uint8)\n",
        "            plt.imshow(lr_img_vis)\n",
        "            plt.title(\"LR Input\")\n",
        "            plt.axis('off')\n",
        "\n",
        "            # Base RCAN output\n",
        "            plt.subplot(1, 4, 2)\n",
        "            base_sr_vis = base_sr[0].cpu().permute(1, 2, 0).numpy().astype(np.uint8)\n",
        "            plt.imshow(base_sr_vis)\n",
        "            plt.title(f\"Base RCAN\\nPSNR: {base_psnr:.2f} dB\")\n",
        "            plt.axis('off')\n",
        "\n",
        "            # Attention RCAN output\n",
        "            plt.subplot(1, 4, 3)\n",
        "            att_sr_vis = att_sr[0].cpu().permute(1, 2, 0).numpy().astype(np.uint8)\n",
        "            plt.imshow(att_sr_vis)\n",
        "            plt.title(f\"Attention RCAN\\nPSNR: {att_psnr:.2f} dB\")\n",
        "            plt.axis('off')\n",
        "\n",
        "            # Attention map\n",
        "            plt.subplot(1, 4, 4)\n",
        "            attention_map = attention[0, 0].cpu().numpy()\n",
        "            im = plt.imshow(attention_map, cmap='viridis')\n",
        "            plt.colorbar(im)\n",
        "            plt.title(f\"Attention Map\\nRange: [{attention.min():.3f}, {attention.max():.3f}]\")\n",
        "            plt.axis('off')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f'comparison_image_{i+1}.png', dpi=300, bbox_inches='tight')\n",
        "            plt.show()\n",
        "\n",
        "    # Calculate overall statistics\n",
        "    base_psnrs = [r['base_psnr'] for r in results]\n",
        "    att_psnrs = [r['att_psnr'] for r in results]\n",
        "    improvements = [r['improvement'] for r in results]\n",
        "\n",
        "    avg_base = np.mean(base_psnrs)\n",
        "    avg_att = np.mean(att_psnrs)\n",
        "    avg_imp = np.mean(improvements)\n",
        "\n",
        "    print(\"\\nOverall Results on Set14:\")\n",
        "    print(f\"Average Base RCAN PSNR: {avg_base:.2f} dB\")\n",
        "    print(f\"Average Attention RCAN PSNR: {avg_att:.2f} dB\")\n",
        "    print(f\"Average Improvement: {avg_imp:.2f} dB\")\n",
        "\n",
        "    # Statistical significance test\n",
        "    import scipy.stats as stats\n",
        "    t_stat, p_value = stats.ttest_rel(att_psnrs, base_psnrs)\n",
        "    print(f\"\\nStatistical Analysis:\")\n",
        "    print(f\"T-statistic: {t_stat:.4f}\")\n",
        "    print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "    # Save detailed results to file\n",
        "    with open('evaluation_results.txt', 'w') as f:\n",
        "        f.write(\"Set14 Evaluation Results\\n\")\n",
        "        f.write(\"=======================\\n\\n\")\n",
        "        f.write(f\"Average Base RCAN PSNR: {avg_base:.2f} dB\\n\")\n",
        "        f.write(f\"Average Attention RCAN PSNR: {avg_att:.2f} dB\\n\")\n",
        "        f.write(f\"Average Improvement: {avg_imp:.2f} dB\\n\\n\")\n",
        "        f.write(\"Per-Image Results:\\n\")\n",
        "        f.write(\"----------------\\n\")\n",
        "        for r in results:\n",
        "            f.write(f\"\\nImage {r['image_idx']}:\\n\")\n",
        "            f.write(f\"Base PSNR: {r['base_psnr']:.2f} dB\\n\")\n",
        "            f.write(f\"Attention PSNR: {r['att_psnr']:.2f} dB\\n\")\n",
        "            f.write(f\"Improvement: {r['improvement']:.2f} dB\\n\")\n",
        "            f.write(\"Attention Stats:\\n\")\n",
        "            for k, v in r['attention_stats'].items():\n",
        "                f.write(f\"  {k}: {v:.4f}\\n\")\n",
        "\n",
        "    return results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results = evaluate_for_paper()\n"
      ],
      "metadata": {
        "id": "LJvWgKbJQbsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solid results and statistically significant. Many more refinements to try out from here, but we're at the deadline."
      ],
      "metadata": {
        "id": "aFQuOm2tUBk_"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}